
@article{kim_multi-co-training_2019,
	title = {Multi-co-training for document classification using various document representations: {TF}–{IDF}, {LDA}, and Doc2Vec},
	volume = {477},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025518308028},
	doi = {10.1016/j.ins.2018.10.006},
	shorttitle = {Multi-co-training for document classification using various document representations},
	abstract = {The purpose of document classification is to assign the most appropriate label to a specified document. The main challenges in document classification are insufficient label information and unstructured sparse format. A semi-supervised learning ({SSL}) approach could be an effective solution to the former problem, whereas the consideration of multiple document representation schemes can resolve the latter problem. Co-training is a popular {SSL} method that attempts to exploit various perspectives in terms of feature subsets for the same example. In this paper, we propose multi-co-training ({MCT}) for improving the performance of document classification. In order to increase the variety of feature sets for classification, we transform a document using three document representation methods: term frequency–inverse document frequency ({TF}–{IDF}) based on the bag-of-words scheme, topic distribution based on latent Dirichlet allocation ({LDA}), and neural-network-based document embedding known as document to vector (Doc2Vec). The experimental results demonstrate that the proposed {MCT} is robust to parameter changes and outperforms benchmark methods under various conditions.},
	pages = {15--29},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Kim, Donghwa and Seo, Deokseong and Cho, Suhyoun and Kang, Pilsung},
	urldate = {2020-06-02},
	date = {2019-03-01},
	langid = {english},
	keywords = {Co-training, Doc2vec, Document classification, {LDA}, Semi-supervised learning, {TF}–{IDF}},
	file = {Kim et al. - 2019 - Multi-co-training for document classification usin.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\88AAEDIX\\Kim et al. - 2019 - Multi-co-training for document classification usin.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\CYAFILIL\\S0020025518308028.html:text/html}
}

@article{liu_overview_2016,
	title = {An overview of topic modeling and its current applications in bioinformatics},
	volume = {5},
	issn = {2193-1801},
	url = {http://springerplus.springeropen.com/articles/10.1186/s40064-016-3252-8},
	doi = {10.1186/s40064-016-3252-8},
	abstract = {Background:  With the rapid accumulation of biological datasets, machine learning methods designed to automate data analysis are urgently needed. In recent years, socalled topic models that originated from the field of natural language processing have been receiving much attention in bioinformatics because of their interpretability. Our aim was to review the application and development of topic models for bioinformatics. Description:  This paper starts with the description of a topic model, with a focus on the understanding of topic modeling. A general outline is provided on how to build an application in a topic model and how to develop a topic model. Meanwhile, the literature on application of topic models to biological data was searched and analyzed in depth. According to the types of models and the analogy between the concept of document-topic-word and a biological object (as well as the tasks of a topic model), we categorized the related studies and provided an outlook on the use of topic models for the development of bioinformatics applications. Conclusion:  Topic modeling is a useful method (in contrast to the traditional means of data reduction in bioinformatics) and enhances researchers’ ability to interpret biological information. Nevertheless, due to the lack of topic models optimized for specific biological data, the studies on topic modeling in biological data still have a long and challenging road ahead. We believe that topic models are a promising method for various applications in bioinformatics research.},
	pages = {1608},
	number = {1},
	journaltitle = {{SpringerPlus}},
	shortjournal = {{SpringerPlus}},
	author = {Liu, Lin and Tang, Lin and Dong, Wen and Yao, Shaowen and Zhou, Wei},
	urldate = {2020-06-13},
	date = {2016-12},
	langid = {english},
	file = {s40064-016-3252-8.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\NI33SNMQ\\s40064-016-3252-8.pdf:application/pdf;s40064-016-3252-8.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\42CEVT9S\\s40064-016-3252-8.pdf:application/pdf}
}

@incollection{griffiths_hierarchical_2004,
	title = {Hierarchical Topic Models and the Nested Chinese Restaurant Process},
	url = {http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf},
	pages = {17--24},
	booktitle = {Advances in Neural Information Processing Systems 16},
	publisher = {{MIT} Press},
	author = {Griffiths, Thomas L. and Jordan, Michael I. and Tenenbaum, Joshua B. and Blei, David M.},
	editor = {Thrun, S. and Saul, L. K. and Schölkopf, B.},
	urldate = {2020-06-14},
	date = {2004},
	file = {NIPS Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\RG9DFU5U\\Griffiths et al. - 2004 - Hierarchical Topic Models and the Nested Chinese R.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\UWEMP594\\2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.html:text/html}
}

@incollection{nguyen_lexical_2013,
	title = {Lexical and Hierarchical Topic Regression},
	url = {http://papers.nips.cc/paper/5163-lexical-and-hierarchical-topic-regression.pdf},
	pages = {1106--1114},
	booktitle = {Advances in Neural Information Processing Systems 26},
	publisher = {Curran Associates, Inc.},
	author = {Nguyen, Viet-An and Ying, Jordan L and Resnik, Philip},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	urldate = {2020-06-14},
	date = {2013},
	file = {NIPS Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\MDI2JCAK\\Nguyen et al. - 2013 - Lexical and Hierarchical Topic Regression.pdf:application/pdf;NIPS Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\TEZRJGDY\\Nguyen et al. - 2013 - Lexical and Hierarchical Topic Regression.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\CNTCQD8X\\5163-lexical-and-hierarchical-topic-regression.html:text/html;NIPS Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\9M5ZWW54\\5163-lexical-and-hierarchical-topic-regression.html:text/html}
}

@inproceedings{kim_simultaneous_2015,
	location = {Sydney, {NSW}, Australia},
	title = {Simultaneous Discovery of Common and Discriminative Topics via Joint Nonnegative Matrix Factorization},
	isbn = {978-1-4503-3664-2},
	url = {http://dl.acm.org/citation.cfm?doid=2783258.2783338},
	doi = {10.1145/2783258.2783338},
	abstract = {Understanding large-scale document collections in an efﬁcient manner is an important problem. Usually, document data are associated with other information (e.g., an author’s gender, age, and location) and their links to other entities (e.g., co-authorship and citation networks). For the analysis of such data, we often have to reveal common as well as discriminative characteristics of documents with respect to their associated information, e.g., male- vs. femaleauthored documents, old vs. new documents, etc. To address such needs, this paper presents a novel topic modeling method based on joint nonnegative matrix factorization, which simultaneously discovers common as well as discriminative topics given multiple document sets. Our approach is based on a block-coordinate descent framework and is capable of utilizing only the most representative, thus meaningful, keywords in each topic through a novel pseudodeﬂation approach. We perform both quantitative and qualitative evaluations using synthetic as well as real-world document data sets such as research paper collections and nonproﬁt micro-ﬁnance data. We show our method has a great potential for providing indepth analyses by clearly identifying common and discriminative topics among multiple document sets.},
	eventtitle = {the 21th {ACM} {SIGKDD} International Conference},
	pages = {567--576},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining - {KDD} '15},
	publisher = {{ACM} Press},
	author = {Kim, Hannah and Choo, Jaegul and Kim, Jingu and Reddy, Chandan K. and Park, Haesun},
	urldate = {2020-06-14},
	date = {2015},
	langid = {english},
	file = {kdd15_discnmf.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\U22IF8F6\\kdd15_discnmf.pdf:application/pdf}
}

@article{ding_vsec-lda_2020,
	title = {{VSEC}-{LDA}: Boosting Topic Modeling with Embedded Vocabulary Selection},
	url = {http://arxiv.org/abs/2001.05578},
	shorttitle = {{VSEC}-{LDA}},
	abstract = {Topic modeling has found wide application in many problems where latent structures of the data are crucial for typical inference tasks. When applying a topic model, a relatively standard pre-processing step is to first build a vocabulary of frequent words. Such a general pre-processing step is often independent of the topic modeling stage, and thus there is no guarantee that the pre-generated vocabulary can support the inference of some optimal (or even meaningful) topic models appropriate for a given task, especially for computer vision applications involving "visual words". In this paper, we propose a new approach to topic modeling, termed Vocabulary-Selection-Embedded Correspondence-{LDA} ({VSEC}-{LDA}), which learns the latent model while simultaneously selecting most relevant words. The selection of words is driven by an entropy-based metric that measures the relative contribution of the words to the underlying model, and is done dynamically while the model is learned. We present three variants of {VSEC}-{LDA} and evaluate the proposed approach with experiments on both synthetic and real databases from different applications. The results demonstrate the effectiveness of built-in vocabulary selection and its importance in improving the performance of topic modeling.},
	journaltitle = {{arXiv}:2001.05578 [cs]},
	author = {Ding, Yuzhen and Li, Baoxin},
	urldate = {2020-06-14},
	date = {2020-01-15},
	eprinttype = {arxiv},
	eprint = {2001.05578},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\TY3RK5X6\\Ding and Li - 2020 - VSEC-LDA Boosting Topic Modeling with Embedded Vo.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\FR3AEEZJ\\Ding and Li - 2020 - VSEC-LDA Boosting Topic Modeling with Embedded Vo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\HRIMMUGI\\2001.html:text/html;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\CD6PP9EM\\2001.html:text/html}
}

@article{hua_probabilistic_2020,
	title = {Probabilistic Topic Modeling for Comparative Analysis of Document Collections},
	volume = {14},
	issn = {1556-4681, 1556-472X},
	url = {https://dl.acm.org/doi/10.1145/3369873},
	doi = {10.1145/3369873},
	pages = {1--27},
	number = {2},
	journaltitle = {{ACM} Transactions on Knowledge Discovery from Data},
	shortjournal = {{ACM} Trans. Knowl. Discov. Data},
	author = {Hua, Ting and Lu, Chang-Tien and Choo, Jaegul and Reddy, Chandan K.},
	urldate = {2020-06-14},
	date = {2020-03-07},
	langid = {english},
	file = {Hua et al. - 2020 - Probabilistic Topic Modeling for Comparative Analy.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\5X6RM6ZM\\Hua et al. - 2020 - Probabilistic Topic Modeling for Comparative Analy.pdf:application/pdf;Hua et al. - 2020 - Probabilistic Topic Modeling for Comparative Analy.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\CIAXKAJ5\\Hua et al. - 2020 - Probabilistic Topic Modeling for Comparative Analy.pdf:application/pdf}
}

@article{blei_probabilistic_2012,
	title = {Probabilistic topic models},
	volume = {55},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2133806.2133826},
	doi = {10.1145/2133806.2133826},
	pages = {77--84},
	number = {4},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Blei, David M.},
	urldate = {2020-06-14},
	date = {2012-04},
	langid = {english},
	file = {Blei - 2012 - Probabilistic topic models.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\DSKPH2DW\\Blei - 2012 - Probabilistic topic models.pdf:application/pdf}
}

@article{tu_graphical_nodate,
	title = {Graphical models and topic modeling},
	pages = {57},
	journaltitle = {Graphical models},
	author = {Tu, Ho},
	langid = {english},
	file = {Tu - Graphical models and topic modeling.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\IBN9DLRA\\Tu - Graphical models and topic modeling.pdf:application/pdf}
}

@article{blei_introduction_nodate,
	title = {Introduction to Probabilistic Topic Models},
	abstract = {Probabilistic topic models are a suite of algorithms whose aim is to discover the hidden thematic structure in large archives of documents. In this article, we review the main ideas of this ﬁeld, survey the current state-of-the-art, and describe some promising future directions. We ﬁrst describe latent Dirichlet allocation ({LDA}) [8], which is the simplest kind of topic model. We discuss its connections to probabilistic modeling, and describe two kinds of algorithms for topic discovery. We then survey the growing body of research that extends and applies topic models in interesting ways. These extensions have been developed by relaxing some of the statistical assumptions of {LDA}, incorporating meta-data into the analysis of the documents, and using similar kinds of models on a diversity of data types such as social networks, images and genetics. Finally, we give our thoughts as to some of the important unexplored directions for topic modeling. These include rigorous methods for checking models built for data exploration, new approaches to visualizing text and other high dimensional data, and moving beyond traditional information engineering applications towards using topic models for more scientiﬁc ends.},
	pages = {16},
	author = {Blei, David M},
	langid = {english},
	file = {Blei - Introduction to Probabilistic Topic Models.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\DX8BDEM3\\Blei - Introduction to Probabilistic Topic Models.pdf:application/pdf}
}

@inproceedings{li_pachinko_2006,
	location = {Pittsburgh, Pennsylvania},
	title = {Pachinko allocation: {DAG}-structured mixture models of topic correlations},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143917},
	doi = {10.1145/1143844.1143917},
	shorttitle = {Pachinko allocation},
	abstract = {Latent Dirichlet allocation ({LDA}) and other related topic models are increasingly popular tools for summarization and manifold discovery in discrete data. However, {LDA} does not capture correlations between topics. In this paper, we introduce the pachinko allocation model ({PAM}), which captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph ({DAG}). The leaves of the {DAG} represent individual words in the vocabulary, while each interior node represents a correlation among its children, which may be words or other interior nodes (topics). {PAM} provides a ﬂexible alternative to recent work by Blei and Laﬀerty (2006), which captures correlations only between pairs of topics. Using text data from newsgroups, historic {NIPS} proceedings and other research paper corpora, we show improved performance of {PAM} in document classiﬁcation, likelihood of held-out data, the ability to support ﬁner-grained topics, and topical keyword coherence.},
	eventtitle = {the 23rd international conference},
	pages = {577--584},
	booktitle = {Proceedings of the 23rd international conference on Machine learning  - {ICML} '06},
	publisher = {{ACM} Press},
	author = {Li, Wei and {McCallum}, Andrew},
	urldate = {2020-06-14},
	date = {2006},
	langid = {english},
	file = {Li and McCallum - 2006 - Pachinko allocation DAG-structured mixture models.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\6W8SMQJ6\\Li and McCallum - 2006 - Pachinko allocation DAG-structured mixture models.pdf:application/pdf}
}

@article{teh_hierarchical_2006,
	title = {Hierarchical Dirichlet Processes},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000000302},
	doi = {10.1198/016214506000000302},
	abstract = {We consider problems involving groups of data, where each observation within a group is a draw from a mixture model, and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, speciﬁcally one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures, and describe applications to problems in information retrieval and text modelling.},
	pages = {1566--1581},
	number = {476},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
	urldate = {2020-06-16},
	date = {2006-12},
	langid = {english},
	file = {Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\SDT4MFS5\\Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:application/pdf}
}

@inproceedings{barde_overview_2017,
	title = {An overview of topic modeling methods and tools},
	doi = {10.1109/ICCONS.2017.8250563},
	abstract = {Topic modeling is a powerful technique for analysis of a huge collection of a document. Topic modeling is used for discovering hidden structure from the collection of a document. The topic is viewed as a recurring pattern of co-occurring words. A topic includes a group of words that often occurs together. Topic modeling can link words with the same context and differentiate across the uses of words with different meanings. In this paper, we discuss methods of Topic Modeling which includes Vector Space Model ({VSM}), Latent Semantic Indexing ({LSI}), Probabilistic Latent Semantic Analysis ({PLSA}), Latent Dirichlet Allocation ({LDA}) with their features and limitations. After that, we will discuss tools available for topic modeling such as Gensim, Standford topic modeling toolbox, {MALLET}, {BigARTM}. Then some of the applications of Topic Modeling covered. Topic models have a wide range of applications like tag recommendation, text categorization, keyword extraction, information filtering and similarity search in the fields of text mining, information retrieval.},
	eventtitle = {2017 International Conference on Intelligent Computing and Control Systems ({ICICCS})},
	pages = {745--750},
	booktitle = {2017 International Conference on Intelligent Computing and Control Systems ({ICICCS})},
	author = {Barde, Bhagyashree Vyankatrao and Bainwad, Anant Madhavrao},
	date = {2017-06},
	keywords = {Analytical models, {BigARTM}, co-occurring words, Computational modeling, document handling, Gensim, Large scale integration, Latent Dirichlet Allocation, Latent Dirichlet Allocation ({LDA}), Latent Semantic Analysis ({LSA}), Latent Semantic Indexing, {MALLET}, Matrix decomposition, Methods of Topic Modeling, Probabilistic Latent Semantic Analysis, Probabilistic Latent Semantic Analysis ({PLSA}), Probabilistic logic, probability, Semantics, Standford topic modeling toolbox, Topic Modeling, topic modeling methods, Vector Space Model, Vector Space Model ({VSM}), vectors},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\5INEUHCN\\8250563.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\69A5NRRW\\Barde and Bainwad - 2017 - An overview of topic modeling methods and tools.pdf:application/pdf}
}

@inproceedings{wang_sparse_2014,
	title = {Sparse posterior probability support vector machines},
	doi = {10.1109/SSP.2014.6884659},
	abstract = {Posterior probability support vector machines ({PPSVMs}) are proved to have good generalization performance and robustness against outliers. However, the disadvantage of a {PPSVM} is lack of sparseness of solution, i.e., the number of support vectors is still too large. This results in high computational burden and decision time. In this paper, we present two approaches to obtain sparse {PPSVMs}, which are expected to combine benefits of both {PPSVMs} and sparse classifiers. The first approach sparsifies the {PPSVMs} by adding l1 norm penalties on the dual cost function of soft margin {PPSVMs}. The second one handles a mixed l1-l2 multi-objective optimization by interior-point algorithm. Simulation results show that both approaches have good generalization performance, good robustness against outliers, and high efficiency on decision evaluation.},
	eventtitle = {2014 {IEEE} Workshop on Statistical Signal Processing ({SSP})},
	pages = {396--399},
	booktitle = {2014 {IEEE} Workshop on Statistical Signal Processing ({SSP})},
	author = {Wang, Dongli and Zhou, Yan},
	date = {2014-06},
	note = {{ISSN}: 2373-0803},
	keywords = {pattern classification, probability, Accuracy, compressed sensing, decision evaluation, dual cost function, interior-point algorithm, Kernel, l1 norm penalties, mixed l1-l2 multiobjective optimization, optimisation, Optimization, outliers, posterior probability, Probability, Robustness, soft margin {PPSVMs}, sparse classifiers, sparse posterior probability support vector machines, sparse representation, Support vector machine, support vector machines, Support vector machines, Training},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\4RRA9B4M\\6884659.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\INRJ29AR\\Wang and Zhou - 2014 - Sparse posterior probability support vector machin.pdf:application/pdf}
}

@article{dieng_dynamic_2019,
	title = {The Dynamic Embedded Topic Model},
	url = {http://arxiv.org/abs/1907.05545},
	abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. For documents collected in sequence, dynamic topic models capture how these patterns vary over time. We develop the dynamic embedded topic model (D-{ETM}), a generative model of documents that combines dynamic latent Dirichlet allocation (D-{LDA}) and word embeddings. The D-{ETM} models each word with a categorical distribution parameterized by the inner product between the word embedding and a per-time-step embedding representation of its assigned topic. The D-{ETM} learns smooth topic trajectories by defining a random walk prior over the embedding representations of the topics. We fit the D-{ETM} using structured amortized variational inference with a recurrent neural network. On three different corpora---a collection of United Nations debates, a set of {ACL} abstracts, and a dataset of Science Magazine articles---we found that the D-{ETM} outperforms D-{LDA} on a document completion task. We further found that the D-{ETM} learns more diverse and coherent topics than D-{LDA} while requiring significantly less time to fit.},
	journaltitle = {{arXiv}:1907.05545 [cs, stat]},
	author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
	urldate = {2020-06-18},
	date = {2019-10-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.05545},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {Dieng et al. - 2019 - The Dynamic Embedded Topic Model.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\K2MGE4QX\\Dieng et al. - 2019 - The Dynamic Embedded Topic Model.pdf:application/pdf}
}

@article{li_integration_nodate,
	title = {Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process},
	pages = {11},
	author = {Li, Dingcheng and Zamani, Siamak and Zhang, Jingyuan and Li, Ping},
	langid = {english},
	file = {Li et al. - Integration of Knowledge Graph Embedding Into Topi.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\4T4HGUFI\\Li et al. - Integration of Knowledge Graph Embedding Into Topi.pdf:application/pdf}
}

@article{teh_tutorial_nodate,
	title = {A Tutorial on Dirichlet Processes  and Hierarchical Dirichlet Processes},
	pages = {75},
	author = {Teh, Yee Whye},
	langid = {english},
	file = {Teh - A Tutorial on Dirichlet Processes  and Hierarchica.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\UP4GTENR\\Teh - A Tutorial on Dirichlet Processes  and Hierarchica.pdf:application/pdf}
}

@article{gershman_tutorial_2011,
	title = {A Tutorial on Bayesian Nonparametric Models},
	url = {http://arxiv.org/abs/1106.2697},
	abstract = {A key problem in statistical modeling is model selection, how to choose a model at an appropriate level of complexity. This problem appears in many settings, most prominently in choosing the number of clusters in mixture models or the number of factors in factor analysis. In this tutorial we describe Bayesian nonparametric methods, a class of methods that side-steps this issue by allowing the data to determine the complexity of the model. This tutorial is a high-level introduction to Bayesian nonparametric methods and contains several examples of their application.},
	journaltitle = {{arXiv}:1106.2697 [stat]},
	author = {Gershman, Samuel J. and Blei, David M.},
	urldate = {2020-06-18},
	date = {2011-08-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1106.2697},
	keywords = {Statistics - Machine Learning, Statistics - Methodology},
	file = {Gershman and Blei - 2011 - A Tutorial on Bayesian Nonparametric Models.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\V6NJ2DZ5\\Gershman and Blei - 2011 - A Tutorial on Bayesian Nonparametric Models.pdf:application/pdf}
}

@article{tian_two-stage_2020,
	title = {A two-stage hybrid probabilistic topic model for refining image annotation},
	volume = {11},
	issn = {1868-8071, 1868-808X},
	url = {http://link.springer.com/10.1007/s13042-019-00983-w},
	doi = {10.1007/s13042-019-00983-w},
	abstract = {Refining image annotation has become one of the core research topics in computer vision and pattern recognition due to its great potentials in image retrieval. However, it is still in its infancy and is not sophisticated enough to extract perfect semantic concepts just according to the image low-level features. In this paper, we propose a two-stage hybrid probabilistic topic model to improve the quality of automatic image annotation. To start with, a probabilistic latent semantic analysis model with asymmetric modalities is learned to estimate the posterior probabilities of each annotation keyword, during which the image-to-word relation can be well established. Next, a label similarity graph is constructed by a weighted linear combination of label similarity and visual similarity of images associated with the corresponding labels. By this way, the information from image low-level visual features and high-level semantic concepts can be seamlessly integrated by fully taking into account the word-to-word and image-to-image relations. Finally, the rank-two relaxation heuristics is exploited to further mine the correlation of the candidate annotations so as to capture the refining results, which plays a critical role in semantic based image retrieval. Extensive experiments show that the proposed model achieves not only superior annotation accuracy but also better retrieval performance.},
	pages = {417--431},
	number = {2},
	journaltitle = {International Journal of Machine Learning and Cybernetics},
	shortjournal = {Int. J. Mach. Learn. \& Cyber.},
	author = {Tian, Dongping and Shi, Zhongzhi},
	urldate = {2020-06-18},
	date = {2020-02},
	langid = {english},
	file = {s13042-019-00983-w.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\YG624CQS\\s13042-019-00983-w.pdf:application/pdf;Tian and Shi - 2020 - A two-stage hybrid probabilistic topic model for r.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\C44MBR7L\\Tian and Shi - 2020 - A two-stage hybrid probabilistic topic model for r.pdf:application/pdf}
}

@article{grifths_indian_nodate,
	title = {The Indian Buffet Process: An Introduction and Review},
	abstract = {The Indian buffet process is a stochastic process deﬁning a probability distribution over equivalence classes of sparse binary matrices with a ﬁnite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially inﬁnite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an inﬁnite latent feature model. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.},
	pages = {40},
	author = {Grifﬁths, Thomas L and Ghahramani, Zoubin},
	langid = {english},
	file = {Grifﬁths and Ghahramani - The Indian Buffet Process An Introduction and Rev.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\RW445VLU\\Grifﬁths and Ghahramani - The Indian Buffet Process An Introduction and Rev.pdf:application/pdf}
}

@incollection{mcauliffe_supervised_2008,
	title = {Supervised Topic Models},
	url = {http://papers.nips.cc/paper/3328-supervised-topic-models.pdf},
	pages = {121--128},
	booktitle = {Advances in Neural Information Processing Systems 20},
	publisher = {Curran Associates, Inc.},
	author = {Mcauliffe, Jon D. and Blei, David M.},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	urldate = {2020-06-21},
	date = {2008},
	file = {NIPS Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\LW4VTF2G\\Mcauliffe and Blei - 2008 - Supervised Topic Models.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\SDJUYGQI\\3328-supervised-topic-models.html:text/html}
}

@online{noauthor_cisc_nodate,
	title = {{CISC} 844 Spring 2020: Schedule},
	url = {https://www.eecis.udel.edu/~shatkay/Course/CISC844S2020Schedule.html},
	urldate = {2020-06-21},
	file = {CISC 844 Spring 2020\: Schedule:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\LGWCD6FU\\CISC844S2020Schedule.html:text/html}
}

@online{noauthor_10-708_nodate,
	title = {10-708 {PGM} {\textbar} Schedule},
	url = {https://sailinglab.github.io/pgm-spring-2019/lectures/},
	urldate = {2020-06-21},
	file = {10-708 PGM | Schedule:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\LLN9RE8K\\lectures.html:text/html}
}

@online{noauthor_distill_nodate,
	title = {Distill — Latest articles about machine learning},
	url = {https://distill.pub/},
	urldate = {2020-06-21},
	file = {Distill — Latest articles about machine learning:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\X97NMTET\\distill.pub.html:text/html}
}

@article{ishwaran_gibbs_2001,
	title = {Gibbs Sampling Methods for Stick-Breaking Priors},
	volume = {96},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214501750332758},
	doi = {10.1198/016214501750332758},
	pages = {161--173},
	number = {453},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Ishwaran, Hemant and James, Lancelot F},
	urldate = {2020-06-21},
	date = {2001-03},
	langid = {english},
	file = {Ishwaran and James - 2001 - Gibbs Sampling Methods for Stick-Breaking Priors.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\DI3HXW39\\Ishwaran and James - 2001 - Gibbs Sampling Methods for Stick-Breaking Priors.pdf:application/pdf}
}

@article{wang_online_nodate,
	title = {Online Variational Inference for the Hierarchical Dirichlet Process},
	abstract = {The hierarchical Dirichlet process ({HDP}) is a Bayesian nonparametric model that can be used to model mixed-membership data with a potentially inﬁnite number of components. It has been applied widely in probabilistic topic modeling, where the data are documents and the components are distributions of terms that reﬂect recurring patterns (or “topics”) in the collection. Given a document collection, posterior inference is used to determine the number of topics needed and to characterize their distributions. One limitation of {HDP} analysis is that existing posterior inference algorithms require multiple passes through all the data—these algorithms are intractable for very large scale applications. We propose an online variational inference algorithm for the {HDP}, an algorithm that is easily applicable to massive and streaming data. Our algorithm is signiﬁcantly faster than traditional inference algorithms for the {HDP}, and lets us analyze much larger data sets. We illustrate the approach on two large collections of text, showing improved performance over online {LDA}, the ﬁnite counterpart to the {HDP} topic model.},
	pages = {9},
	author = {Wang, Chong and Paisley, John and Blei, David M},
	langid = {english},
	file = {Wang et al. - Online Variational Inference for the Hierarchical .pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\FJBJI3V9\\Wang et al. - Online Variational Inference for the Hierarchical .pdf:application/pdf}
}

@inproceedings{khalid_topic_2020,
	title = {Topic Detection from Conversational Dialogue Corpus with Parallel Latent Dirichlet Allocation Model and Elbow Method},
	url = {https://aircconline.com/csit/papers/vol10/csit100508.pdf},
	doi = {10.5121/csit.2020.100508},
	abstract = {A conversational system needs to know how to switch between topics to continue the conversation for a more extended period. For this topic detection from dialogue corpus has become an important task for a conversation and accurate prediction of conversation topics is important for creating coherent and engaging dialogue systems. In this paper, we proposed a topic detection approach with Parallel Latent Dirichlet Allocation ({PLDA}) Model by clustering a vocabulary of known similar words based on {TF}-{IDF} scores and Bag of Words ({BOW}) technique. In the experiment, we use K-mean clustering with Elbow Method for interpretation and validation of consistency within-cluster analysis to select the optimal number of clusters. We evaluate our approach by comparing it with traditional {LDA} and clustering technique. The experimental results show that combining {PLDA} with Elbow method selects the optimal number of clusters and refine the topics for the conversation.},
	eventtitle = {9th International Conference on Information Technology Convergence and Services ({ITCSE} 2020)},
	pages = {95--102},
	booktitle = {9th International Conference on Information Technology Convergence and Services ({ITCSE} 2020)},
	publisher = {{AIRCC} Publishing Corporation},
	author = {Khalid, Haider and Wade, Vincent},
	urldate = {2020-06-21},
	date = {2020-05-30},
	langid = {english},
	file = {Khalid and Wade - 2020 - Topic Detection from Conversational Dialogue Corpu.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\D7G767JL\\Khalid and Wade - 2020 - Topic Detection from Conversational Dialogue Corpu.pdf:application/pdf}
}

@online{noauthor_index_nodate,
	title = {Index of /{\textasciitilde}bao/{VIASM}-{SML}/Lecture},
	url = {https://www.jaist.ac.jp/~bao/VIASM-SML/Lecture/},
	urldate = {2020-06-21},
	file = {Index of /~bao/VIASM-SML/Lecture:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\MRKDIRVW\\Lecture.html:text/html}
}

@online{noauthor_cmsc_nodate,
	title = {{CMSC} 726},
	url = {http://users.umiacs.umd.edu/~jbg/teaching/CMSC_726/},
	urldate = {2020-06-21},
	file = {CMSC 726:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\FWMYJ3AG\\CMSC_726.html:text/html}
}

@online{noauthor_machine_nodate,
	title = {Machine Learning Advanced Tutorial Lecture Series},
	url = {http://mlg.eng.cam.ac.uk/tutorials/07/},
	urldate = {2020-06-21},
	file = {Machine Learning Advanced Tutorial Lecture Series:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\K3P22XRU\\07.html:text/html}
}

@inproceedings{liu_overview_2016-1,
	title = {An Overview of Hierarchical Topic Modeling},
	volume = {01},
	doi = {10.1109/IHMSC.2016.101},
	abstract = {Topic correlations are common in real-world textual information. However, classic topic modeling isn't able to model the correlations among topics because of a single distribution over topics in each document. Therefore, hierarchical topic modeling is designed to relax this restriction. In this paper, hierarchical topic modeling is summarized by analysis of existing studies, especially, two important representatives of hierarchical topic models and their extension are focused on. To the best of our knowledge, this is the first effort to review the development of hierarchical topic modeling.},
	eventtitle = {2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics ({IHMSC})},
	pages = {391--394},
	booktitle = {2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics ({IHMSC})},
	author = {Liu, Lin and Tang, Lin and He, Libo and Zhou, Wei and Yao, Shaowen},
	date = {2016-08},
	keywords = {Analytical models, Probabilistic logic, Correlation, document, generative process, hierarchical topic, hierarchical topic modeling, natural language processing, Parameter estimation, Probability distribution, real-world textual information, Resource management, single distribution, text analysis, topic correlations, Topic modeling, Vocabulary},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\63BN6N6T\\7783636.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\FN3YKLRL\\Liu et al. - 2016 - An Overview of Hierarchical Topic Modeling.pdf:application/pdf}
}

@online{noauthor_personalized_nodate,
	title = {A personalized hashtag recommendation approach using {LDA}-based topic model in microblog environment {\textbar} Elsevier Enhanced Reader},
	url = {https://reader.elsevier.com/reader/sd/pii/S0167739X15003258?token=F0E68E62AC34070CAE2865A8E15A6DB619DCDDF756CA0153BA2DCC6F5AC9925F0B73627804F1194884CEFD7E24F0C98D},
	urldate = {2020-06-25},
	langid = {english},
	doi = {10.1016/j.future.2015.10.012},
	note = {Library Catalog: reader.elsevier.com},
	file = {Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\ISQEW3KF\\S0167739X15003258.html:text/html}
}

@article{apaza_online_nodate,
	title = {Online Courses Recommendation based on {LDA}},
	abstract = {In this paper we propose a course recommendation system based on historical grades of students in college. Our model will be able to recommend available courses in sites such as: Coursera, Udacity, Edx, etc. To do so, probabilistic topic models are used as follows. On one hand, Latent Dirichlet Allocation ({LDA}) topic model infers topics from content given in a college course syllabus. On the other hand, topics are also extracted from a massive online open course ({MOOC}) syllabus. These two sets of topics and grading information are matched using a content based recommendation system so as to recommend relevant online courses to students. Preliminary results show suitability of our approach.},
	pages = {7},
	author = {Apaza, Rel Guzman and Cervantes, Elizabeth Vera and Quispe, Laura Cruz and Luna, Jose Ochoa},
	langid = {english},
	file = {Apaza et al. - Online Courses Recommendation based on LDA.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\UDFS7Z2I\\Apaza et al. - Online Courses Recommendation based on LDA.pdf:application/pdf}
}

@article{gao_seco-lda_2019,
	title = {{SeCo}-{LDA}: Mining Service Co-Occurrence Topics for Composition Recommendation},
	volume = {12},
	issn = {1939-1374},
	doi = {10.1109/TSC.2018.2821149},
	shorttitle = {{SeCo}-{LDA}},
	abstract = {Service composition remains an important topic where recommendation is widely recognized as a core mechanism. Existing works on service recommendation typically examine either association rules from mashup-service usage records, or latent topics from service descriptions. This paper moves one step further, by studying latent topic models over service collaboration history. A concept of service co-occurrence topic is coined, equipped with a mechanism developed to construct service co-occurrence documents. The key idea is to treat each service as a document and its co-occurring services as the bag of words in that document. Four gauges are constructed to measure self-co-occurrence of a specific service. A theoretical approach, Service Co-occurrence {LDA} ({SeCo}-{LDA}), is developed to extract latent service co-occurrence topics, including representative services and words, temporal strength, and services' impact on topics. Such derived knowledge of topics will help to reveal the trend of service composition, understand collaboration behaviors among services and lead to better service recommendation. To verify the effectiveness and efficiency of our approach, experiments on a real-world data set were conducted. Compared with methods of Apriori, content matching based on service description, and {LDA} using mashup-service usage records, our experiments show that {SeCo}-{LDA} can recommend service composition more effectively, i.e., 5\% better in terms of Mean Average Precision than baselines.},
	pages = {446--459},
	number = {3},
	journaltitle = {{IEEE} Transactions on Services Computing},
	author = {Gao, Zhenfeng and Fan, Yushun and Wu, Cheng and Tan, Wei and Zhang, Jia and Ni, Yayu and Bai, Bing and Chen, Shuhui},
	date = {2019-05},
	note = {Conference Name: {IEEE} Transactions on Services Computing},
	keywords = {Computational modeling, Probabilistic logic, Semantics, automatic service composition, Biological system modeling, co-occurrence documents, co-occurrence topic, composition recommendation, data mining, Ecosystems, groupware, Market research, mashup-service usage records, Mashups, recommender systems, {SeCo}-{LDA}, service co-occurrence {LDA}, service co-occurrence topics mining, service collaboration history, service composition, service composition recommendation, service recommendation, Topic model},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\BKDZLR7F\\8328903.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\WGZTL7IX\\Gao et al. - 2019 - SeCo-LDA Mining Service Co-Occurrence Topics for .pdf:application/pdf}
}

@inproceedings{xu_uis-lda_2017,
	location = {Leipzig Germany},
	title = {{UIS}-{LDA}: a user recommendation based on social connections and interests of users in uni-directional social networks},
	isbn = {978-1-4503-4951-2},
	url = {https://dl.acm.org/doi/10.1145/3106426.3106494},
	doi = {10.1145/3106426.3106494},
	shorttitle = {{UIS}-{LDA}},
	eventtitle = {{WI} '17: International Conference on Web Intelligence 2017},
	pages = {260--265},
	booktitle = {Proceedings of the International Conference on Web Intelligence},
	publisher = {{ACM}},
	author = {Xu, Ke and Cai, Yi and Min, Huaqing and Zheng, Xushen and Xie, Haoran and Wong, Tak-Lam},
	urldate = {2020-06-25},
	date = {2017-08-23},
	langid = {english},
	file = {Xu et al. - 2017 - UIS-LDA a user recommendation based on social con.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\IPGXNJF8\\Xu et al. - 2017 - UIS-LDA a user recommendation based on social con.pdf:application/pdf}
}

@article{blei_correlated_2007,
	title = {A correlated topic model of Science},
	volume = {1},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/euclid.aoas/1183143727},
	doi = {10.1214/07-AOAS114},
	abstract = {Topic models, such as latent Dirichlet allocation ({LDA}), can be useful tools for the statistical analysis of document collections and other discrete data. The {LDA} model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of {LDA} is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model ({CTM}), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139–177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the {CTM} to the articles from Science published from 1990–1999, a data set that comprises 57M words. The {CTM} gives a better fit of the data than {LDA}, and we demonstrate its use as an exploratory tool of large document collections.},
	pages = {17--35},
	number = {1},
	journaltitle = {Annals of Applied Statistics},
	shortjournal = {Ann. Appl. Stat.},
	author = {Blei, David M. and Lafferty, John D.},
	urldate = {2020-06-25},
	date = {2007-06},
	mrnumber = {MR2393839},
	zmnumber = {1129.62122},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {text analysis, approximate posterior inference, Hierarchical models, variational methods},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\RB9N3IEG\\Blei and Lafferty - 2007 - A correlated topic model of Science.pdf:application/pdf;Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\WWJ5HNJP\\1183143727.html:text/html}
}

@article{blei_latent_2003,
	title = {Latent Dirichlet Allocation},
	volume = {3},
	issn = {{ISSN} 1533-7928},
	url = {http://www.jmlr.org/papers/v3/blei03a},
	pages = {993--1022},
	issue = {Jan},
	journaltitle = {Journal of Machine Learning Research},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	urldate = {2020-06-25},
	date = {2003},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\IXQN4VUJ\\Blei et al. - 2003 - Latent Dirichlet Allocation.pdf:application/pdf;Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\PNG6BQV8\\blei03a.html:text/html}
}

@inproceedings{blei_dynamic_2006,
	location = {Pittsburgh, Pennsylvania, {USA}},
	title = {Dynamic topic models},
	isbn = {978-1-59593-383-6},
	url = {https://doi.org/10.1145/1143844.1143859},
	doi = {10.1145/1143844.1143859},
	series = {{ICML} '06},
	abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the {OCR}'ed archives of the journal Science from 1880 through 2000.},
	pages = {113--120},
	booktitle = {Proceedings of the 23rd international conference on Machine learning},
	publisher = {Association for Computing Machinery},
	author = {Blei, David M. and Lafferty, John D.},
	urldate = {2020-06-25},
	date = {2006-06-25},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\7FC54GHV\\Blei and Lafferty - 2006 - Dynamic topic models.pdf:application/pdf}
}

@article{jelodar_latent_2019,
	title = {Latent Dirichlet allocation ({LDA}) and topic modeling: models, applications, a survey},
	volume = {78},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-018-6894-4},
	doi = {10.1007/s11042-018-6894-4},
	shorttitle = {Latent Dirichlet allocation ({LDA}) and topic modeling},
	abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation ({LDA}) is one of the most popular in this field. Researchers have proposed various models based on the {LDA} in topic modeling. According to previous work, this paper will be very useful and valuable for introducing {LDA} approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on {LDA} to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on {LDA}.},
	pages = {15169--15211},
	number = {11},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimed Tools Appl},
	author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
	urldate = {2020-06-25},
	date = {2019-06},
	langid = {english},
	file = {Jelodar et al. - 2019 - Latent Dirichlet allocation (LDA) and topic modeli.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\EMCT9XLS\\Jelodar et al. - 2019 - Latent Dirichlet allocation (LDA) and topic modeli.pdf:application/pdf;Jelodar et al. - 2019 - Latent Dirichlet allocation (LDA) and topic modeli.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\HRI93GHI\\Jelodar et al. - 2019 - Latent Dirichlet allocation (LDA) and topic modeli.pdf:application/pdf}
}

@inproceedings{xia_survey_2019,
	title = {A Survey of Topic Models in Text Classification},
	doi = {10.1109/ICAIBD.2019.8836970},
	abstract = {A massive of text that is generated every minute is increasing dramatically. Therefore, it is more and more important to find an effective model to automatic classify the amount of text. Topic models is the most powerful techniques in text classification. There are many research results in the field of topic model have been published in scholarly journals. The Latent Dirichlet Allocation ({LDA}) is one of the most popular topic models in text classification. Researchers have proposed many topic evolution models based on {LDA} to solve some specific problems in applications of text classification. And some joint models which based on topic models combined other algorithms have been studied to enhance the performance of text classification. In this paper, we investigated three categories topic models for text classification and briefly introduced their advantages and disadvantages in the applications of text mining. Also, we introduce the generated process of documents and illustrate the graphical model for each topic models.},
	eventtitle = {2019 2nd International Conference on Artificial Intelligence and Big Data ({ICAIBD})},
	pages = {244--250},
	booktitle = {2019 2nd International Conference on Artificial Intelligence and Big Data ({ICAIBD})},
	author = {Xia, Linzhong and Luo, Dean and Zhang, Chunxiao and Wu, Zhou},
	date = {2019-05},
	keywords = {{LDA}, pattern classification, Analytical models, Probabilistic logic, Semantics, text analysis, Biological system modeling, data mining, Topic model, Data models, graphical model, Graphical models, joint models, latent dirichlet allocation, latent Dirichlet allocation, Text categorization, text classification, text mining, topic evolution model, topic evolution models},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\LPAVX967\\8836970.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\F9585JKL\\8836970.html:text/html;IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\X69YC6LQ\\8836970.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\LADYQ8RK\\Xia et al. - 2019 - A Survey of Topic Models in Text Classification.pdf:application/pdf}
}

@article{wang_st-sage_2017,
	title = {{ST}-{SAGE}: A Spatial-Temporal Sparse Additive Generative Model for Spatial Item Recommendation},
	volume = {8},
	issn = {2157-6904},
	url = {https://doi.org/10.1145/3011019},
	doi = {10.1145/3011019},
	shorttitle = {{ST}-{SAGE}},
	abstract = {With the rapid development of location-based social networks ({LBSNs}), spatial item recommendation has become an important mobile application, especially when users travel away from home. However, this type of recommendation is very challenging compared to traditional recommender systems. A user may visit only a limited number of spatial items, leading to a very sparse user-item matrix. This matrix becomes even sparser when the user travels to a distant place, as most of the items visited by a user are usually located within a short distance from the user’s home. Moreover, user interests and behavior patterns may vary dramatically across different time and geographical regions. In light of this, we propose {ST}-{SAGE}, a spatial-temporal sparse additive generative model for spatial item recommendation in this article. {ST}-{SAGE} considers both personal interests of the users and the preferences of the crowd in the target region at the given time by exploiting both the co-occurrence patterns and content of spatial items. To further alleviate the data-sparsity issue, {ST}-{SAGE} exploits the geographical correlation by smoothing the crowd’s preferences over a well-designed spatial index structure called the spatial pyramid. To speed up the training process of {ST}-{SAGE}, we implement a parallel version of the model inference algorithm on the {GraphLab} framework. We conduct extensive experiments; the experimental results clearly demonstrate that {ST}-{SAGE} outperforms the state-of-the-art recommender systems in terms of recommendation effectiveness, model training efficiency, and online recommendation efficiency.},
	pages = {48:1--48:25},
	number = {3},
	journaltitle = {{ACM} Transactions on Intelligent Systems and Technology},
	shortjournal = {{ACM} Trans. Intell. Syst. Technol.},
	author = {Wang, Weiqing and Yin, Hongzhi and Chen, Ling and Sun, Yizhou and Sadiq, Shazia and Zhou, Xiaofang},
	urldate = {2020-06-25},
	date = {2017-04-20},
	keywords = {efficient retrieval algorithm, location-based service, online learning, Point of interest ({POI}), real-time recommendation},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\S9CTP3I8\\Wang et al. - 2017 - ST-SAGE A Spatial-Temporal Sparse Additive Genera.pdf:application/pdf}
}

@article{hyland_latent_nodate,
	title = {Latent Dirichlet Allocation in Topic Modeling - A Bayesian treatment of text processing},
	pages = {73},
	author = {Hyland, Christopher},
	langid = {english},
	file = {Hyland - Latent Dirichlet Allocation in Topic Modeling - A .pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\BH4889SN\\Hyland - Latent Dirichlet Allocation in Topic Modeling - A .pdf:application/pdf}
}

@book{murphy_machine_2012,
	title = {Machine Learning: A Probabilistic Perspective},
	isbn = {978-0-262-01802-9},
	shorttitle = {Machine Learning},
	abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a {MATLAB} software package--{PMTK} (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
	pagetotal = {1096},
	publisher = {The {MIT} Press},
	author = {Murphy, Kevin P.},
	date = {2012}
}

@article{blei_variational_2006,
	title = {Variational inference for Dirichlet process mixtures},
	volume = {1},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1340371077},
	doi = {10.1214/06-BA104},
	abstract = {Dirichlet process ({DP}) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain ({MCMC}) sampling methods for {DP} mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, {MCMC} sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for {DP} mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for {DP} mixtures of Gaussians and present an application to a large-scale image analysis problem.},
	pages = {121--143},
	number = {1},
	journaltitle = {Bayesian Analysis},
	shortjournal = {Bayesian Anal.},
	author = {Blei, David M. and Jordan, Michael I.},
	urldate = {2020-06-25},
	date = {2006-03},
	mrnumber = {MR2227367},
	zmnumber = {1331.62259},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Bayesian computation, Dirichlet processes, hierarchical models, image processing, variational inference},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\WRCX4J8T\\Blei and Jordan - 2006 - Variational inference for Dirichlet process mixtur.pdf:application/pdf;Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\UPYBLCST\\1340371077.html:text/html}
}

@article{hoffman_stochastic_2013,
	title = {Stochastic variational inference},
	volume = {14},
	issn = {1532-4435},
	abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
	pages = {1303--1347},
	number = {1},
	journaltitle = {The Journal of Machine Learning Research},
	shortjournal = {J. Mach. Learn. Res.},
	author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
	date = {2013-05-01},
	keywords = {variational inference, Bayesian inference, Bayesian nonparametrics, stochastic optimization, topic models},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\8CWCAFH6\\Hoffman et al. - 2013 - Stochastic variational inference.pdf:application/pdf}
}

@inproceedings{zhai_mr_2012,
	location = {Lyon, France},
	title = {Mr. {LDA}: a flexible large scale topic modeling package using variational inference in {MapReduce}},
	isbn = {978-1-4503-1229-5},
	url = {http://dl.acm.org/citation.cfm?doid=2187836.2187955},
	doi = {10.1145/2187836.2187955},
	shorttitle = {Mr. {LDA}},
	eventtitle = {the 21st international conference},
	pages = {879--888},
	booktitle = {Proceedings of the 21st international conference on World Wide Web  - {WWW} '12},
	publisher = {{ACM} Press},
	author = {Zhai, Ke and Boyd-Graber, Jordan and Asadi, Nima and Alkhouja, Mohamad L.},
	urldate = {2020-06-25},
	date = {2012},
	langid = {english},
	file = {Zhai et al. - 2012 - Mr. LDA a flexible large scale topic modeling pac.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\W5UFYML5\\Zhai et al. - 2012 - Mr. LDA a flexible large scale topic modeling pac.pdf:application/pdf}
}

@article{boyd-graber_applications_2017,
	title = {Applications of Topic Models},
	volume = {11},
	issn = {1554-0669, 1554-0677},
	url = {http://www.nowpublishers.com/article/Details/INR-030},
	doi = {10.1561/1500000030},
	pages = {143--296},
	number = {2},
	journaltitle = {Foundations and Trends® in Information Retrieval},
	shortjournal = {{FNT} in Information Retrieval},
	author = {Boyd-Graber, Jordan and Hu, Yuening and Mimno, David},
	urldate = {2020-06-25},
	date = {2017},
	langid = {english},
	file = {Boyd-Graber et al. - 2017 - Applications of Topic Models.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\2JTIY3GV\\Boyd-Graber et al. - 2017 - Applications of Topic Models.pdf:application/pdf}
}

@article{hofmann_probabilistic_2013,
	title = {Probabilistic Latent Semantic Analysis},
	url = {http://arxiv.org/abs/1301.6705},
	abstract = {Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered {EM}. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.},
	journaltitle = {{arXiv}:1301.6705 [cs, stat]},
	author = {Hofmann, Thomas},
	urldate = {2020-06-25},
	date = {2013-01-23},
	eprinttype = {arxiv},
	eprint = {1301.6705},
	keywords = {Statistics - Machine Learning, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\AZCQQAZ7\\Hofmann - 2013 - Probabilistic Latent Semantic Analysis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\83JBQTHU\\1301.html:text/html}
}

@article{landauer_solution_1997,
	title = {A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge},
	volume = {104},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/0033-295X.104.2.211},
	shorttitle = {A solution to Plato's problem},
	abstract = {How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis ({LSA}), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, {LSA} acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. {LSA} uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena and problems are sketched. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {211--240},
	number = {2},
	journaltitle = {Psychological Review},
	author = {Landauer, Thomas K. and Dumais, Susan T.},
	date = {1997},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Semantics, Knowledge Level, Learning, Psycholinguistics, Theories},
	file = {Full Text:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\9S4KXFUM\\Landauer and Dumais - 1997 - A solution to Plato's problem The latent semantic.pdf:application/pdf;Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\HPRX6HIN\\1997-03612-001.html:text/html}
}

@online{noauthor_6931a_nodate,
	title = {6931A},
	url = {http://www.cse.ust.hk/~lzhang/teach/6931a/},
	urldate = {2020-06-26},
	file = {6931A:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\XHY9T9Q3\\6931a.html:text/html}
}

@online{noauthor_nevin_nodate,
	title = {Nevin L. Zhang},
	url = {http://www.cse.ust.hk/~lzhang/},
	urldate = {2020-06-26},
	file = {Nevin L. Zhang:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\KDJ7L9XH\\~lzhang.html:text/html}
}

@article{chang_relational_nodate,
	title = {Relational Topic Models for Document Networks},
	abstract = {We develop the relational topic model ({RTM}), a model of documents and the links between them. For each pair of documents, the {RTM} models their link as a binary random variable that is conditioned on their contents. The model can be used to summarize a network of documents, predict links between them, and predict words within them. We derive efﬁcient inference and learning algorithms based on variational methods and evaluate the predictive performance of the {RTM} for large networks of scientiﬁc abstracts and web documents.},
	pages = {8},
	author = {Chang, Jonathan and Blei, David M},
	langid = {english},
	file = {Chang and Blei - Relational Topic Models for Document Networks.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\AWFRNSMC\\Chang and Blei - Relational Topic Models for Document Networks.pdf:application/pdf}
}

@inproceedings{eren_covid-19_2020,
	title = {{COVID}-19 Literature Clustering},
	url = {https://github.com/MaksimEkin/COVID19-Literature-Clustering},
	author = {Eren, E. Maksim. Solovyev, Nick. Nicholas, Charles. Raff, Edward},
	date = {2020-04},
	note = {event-place: University of Maryland Baltimore County ({UMBC}), Baltimore, {MD}, {USA}}
}

@inproceedings{lau_machine_2014,
	location = {Gothenburg, Sweden},
	title = {Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality},
	url = {http://aclweb.org/anthology/E14-1056},
	doi = {10.3115/v1/E14-1056},
	shorttitle = {Machine Reading Tea Leaves},
	abstract = {Topic models based on latent Dirichlet allocation and related methods are used in a range of user-focused tasks including document navigation and trend analysis, but evaluation of the intrinsic quality of the topic model and topics remains an open research area. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation.},
	eventtitle = {Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics},
	pages = {530--539},
	booktitle = {Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Lau, Jey Han and Newman, David and Baldwin, Timothy},
	urldate = {2020-06-26},
	date = {2014},
	langid = {english},
	file = {Lau et al. - 2014 - Machine Reading Tea Leaves Automatically Evaluati.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\JRIVJAUB\\Lau et al. - 2014 - Machine Reading Tea Leaves Automatically Evaluati.pdf:application/pdf}
}

@article{dieng_topic_2019,
	title = {Topic Modeling in Embedding Spaces},
	url = {http://arxiv.org/abs/1907.04907},
	abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the Embedded Topic Model ({ETM}), a generative model of documents that marries traditional topic models with word embeddings. In particular, it models each word with a categorical distribution whose natural parameter is the inner product between a word embedding and an embedding of its assigned topic. To fit the {ETM}, we develop an efficient amortized variational inference algorithm. The {ETM} discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation ({LDA}), in terms of both topic quality and predictive performance.},
	journaltitle = {{arXiv}:1907.04907 [cs, stat]},
	author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
	urldate = {2020-06-26},
	date = {2019-07-07},
	eprinttype = {arxiv},
	eprint = {1907.04907},
	keywords = {Computer Science - Computation and Language, Statistics - Machine Learning, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\4QV64MTA\\Dieng et al. - 2019 - Topic Modeling in Embedding Spaces.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\JSYKQ6LG\\1907.html:text/html}
}

@article{bengio_representation_2014,
	title = {Representation Learning: A Review and New Perspectives},
	url = {http://arxiv.org/abs/1206.5538},
	shorttitle = {Representation Learning},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for {AI} is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	journaltitle = {{arXiv}:1206.5538 [cs]},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	urldate = {2020-06-28},
	date = {2014-04-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1206.5538},
	keywords = {Computer Science - Machine Learning},
	file = {1206.5538.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\CUR7537Y\\1206.5538.pdf:application/pdf}
}

@article{qiang_short_2020,
	title = {Short Text Topic Modeling Techniques, Applications, and Performance: A Survey},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2020.2992485},
	shorttitle = {Short Text Topic Modeling Techniques, Applications, and Performance},
	abstract = {Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., {PLSA} and {LDA}) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the first comprehensive open-source library, called {STTM}, for use in Java that integrates all surveyed algorithms within a unified interface, benchmark datasets, to facilitate the expansion of new methods in this research field. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Qiang, Jipeng and Qian, Zhenyu and Li, Yun and Yuan, Yunhao and Wu, Xindong},
	date = {2020},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Semantics, Topic modeling, Vocabulary, Libraries, Metadata, Short text, Short text topic modeling, Sparseness, Tagging, Task analysis, Twitter},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\RTWEKYHK\\9086136.html:text/html;Submitted Version:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\XYS9HNT4\\Qiang et al. - 2020 - Short Text Topic Modeling Techniques, Applications.pdf:application/pdf}
}

@article{vayansky_review_2020,
	title = {A review of topic modeling methods},
	issn = {0306-4379},
	url = {http://www.sciencedirect.com/science/article/pii/S0306437920300703},
	doi = {10.1016/j.is.2020.101582},
	abstract = {Topic modeling is a popular analytical tool for evaluating data. Numerous methods of topic modeling have been developed which consider many kinds of relationships and restrictions within datasets; however, these methods are not frequently employed. Instead many researchers gravitate to Latent Dirichlet Analysis, which although flexible and adaptive, is not always suited for modeling more complex data relationships. We present different topic modeling approaches capable of dealing with correlation between topics, the changes of topics over time, as well as the ability to handle short texts such as encountered in social media or sparse text data. We also briefly review the algorithms which are used to optimize and infer parameters in topic modeling, which is essential to producing meaningful results regardless of method. We believe this review will encourage more diversity when performing topic modeling and help determine what topic modeling method best suits the user needs.},
	pages = {101582},
	journaltitle = {Information Systems},
	shortjournal = {Information Systems},
	author = {Vayansky, Ike and Kumar, Sathish A. P.},
	urldate = {2020-06-28},
	date = {2020-06-18},
	langid = {english},
	keywords = {Topic modeling, Inference algorithms, Probabilistic Bayesian networks, Social Media analysis, Temporal analysis, Text analysis, Topic correlation},
	file = {ScienceDirect Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\XW7KUY7R\\S0306437920300703.html:text/html;Vayansky and Kumar - 2020 - A review of topic modeling methods.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\UZ3Y2P7I\\Vayansky and Kumar - 2020 - A review of topic modeling methods.pdf:application/pdf}
}

@online{noauthor__nodate,
	title = {東京大学附属図書館｜電子ジャーナル・電子ブックリスト},
	url = {https://gateway.itc.u-tokyo.ac.jp/,DanaInfo=vs2ga4mq9g.search.serialssolutions.com+?V=1.0&N=100&tab=ALL&L=VS2GA4MQ9G&S=T_W_A&C=MLP%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A7%E3%83%83%E3%82%B7%E3%83%A7%E3%83%8A%E3%83%AB%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA},
	urldate = {2020-07-05},
	file = {東京大学附属図書館｜電子ジャーナル・電子ブックリスト:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\A53WPJS4\\,DanaInfo=vs2ga4mq9g.search.serialssolutions.html:text/html}
}

@article{xun_correlated_2017,
	title = {A Correlated Topic Model Using Word Embeddings},
	abstract = {Conventional correlated topic models are able to capture correlation structure among latent topics by replacing the Dirichlet prior with the logistic normal distribution. Word embeddings have been proven to be able to capture semantic regularities in language. Therefore, the semantic relatedness and correlations between words can be directly calculated in the word embedding space, for example, via cosine values. In this paper, we propose a novel correlated topic model using word embeddings. The proposed model enables us to exploit the additional word-level correlation information in word embeddings and directly model topic correlation in the continuous word embedding space. In the model, words in documents are replaced with meaningful word embeddings, topics are modeled as multivariate Gaussian distributions over the word embeddings and topic correlations are learned among the continuous Gaussian topics. A Gibbs sampling solution with data augmentation is given to perform inference. We evaluate our model on the 20 Newsgroups dataset and the Reuters-21578 dataset qualitatively and quantitatively. The experimental results show the effectiveness of our proposed model.},
	pages = {7},
	author = {Xun, Guangxu and Li, Yaliang and Zhao, Wayne Xin and Gao, Jing and Zhang, Aidong},
	date = {2017},
	langid = {english},
	keywords = {read},
	file = {Xun et al. - A Correlated Topic Model Using Word Embeddings.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\3KT5FAH8\\Xun et al. - A Correlated Topic Model Using Word Embeddings.pdf:application/pdf}
}

@inproceedings{he_efficient_2017,
	location = {Halifax {NS} Canada},
	title = {Efficient Correlated Topic Modeling with Topic Embedding},
	isbn = {978-1-4503-4887-4},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098074},
	doi = {10.1145/3097983.3098074},
	abstract = {Correlated topic modeling has been limited to small model and problem sizes due to their high computational cost and poor scaling. In this paper, we propose a new model which learns compact topic embeddings and captures topic correlations through the closeness between the topic vectors. Our method enables e�cient inference in the low-dimensional embedding space, reducing previous cubic or quadratic time complexity to linear w.r.t the topic size. We further speedup variational inference with a fast sampler to exploit sparsity of topic occurrence. Extensive experiments show that our approach is capable of handling model and data scales which are several orders of magnitude larger than existing correlation results, without sacri�cing modeling quality by providing competitive or superior performance in document classi�cation and retrieval.},
	eventtitle = {{KDD} '17: The 23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {225--233},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {He, Junxian and Hu, Zhiting and Berg-Kirkpatrick, Taylor and Huang, Ying and Xing, Eric P.},
	urldate = {2020-07-05},
	date = {2017-08-04},
	langid = {english},
	file = {3097983.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\K2SYSQHH\\3097983.pdf:application/pdf;He et al. - 2017 - Efficient Correlated Topic Modeling with Topic Emb.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\BMUGLUU4\\He et al. - 2017 - Efficient Correlated Topic Modeling with Topic Emb.pdf:application/pdf}
}

@article{fu_anchor-free_2019,
	title = {Anchor-Free Correlated Topic Modeling},
	volume = {41},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2018.2827377},
	abstract = {In topic modeling, identifiability of the topics is an essential issue. Many topic modeling approaches have been developed under the premise that each topic has a characteristic anchor word that only appears in that topic. The anchor-word assumption is fragile in practice, because words and terms have multiple uses; yet it is commonly adopted because it enables identifiability guarantees. Remedies in the literature include using three- or higher-order word co-occurrence statistics to come up with tensor factorization models, but such statistics need many more samples to obtain reliable estimates, and identifiability still hinges on additional assumptions, such as consecutive words being persistently drawn from the same topic. In this work, we propose a new topic identification criterion using second order statistics of the words. The criterion is theoretically guaranteed to identify the underlying topics even when the anchor-word assumption is grossly violated. An algorithm based on alternating optimization, and an efficient primal-dual algorithm are proposed to handle the resulting identification problem. The former exhibits high performance and is completely parameter-free; the latter affords up to 200 times speedup relative to the former, but requires step-size tuning and a slight sacrifice in accuracy. A variety of real text corpora are employed to showcase the effectiveness of the approach, where the proposed anchor-free method demonstrates substantial improvements compared to a number of anchor-word based approaches under various evaluation metrics.},
	pages = {1056--1071},
	number = {5},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Fu, Xiao and Huang, Kejun and Sidiropoulos, Nicholas D. and Shi, Qingjiang and Hong, Mingyi},
	date = {2019-05},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Analytical models, optimisation, Optimization, Correlation, text analysis, Topic modeling, Biological system modeling, data mining, Data models, alternating optimization, anchor free, anchor-free correlated topic modeling, anchor-word assumption, anchor-word based approaches, convergence of numerical methods, Games, higher-order word co-occurrence statistics, identifiability, matrix decomposition, non-convex optimization, nonnegative matrix factorization, primal-dual algorithm, second order statistics, statistical analysis, sufficiently scattered, Tensile stress, tensor factorization models, tensors, topic identification criterion, word processing},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\ISMCELE4\\8338424.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\F47JQQSD\\Fu et al. - 2019 - Anchor-Free Correlated Topic Modeling.pdf:application/pdf}
}

@article{zhang_advances_2019,
	title = {Advances in Variational Inference},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8588399/},
	doi = {10.1109/TPAMI.2018.2889774},
	abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference ({VI}) lets us approximate a highdimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We ﬁrst introduce standard mean ﬁeld variational inference, then review recent advances focusing on the following aspects: (a) scalable {VI}, which includes stochastic approximations, (b) generic {VI}, which extends the applicability of {VI} to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate {VI}, which includes variational models beyond the mean ﬁeld approximation or with atypical divergences, and (d) amortized {VI}, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
	pages = {2008--2026},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
	urldate = {2020-07-08},
	date = {2019-08-01},
	langid = {english},
	file = {08588399.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\7QDPBDVG\\08588399.pdf:application/pdf}
}

@inbook{anandarajan_probabilistic_2019,
	location = {Cham},
	title = {Probabilistic Topic Models},
	volume = {2},
	isbn = {978-3-319-95662-6 978-3-319-95663-3},
	url = {http://link.springer.com/10.1007/978-3-319-95663-3_8},
	abstract = {In this chapter, the reader is introduced to an unsupervised, probabilistic analysis model known as topic models. In topic models, the full {TDM} (or {DTM}) is broken down into two major components: the topic distribution over terms and the document distribution over topics. The topic models introduced in this chapter include latent Dirichlet allocation, dynamic topic models, correlated topic models, supervised latent Dirichlet allocation, and structural topic models. Finally, decision-­ making and topic model validation are presented.},
	pages = {117--130},
	booktitle = {Practical Text Analytics},
	publisher = {Springer International Publishing},
	author = {Anandarajan, Murugan and Hill, Chelsey and Nolan, Thomas},
	bookauthor = {Anandarajan, Murugan and Hill, Chelsey and Nolan, Thomas},
	urldate = {2020-07-10},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-319-95663-3_8},
	note = {Series Title: Advances in Analytics and Data Science},
	file = {Anandarajan et al. - 2019 - Probabilistic Topic Models.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\9QHBYCL9\\Anandarajan et al. - 2019 - Probabilistic Topic Models.pdf:application/pdf}
}

@inproceedings{liu_neural_2019,
	location = {San Francisco, {CA}, {USA}},
	title = {Neural Variational Correlated Topic Modeling},
	isbn = {978-1-4503-6674-8},
	url = {https://doi.org/10.1145/3308558.3313561},
	doi = {10.1145/3308558.3313561},
	series = {{WWW} '19},
	abstract = {With the rapid development of the Internet, millions of documents, such as news and web pages, are generated everyday. Mining the topics and knowledge on them has attracted a lot of interest on both academic and industrial areas. As one of the prevalent unsupervised data mining tools, topic models are usually explored as probabilistic generative models for large collections of texts. Traditional probabilistic topic models tend to find a closed form solution of model parameters and approach the intractable posteriors via approximation methods, which usually lead to the inaccurate inference of parameters and low efficiency when it comes to a quite large volume of data. Recently, an emerging trend of neural variational inference can overcome the above issues, which offers a scalable and powerful deep generative framework for modeling latent topics via neural networks. Interestingly, a common assumption for the most neural variational topic models is that topics are independent and irrelevant to each other. However, this assumption is unreasonable in many practical scenarios. In this paper, we propose a novel Centralized Transformation Flow to capture the correlations among topics by reshaping topic distributions. Furthermore, we present the Transformation Flow Lower Bound to improve the performance of the proposed model. Extensive experiments on two standard benchmark datasets have well-validated the effectiveness of the proposed approach.},
	pages = {1142--1152},
	booktitle = {The World Wide Web Conference},
	publisher = {Association for Computing Machinery},
	author = {Liu, Luyang and Huang, Heyan and Gao, Yang and Zhang, Yongfeng and Wei, Xiaochi},
	urldate = {2020-07-10},
	date = {2019-05-13},
	keywords = {Natural language processing;topic model;neural variational inference},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\AS83ECD7\\Liu et al. - 2019 - Neural Variational Correlated Topic Modeling.pdf:application/pdf}
}

@article{tomasi_stochastic_nodate,
	title = {Stochastic Variational Inference for Dynamic Correlated Topic Models},
	abstract = {Correlated topic models ({CTM}) are useful tools for statistical analysis of documents. They explicitly capture the correlation between topics associated with each document. We propose an extension to {CTM} that models the evolution of both topic correlation and word cooccurrence over time. This allows us to identify the changes of topic correlations over time, e.g., in the machine learning literature, the correlation between the topics “stochastic gradient descent” and “variational inference” increased in the last few years due to advances in stochastic variational inference methods. Our temporal dynamic priors are based on Gaussian processes ({GPs}), allowing us to capture diverse temporal behaviours such as smooth, with long-term memory, temporally concentrated, and periodic. The evolution of topic correlations is modeled through generalised Wishart processes ({GWPs}). We develop a stochastic variational inference method, which enables us to handle large sets of continuous temporal data. Our experiments applied to real world data demonstrate that our model can be used to effectively discover temporal patterns of topic distributions, words associated to topics and topic relationships.},
	pages = {10},
	author = {Tomasi, Federico and Ravichandran, Praveen and Levy-Fix, Gal and Lalmas, Mounia and Dai, Zhenwen},
	langid = {english},
	file = {Tomasi et al. - Stochastic Variational Inference for Dynamic Corre.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\2MYVWJ6D\\Tomasi et al. - Stochastic Variational Inference for Dynamic Corre.pdf:application/pdf;Tomasi et al. - Stochastic Variational Inference for Dynamic Corre.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\HZLR76VT\\Tomasi et al. - Stochastic Variational Inference for Dynamic Corre.pdf:application/pdf}
}

@article{zhou_graph_2019,
	title = {Graph Neural Networks: A Review of Methods and Applications},
	url = {http://arxiv.org/abs/1812.08434},
	shorttitle = {Graph Neural Networks},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular ﬁngerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks ({GNNs}) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive {GNNs} have been found difﬁcult to train for a ﬁxed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network ({GCN}), graph attention network ({GAT}), gated graph neural network ({GGNN}) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
	journaltitle = {{arXiv}:1812.08434 [cs, stat]},
	author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	urldate = {2020-07-11},
	date = {2019-07-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.08434},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Zhou et al. - 2019 - Graph Neural Networks A Review of Methods and App.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\JDBZUXDN\\Zhou et al. - 2019 - Graph Neural Networks A Review of Methods and App.pdf:application/pdf}
}

@article{hamilton_representation_2018,
	title = {Representation Learning on Graphs: Methods and Applications},
	url = {http://arxiv.org/abs/1709.05584},
	shorttitle = {Representation Learning on Graphs},
	abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is ﬁnding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-deﬁned heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a uniﬁed framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
	journaltitle = {{arXiv}:1709.05584 [cs]},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	urldate = {2020-07-11},
	date = {2018-04-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1709.05584},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	file = {Hamilton et al. - 2018 - Representation Learning on Graphs Methods and App.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\JEKZ7YV2\\Hamilton et al. - 2018 - Representation Learning on Graphs Methods and App.pdf:application/pdf}
}

@inproceedings{salomatin_multi-field_2009,
	title = {Multi-field Correlated Topic Modeling},
	isbn = {978-0-89871-682-5 978-1-61197-279-5},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972795.54},
	doi = {10.1137/1.9781611972795.54},
	abstract = {Popular methods for probabilistic topic modeling like the Latent Dirichlet Allocation ({LDA}, [1]) and Correlated Topic Models ({CTM}, [2]) share an important property, i.e., using a common set of topics to model all the data. This property can be too restrictive for modeling complex data entries where multiple ﬁelds of heterogeneous data jointly provide rich information about each object or event. We propose a new extension of the {CTM} method to enable modeling with multi-ﬁeld topics in a global graphical structure, and a mean-ﬁeld variational algorithm to allow joint learning of multinomial topic models from discrete data and Gaussianstyle topic models for real-valued data. We conducted experiments with both simulated and real data, and observed that the multi-ﬁeld {CTM} outperforms a conventional {CTM} in both likelihood maximization and perplexity reduction. A deeper analysis on the simulated data reveals that the superior performance is the result of successful discovery of the mapping among ﬁeld-speciﬁc topics and observed data.},
	eventtitle = {Proceedings of the 2009 {SIAM} International Conference on Data Mining},
	pages = {628--637},
	booktitle = {Proceedings of the 2009 {SIAM} International Conference on Data Mining},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Salomatin, Konstantin and Yang, Yiming and Lad, Abhimanyu},
	urldate = {2020-07-11},
	date = {2009-04-30},
	langid = {english},
	file = {Salomatin et al. - 2009 - Multi-field Correlated Topic Modeling.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\3GFJXY8J\\Salomatin et al. - 2009 - Multi-field Correlated Topic Modeling.pdf:application/pdf}
}

@article{moody_mixing_2016,
	title = {Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec},
	url = {http://arxiv.org/abs/1605.02019},
	abstract = {Distributed dense word vectors have been shown to be effective at capturing tokenlevel semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.},
	journaltitle = {{arXiv}:1605.02019 [cs]},
	author = {Moody, Christopher E.},
	urldate = {2020-07-11},
	date = {2016-05-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1605.02019},
	keywords = {Computer Science - Computation and Language},
	file = {1605.02019.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\US62GZ9M\\1605.02019.pdf:application/pdf}
}

@inproceedings{yang_graph_2020,
	location = {Taipei Taiwan},
	title = {Graph Attention Topic Modeling Network},
	isbn = {978-1-4503-7023-3},
	url = {https://dl.acm.org/doi/10.1145/3366423.3380102},
	doi = {10.1145/3366423.3380102},
	eventtitle = {{WWW} '20: The Web Conference 2020},
	pages = {144--154},
	booktitle = {Proceedings of The Web Conference 2020},
	publisher = {{ACM}},
	author = {Yang, Liang and Wu, Fan and Gu, Junhua and Wang, Chuan and Cao, Xiaochun and Jin, Di and Guo, Yuanfang},
	urldate = {2020-07-11},
	date = {2020-04-20},
	langid = {english},
	file = {Yang et al. - 2020 - Graph Attention Topic Modeling Network.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\UJNQQGEF\\Yang et al. - 2020 - Graph Attention Topic Modeling Network.pdf:application/pdf}
}

@inproceedings{lee_practical_2019,
	location = {Hong Kong, China},
	title = {Practical Correlated Topic Modeling and Analysis via the Rectified Anchor Word Algorithm},
	url = {https://www.aclweb.org/anthology/D19-1504},
	doi = {10.18653/v1/D19-1504},
	abstract = {Despite great scalability on large data and their ability to understand correlations between topics, spectral topic models have not been widely used due to the absence of reliability in real data and lack of practical implementations. This paper aims to solidify the foundations of spectral topic inference and provide a practical implementation for anchor-based topic modeling. Beginning with vocabulary curation, we scrutinize every single inference step with other viable options. We also evaluate our matrix-based approach against popular alternatives including a tensor-based spectral method as well as probabilistic algorithms. Our quantitative and qualitative experiments demonstrate the power of Rectiﬁed Anchor Word algorithm in various real datasets, providing a complete guide to practical correlated topic modeling.},
	eventtitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	pages = {4990--5000},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Moontae and Cho, Sungjun and Bindel, David and Mimno, David},
	urldate = {2020-07-11},
	date = {2019},
	langid = {english},
	file = {Lee et al. - 2019 - Practical Correlated Topic Modeling and Analysis v.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\6V7INS6D\\Lee et al. - 2019 - Practical Correlated Topic Modeling and Analysis v.pdf:application/pdf}
}

@article{liu_correlated_2018,
	title = {Correlated Topic Modelling via Householder Flow},
	abstract = {Topic models can be one of the prevalent unsupervised learning approaches in natural language processing. Recent works on neural variational inference offer a powerful framework combining neural networks and interpretable probability models. However, one fundamental assumption is that topics in the latent space are independent to each other, which is actually not the case in the reality. In this paper, we propose the Correlated Householder Topic Model ({CHTM}) to capture the correlations among topics, and model them via Householder flow. The experiments show that ,by incorporating topic correlation, {CHTM} outperforms baseline methods on two standard datasets.},
	pages = {5},
	author = {Liu, Luyang and Huang, Heyan and Gao, Yang},
	date = {2018},
	langid = {english},
	keywords = {read},
	file = {Liu et al. - 2018 - Correlated Topic Modelling via Householder Flow.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\5BYZEVGN\\Liu et al. - 2018 - Correlated Topic Modelling via Householder Flow.pdf:application/pdf;Liu et al. - 2018 - Correlated Topic Modelling via Householder Flow.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\7T9KJNZS\\Liu et al. - 2018 - Correlated Topic Modelling via Householder Flow.pdf:application/pdf}
}

@article{fox_tutorial_2012,
	title = {A tutorial on variational Bayesian inference},
	volume = {38},
	issn = {0269-2821, 1573-7462},
	url = {http://link.springer.com/10.1007/s10462-011-9236-8},
	doi = {10.1007/s10462-011-9236-8},
	abstract = {This tutorial describes the mean-ﬁeld variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to ﬁnd an approximate meanﬁeld distribution close to the target joint in the {KL}-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
	pages = {85--95},
	number = {2},
	journaltitle = {Artificial Intelligence Review},
	shortjournal = {Artif Intell Rev},
	author = {Fox, Charles W. and Roberts, Stephen J.},
	urldate = {2020-07-13},
	date = {2012-08},
	langid = {english},
	file = {Fox and Roberts - 2012 - A tutorial on variational Bayesian inference.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\LSYSYCP3\\Fox and Roberts - 2012 - A tutorial on variational Bayesian inference.pdf:application/pdf}
}

@inproceedings{xun_collaboratively_2017,
	location = {Halifax {NS} Canada},
	title = {Collaboratively Improving Topic Discovery and Word Embeddings by Coordinating Global and Local Contexts},
	isbn = {978-1-4503-4887-4},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098009},
	doi = {10.1145/3097983.3098009},
	abstract = {A text corpus typically contains two types of context information – global context and local context. Global context carries topical information which can be utilized by topic models to discover topic structures from the text corpus, while local context can train word embeddings to capture semantic regularities re ected in the text corpus. This encourages us to exploit the useful information in both the global and the local context information. In this paper, we propose a uni ed language model based on matrix factorization techniques which 1) takes the complementary global and local context information into consideration simultaneously, and 2) models topics and learns word embeddings collaboratively. We empirically show that by incorporating both global and local context, this collaborative model can not only signi cantly improve the performance of topic discovery over the baseline topic models, but also learn better word embeddings than the baseline word embedding models. We also provide qualitative analysis that explains how the cooperation of global and local context information can result in better topic structures and word embeddings.},
	eventtitle = {{KDD} '17: The 23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	pages = {535--543},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Xun, Guangxu and Li, Yaliang and Gao, Jing and Zhang, Aidong},
	urldate = {2020-07-28},
	date = {2017-08-04},
	langid = {english},
	file = {,DanaInfo=dl.acm.org,SSL+3097983.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\C3I2FT3D\\,DanaInfo=dl.acm.org,SSL+3097983.pdf:application/pdf}
}

@inproceedings{das_gaussian_2015,
	location = {Beijing, China},
	title = {Gaussian {LDA} for Topic Models with Word Embeddings},
	url = {http://aclweb.org/anthology/P15-1077},
	doi = {10.3115/v1/P15-1077},
	abstract = {Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at capturing semantic regularities in language. In this paper we replace {LDA}’s parameterization of “topics” as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space. This encourages the model to group words that are a priori known to be semantically related into topics. To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decompositions of covariance matrices of the posterior predictive distributions. We further derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis–Hastings step. Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and {NIPS}). Qualitatively, Gaussian {LDA} infers different (but still very sensible) topics relative to standard {LDA}. Quantitatively, our technique outperforms existing models at dealing with {OOV} words in held-out documents.},
	eventtitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	pages = {795--804},
	booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Das, Rajarshi and Zaheer, Manzil and Dyer, Chris},
	urldate = {2020-07-28},
	date = {2015},
	langid = {english},
	file = {Das et al. - 2015 - Gaussian LDA for Topic Models with Word Embeddings.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\U8FVR5SE\\Das et al. - 2015 - Gaussian LDA for Topic Models with Word Embeddings.pdf:application/pdf}
}

@article{kamyab_multilingual_nodate,
	title = {Multilingual Gaussian Latent Dirichlet Allocation},
	abstract = {Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in corpus data. Depending on basic assumptions, there are different probabilistic algorithms for this purpose. One way to achieve this goal is to use the semantic relations between the words in the documents.},
	pages = {64},
	author = {Kamyab, Elias},
	langid = {english},
	file = {Kamyab - Multilingual Gaussian Latent Dirichlet Allocation.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\U8EGEBJ6\\Kamyab - Multilingual Gaussian Latent Dirichlet Allocation.pdf:application/pdf}
}

@article{griffiths_finding_2004,
	title = {Finding scientific topics},
	volume = {101},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0307752101},
	doi = {10.1073/pnas.0307752101},
	pages = {5228--5235},
	issue = {Supplement 1},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proceedings of the National Academy of Sciences},
	author = {Griffiths, T. L. and Steyvers, M.},
	urldate = {2020-08-03},
	date = {2004-04-06},
	langid = {english},
	file = {Griffiths and Steyvers - 2004 - Finding scientific topics.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\4CP3VJK3\\Griffiths and Steyvers - 2004 - Finding scientific topics.pdf:application/pdf}
}

@inproceedings{shi_jointly_2017,
	location = {New York, {NY}, {USA}},
	title = {Jointly Learning Word Embeddings and Latent Topics},
	isbn = {978-1-4503-5022-8},
	url = {https://doi.org/10.1145/3077136.3080806},
	doi = {10.1145/3077136.3080806},
	series = {{SIGIR} '17},
	abstract = {Word embedding models such as Skip-gram learn a vector-space representation for each word, based on the local word collocation patterns that are observed in a text corpus. Latent topic models, on the other hand, take a more global view, looking at the word distributions across the corpus to assign a topic to each word occurrence. These two paradigms are complementary in how they represent the meaning of word occurrences. While some previous works have already looked at using word embeddings for improving the quality of latent topics, and conversely, at using latent topics for improving word embeddings, such "two-step'' methods cannot capture the mutual interaction between the two paradigms. In this paper, we propose {STE}, a framework which can learn word embeddings and latent topics in a unified manner. {STE} naturally obtains topic-specific word embeddings, and thus addresses the issue of polysemy. At the same time, it also learns the term distributions of the topics, and the topic distributions of the documents. Our experimental results demonstrate that the {STE} model can indeed generate useful topic-specific word embeddings and coherent latent topics in an effective and efficient way.},
	pages = {375--384},
	booktitle = {Proceedings of the 40th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Shi, Bei and Lam, Wai and Jameel, Shoaib and Schockaert, Steven and Lai, Kwun Ping},
	urldate = {2020-08-08},
	date = {2017-08-07},
	keywords = {document modeling, topic model, word embedding},
	file = {Accepted Version:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\ZD5TWXEY\\Shi et al. - 2017 - Jointly Learning Word Embeddings and Latent Topics.pdf:application/pdf}
}

@inproceedings{li_topic_2016,
	location = {New York, {NY}, {USA}},
	title = {Topic Modeling for Short Texts with Auxiliary Word Embeddings},
	isbn = {978-1-4503-4069-4},
	url = {https://doi.org/10.1145/2911451.2911499},
	doi = {10.1145/2911451.2911499},
	series = {{SIGIR} '16},
	abstract = {For many applications that require semantic understanding of short texts, inferring discriminative and coherent latent topics from short texts is a critical and fundamental task. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Data sparsity therefore becomes a bottleneck for conventional topic models to achieve good results on short texts. On the other hand, when a human being interprets a piece of short text, the understanding is not solely based on its content words, but also her background knowledge (e.g., semantically related words). The recent advances in word embedding offer effective learning of word semantic relations from a large corpus. Exploiting such auxiliary word embeddings to enrich topic modeling for short texts is the main focus of this paper. To this end, we propose a simple, fast, and effective topic model for short texts, named {GPU}-{DMM}. Based on the Dirichlet Multinomial Mixture ({DMM}) model, {GPU}-{DMM} promotes the semantically related words under the same topic during the sampling process by using the generalized Polya urn ({GPU}) model. In this sense, the background knowledge about word semantic relatedness learned from millions of external documents can be easily exploited to improve topic modeling for short texts. Through extensive experiments on two real-world short text collections in two languages, we show that {GPU}-{DMM} achieves comparable or better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to the best accuracy in text classification task, which is used as an indirect evaluation.},
	pages = {165--174},
	booktitle = {Proceedings of the 39th International {ACM} {SIGIR} conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Li, Chenliang and Wang, Haoran and Zhang, Zhiqian and Sun, Aixin and Ma, Zongyang},
	urldate = {2020-08-08},
	date = {2016-07-07},
	keywords = {topic model, short texts, word embeddings}
}

@inproceedings{liu_topical_2015,
	location = {Austin, Texas},
	title = {Topical word embeddings},
	isbn = {978-0-262-51129-2},
	series = {{AAAI}'15},
	abstract = {Most word embedding models typically represent each word using a single vector, which makes these models indiscriminative for ubiquitous homonymy and polysemy. In order to enhance discriminativeness, we employ latent topic models to assign topics for each word in the text corpus, and learn topical word embeddings ({TWE}) based on both words and their topics. In this way, contextual word embeddings can be flexibly obtained to measure contextual word similarity. We can also build document representations, which are more expressive than some widely-used document models such as latent topic models. In the experiments, we evaluate the {TWE} models on two tasks, contextual word similarity and text classification. The experimental results show that our models outperform typical word embedding models including the multi-prototype version on contextual word similarity, and also exceed latent topic models and other representative document models on text classification. The source code of this paper can be obtained from https://github.com/largelymfs/topical\_word\_embeddings.},
	pages = {2418--2424},
	booktitle = {Proceedings of the Twenty-Ninth {AAAI} Conference on Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Liu, Yang and Liu, Zhiyuan and Chua, Tat-Seng and Sun, Maosong},
	urldate = {2020-08-08},
	date = {2015-01-25}
}

@article{steyvers_probabilistic_nodate,
	title = {Probabilistic Topic Models},
	pages = {15},
	author = {Steyvers, Mark and Griffiths, Tom},
	langid = {english},
	file = {Steyvers and Griffiths - Probabilistic Topic Models.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\VA8YAVE6\\Steyvers and Griffiths - Probabilistic Topic Models.pdf:application/pdf}
}

@article{dieng_topic_2020,
	title = {Topic Modeling in Embedding Spaces},
	volume = {8},
	issn = {2307-387X},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00325},
	doi = {10.1162/tacl_a_00325},
	abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavytailed vocabularies. To this end, we develop the embedded topic model ({ETM}), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the {ETM} models each word with a categorical distribution whose natural parameter is the inner product between the word’s embedding and an embedding of its assigned topic. To fit the {ETM}, we develop an efficient amortized variational inference algorithm. The {ETM} discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.},
	pages = {439--453},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
	urldate = {2020-08-09},
	date = {2020-07},
	langid = {english},
	file = {Dieng et al. - 2020 - Topic Modeling in Embedding Spaces.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\IME3G3SK\\Dieng et al. - 2020 - Topic Modeling in Embedding Spaces.pdf:application/pdf}
}

@unpublished{christopher_moody_word2vec_nodate,
	title = {word2vec, {LDA}, and introducing a new hybrid algorithm: lda2vec},
	url = {https://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994},
	shorttitle = {word2vec, {LDA}, and introducing a new hybrid algorithm},
	abstract = {Available with notes:
http://www.slideshare.net/{ChristopherMoody}3/word2vec-},
	type = {Science},
	howpublished = {Science},
	author = {Christopher Moody},
	urldate = {2020-08-10}
}

@article{hoyer_non-negative_nodate,
	title = {Non-negative Matrix Factorization with Sparseness Constraints},
	abstract = {Non-negative matrix factorization ({NMF}) is a recently developed technique for ﬁnding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of ‘sparseness’ improves the found decompositions. Additionally, we provide complete {MATLAB} code both for standard {NMF} and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.},
	pages = {13},
	author = {Hoyer, Patrik O and Hoyer, Patrik},
	langid = {english},
	file = {Hoyer and Hoyer - Non-negative Matrix Factorization with Sparseness .pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\HVVEHMCL\\Hoyer and Hoyer - Non-negative Matrix Factorization with Sparseness .pdf:application/pdf}
}

@article{cai_graph_2011,
	title = {Graph Regularized Nonnegative Matrix Factorization for Data Representation},
	volume = {33},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2010.231},
	abstract = {Matrix factorization techniques have been frequently applied in information retrieval, computer vision, and pattern recognition. Among them, Nonnegative Matrix Factorization ({NMF}) has received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts based in the human brain. On the other hand, from the geometric perspective, the data is usually sampled from a low-dimensional manifold embedded in a high-dimensional ambient space. One then hopes to find a compact representation,which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure. In this paper, we propose a novel algorithm, called Graph Regularized Nonnegative Matrix Factorization ({GNMF}), for this purpose. In {GNMF}, an affinity graph is constructed to encode the geometrical information and we seek a matrix factorization, which respects the graph structure. Our empirical study shows encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms on real-world problems.},
	pages = {1548--1560},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cai, Deng and He, Xiaofei and Han, Jiawei and Huang, Thomas S.},
	date = {2011-08},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Algorithm design and analysis, Clustering algorithms, Matrix decomposition, matrix decomposition, Algorithms, Brain, Cluster Analysis, clustering., computational geometry, computer vision, data representation, data structures, geometric perspective, graph Laplacian, graph regularized nonnegative matrix factorization, graph theory, high dimensional ambient space, Humans, Image Processing, Computer-Assisted, information retrieval, Linear approximation, low dimensional manifold, manifold regularization, Manifolds, Models, Biological, Models, Statistical, Nearest neighbor searches, Nonnegative matrix factorization, pattern recognition},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\7ZIPLMC2\\5674058.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\SPHTJ67V\\Cai et al. - 2011 - Graph Regularized Nonnegative Matrix Factorization.pdf:application/pdf}
}

@article{xu_distilled_2018,
	title = {Distilled Wasserstein Learning for Word Embedding and Topic Modeling},
	abstract = {We propose a novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics. The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model. The word distributions of topics, their optimal transports to the word distributions of documents, and the embeddings of words are learned in a uniﬁed framework. When learning the topic model, we leverage a distilled underlying distance matrix to update the topic distributions and smoothly calculate the corresponding optimal transports. Such a strategy provides the updating of word embeddings with robust guidance, improving the algorithmic convergence. As an application, we focus on patient admission records, in which the proposed method embeds the codes of diseases and procedures and learns the topics of admissions, obtaining superior performance on clinically-meaningful disease network construction, mortality prediction as a function of admission codes, and procedure recommendation.},
	pages = {10},
	author = {Xu, Hongteng and Wang, Wenlin and Liu, Wei and Carin, Lawrence},
	date = {2018},
	langid = {english},
	file = {Xu et al. - Distilled Wasserstein Learning for Word Embedding .pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\4MQXA8X3\\Xu et al. - Distilled Wasserstein Learning for Word Embedding .pdf:application/pdf}
}

@article{xu_gromov-wasserstein_2019,
	title = {Gromov-Wasserstein Learning for Graph Matching and Node Embedding},
	url = {http://arxiv.org/abs/1901.06003},
	abstract = {A novel Gromov-Wasserstein learning framework is proposed to jointly match (align) graphs and learn embedding vectors for the associated graph nodes. Using Gromov-Wasserstein discrepancy, we measure the dissimilarity between two graphs and ﬁnd their correspondence, according to the learned optimal transport. The node embeddings associated with the two graphs are learned under the guidance of the optimal transport, the distance of which not only reﬂects the topological structure of each graph but also yields the correspondence across the graphs. These two learning steps are mutually-beneﬁcial, and are uniﬁed here by minimizing the Gromov-Wasserstein discrepancy with structural regularizers. This framework leads to an optimization problem that is solved by a proximal point method. We apply the proposed method to matching problems in real-world networks, and demonstrate its superior performance compared to alternative approaches.},
	journaltitle = {{arXiv}:1901.06003 [cs, stat]},
	author = {Xu, Hongteng and Luo, Dixin and Zha, Hongyuan and Carin, Lawrence},
	urldate = {2020-08-10},
	date = {2019-05-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1901.06003},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	file = {Xu et al. - 2019 - Gromov-Wasserstein Learning for Graph Matching and.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\T24PWNZI\\Xu et al. - 2019 - Gromov-Wasserstein Learning for Graph Matching and.pdf:application/pdf}
}

@inproceedings{batmanghelich_nonparametric_2016,
	location = {Berlin, Germany},
	title = {Nonparametric Spherical Topic Modeling with Word Embeddings},
	url = {http://aclweb.org/anthology/P16-2087},
	doi = {10.18653/v1/P16-2087},
	eventtitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	pages = {537--542},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Batmanghelich, Kayhan and Saeedi, Ardavan and Narasimhan, Karthik and Gershman, Sam},
	urldate = {2020-08-10},
	date = {2016},
	langid = {english},
	file = {Batmanghelich et al. - 2016 - Nonparametric Spherical Topic Modeling with Word E.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\N7G45TXX\\Batmanghelich et al. - 2016 - Nonparametric Spherical Topic Modeling with Word E.pdf:application/pdf}
}

@article{straub_bayesian_2016,
	title = {Bayesian Inference with the von-Mises-Fisher Distribution in 3D},
	abstract = {In this writeup, I give an introduction to the von-Mises-Fisher ({vMF}) distribution which is a commonly used isotropic distribution for directional data. The writeup is an excerpt of my {PhD} thesis [10] with a focus on Bayesian inference and computational considerations when working with the {vMF} distribution. While the initial discussion is general, some of the results and derivations for efﬁcient inference are specialized to 3D directional data. Speciﬁcally, after the introduction of the {vMF} distribution and two different conjugate prior distributions, I outline general sampling from the posterior {vMF} distribution before deriving the normalization of the prior and the marginal data distribution for 3D. The last two sections show the cumulative density function and the entropy for the 3D {vMF} distribution.},
	pages = {10},
	author = {Straub, Julian},
	date = {2016},
	langid = {english},
	file = {Straub - Bayesian Inference with the von-Mises-Fisher Distr.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\Z5ZQ3SUE\\Straub - Bayesian Inference with the von-Mises-Fisher Distr.pdf:application/pdf}
}

@article{sra_directional_2016,
	title = {Directional Statistics in Machine Learning: a Brief Review},
	url = {http://arxiv.org/abs/1605.00316},
	shorttitle = {Directional Statistics in Machine Learning},
	abstract = {The modern data analyst must cope with data encoded in various forms, vectors, matrices, strings, graphs, or more. Consequently, statistical and machine learning models tailored to different data encodings are important. We focus on data encoded as normalized vectors, so that their “direction” is more important than their magnitude. Speciﬁcally, we consider high-dimensional vectors that lie either on the surface of the unit hypersphere or on the real projective plane. For such data, we brieﬂy review common mathematical models prevalent in machine learning, while also outlining some technical aspects, software, applications, and open mathematical challenges.},
	journaltitle = {{arXiv}:1605.00316 [stat]},
	author = {Sra, Suvrit},
	urldate = {2020-08-10},
	date = {2016-05-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1605.00316},
	keywords = {Statistics - Machine Learning},
	file = {Sra - 2016 - Directional Statistics in Machine Learning a Brie.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\D8D2PRW5\\Sra - 2016 - Directional Statistics in Machine Learning a Brie.pdf:application/pdf}
}

@online{noauthor_ieee_nodate,
	title = {{IEEE} Xplore Full-Text {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7015568&casa_token=YA4DDRXYo3AAAAAA:tJp6C_ZDQBaLGAAc0KonyW3_uTivjpm2tpBDAWLsXXJqnwdYD-Nwgzl-NBssypdwZ5zcEiaFDg},
	urldate = {2020-08-10},
	file = {IEEE Xplore Full-Text PDF\::C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\HFKBT2BC\\stamp.html:text/html}
}

@article{xuan_topic_2015,
	title = {Topic Model for Graph Mining},
	volume = {45},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2014.2386282},
	abstract = {Graph mining has been a popular research area because of its numerous application scenarios. Many unstructured and structured data can be represented as graphs, such as, documents, chemical molecular structures, and images. However, an issue in relation to current research on graphs is that they cannot adequately discover the topics hidden in graph-structured data which can be beneficial for both the unsupervised learning and supervised learning of the graphs. Although topic models have proved to be very successful in discovering latent topics, the standard topic models cannot be directly applied to graph-structured data due to the “bag-of-word” assumption. In this paper, an innovative graph topic model ({GTM}) is proposed to address this issue, which uses Bernoulli distributions to model the edges between nodes in a graph. It can, therefore, make the edges in a graph contribute to latent topic discovery and further improve the accuracy of the supervised and unsupervised learning of graphs. The experimental results on two different types of graph datasets show that the proposed {GTM} outperforms the latent Dirichlet allocation on classification by using the unveiled topics of these two models to represent graphs.},
	pages = {2792--2803},
	number = {12},
	journaltitle = {{IEEE} Transactions on Cybernetics},
	author = {Xuan, Junyu and Lu, Jie and Zhang, Guangquan and Luo, Xiangfeng},
	date = {2015-12},
	note = {Conference Name: {IEEE} Transactions on Cybernetics},
	keywords = {pattern classification, text analysis, data mining, Data models, latent Dirichlet allocation, Inference algorithms, topic model, data structures, graph theory, bag-of-word assumption, Bernoulli distribution, Chemical elements, Chemicals, classification, Data mining, edge modeling, graph dataset, graph mining, Graph mining, graph nodes, graph representation, graph supervised learning, graph-structured data, {GTM}, Hidden Markov models, innovative graph topic model, latent Dirichlet allocation ({LDA}), latent topic discovery, unsupervised learning, Vectors},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\GE6T79DZ\\7015568.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\B4HXLF77\\Xuan et al. - 2015 - Topic Model for Graph Mining.pdf:application/pdf}
}

@inproceedings{sun_topic_2018,
	title = {Topic Model Based Knowledge Graph for Entity Similarity Measuring},
	doi = {10.1109/ICEBE.2018.00024},
	abstract = {Entity similarity measuring is the basic work of academic search and recommendation. To analyze and measure the similarity among entities, existing methods are mainly based on either textual content or relationship unilaterally. Thus they can not measure similarity between paper and scholar or are limited in a small range of field. To address this, we propose a topic model, utilizing both textual content of papers and relationship between entities, and then introduce an algorithm for computing similarity between graph entities based on knowledge graph enhanced by topic model. Through the experiments we show that our model outperforms traditional topic model in topic coherence and we are able to list similar papers, scholars and conferences simultaneously and more accurately.},
	eventtitle = {2018 {IEEE} 15th International Conference on e-Business Engineering ({ICEBE})},
	pages = {94--101},
	booktitle = {2018 {IEEE} 15th International Conference on e-Business Engineering ({ICEBE})},
	author = {Sun, Haoran and Ren, Rui and Cai, Hongming and Xu, Boyi and Liu, Yonggang and Li, Tongyu},
	date = {2018-10},
	keywords = {Computational modeling, Correlation, Probability distribution, text analysis, topic model, graph theory, information retrieval, academic search, entity similarity measuring, graph entities, Indexes, Knowledge engineering, knowledge graph, Software, textual content, topic coherence, topic model based knowledge graph, traditional topic model, Writing},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\K5NWRA3J\\8592635.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\MXKSSUV2\\Sun et al. - 2018 - Topic Model Based Knowledge Graph for Entity Simil.pdf:application/pdf}
}

@article{gotoh_graph-based_nodate,
	title = {Graph-based Correlated Topic Model for Motion Patterns Analysis in Crowded Scenes from Tracklets},
	abstract = {This paper presents a graph-based correlated topic model ({GCTM}) to model the different motion patterns at highly cluttered and crowded environment. Unlike the existing methods that address trajectory clustering and crowd topic modelling using local motion features such as optical ﬂow, it builds on portions of trajectory known as tracklets extracted from crowded scenes. It extends the correlated topic model ({CTM}) in the text processing ﬁeld, by integrating the spatio-temporal graph ({STG}) as a prior to capture the spatial and temporal coherence between tracklets during the learning process. Two types of correlation are deﬁned over the tracklets. Firstly intra-correlation of the extracted tracklets is encoded by the locality-constrained linear coding ({LLC}) with the geodesic distance and the shortest path graph. Secondly inter-correlation is derived between the tracklets by constructing a shortest path graph with k-nearest neighbourhood ({kNN}) from both the spatial and temporal domains. Experiments and comparisons show that the {GCTM} outperforms state-of-the-art methods both on qualitative results of learning motion patterns and on quantitative results of clustering tracklets.},
	pages = {12},
	author = {Gotoh, Yoshihiko},
	langid = {english},
	file = {Gotoh - Graph-based Correlated Topic Model for Motion Patt.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\2WVRGSEW\\Gotoh - Graph-based Correlated Topic Model for Motion Patt.pdf:application/pdf}
}

@article{mimno_gibbs_nodate,
	title = {Gibbs Sampling for Logistic Normal Topic Models with Graph-Based Priors},
	abstract = {Previous work on probabilistic topic models has either focused on models with relatively simple conjugate priors that support Gibbs sampling or models with non-conjugate priors that typically require variational inference. Gibbs sampling is more accurate than variational inference and better supports the construction of composite models. We present a method for Gibbs sampling in non-conjugate logistic normal topic models, and demonstrate it on a new class of topic models with arbitrary graph-structured priors that reﬂect the complex relationships commonly found in document collections, while retaining simple, robust inference.},
	pages = {8},
	author = {Mimno, David and Wallach, Hanna M and {McCallum}, Andrew},
	langid = {english},
	file = {Mimno et al. - Gibbs Sampling for Logistic Normal Topic Models wi.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\4PKELZZ6\\Mimno et al. - Gibbs Sampling for Logistic Normal Topic Models wi.pdf:application/pdf}
}

@article{mei_topic_2008,
	title = {Topic Modeling with Network Regularization},
	abstract = {In this paper, we formally deﬁne the problem of topic modeling with network structure ({TMN}). We propose a novel solution to this problem, which regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data. The proposed method combines topic modeling and social network analysis, and leverages the power of both statistical topic models and discrete regularization. The output of this model can summarize well topics in text, map a topic onto the network, and discover topical communities. With appropriate instantiations of the topic model and the graph-based regularizer, our model can be applied to a wide range of text mining problems such as authortopic analysis, community discovery, and spatial text mining. Empirical experiments on two data sets with diﬀerent genres show that our approach is eﬀective and outperforms both text-oriented methods and network-oriented methods alone. The proposed model is general; it can be applied to any text collections with a mixture of topics and an associated network structure.},
	pages = {10},
	journaltitle = {Data Mining},
	author = {Mei, Qiaozhu and Cai, Deng and Zhang, Duo and Zhai, {ChengXiang}},
	date = {2008},
	langid = {english},
	file = {Mei et al. - 2008 - Topic Modeling with Network Regularization.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\CD88BGHK\\Mei et al. - 2008 - Topic Modeling with Network Regularization.pdf:application/pdf}
}

@online{noauthor_notitle_nodate,
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewPDFInterstitial/9676/9912},
	urldate = {2020-08-10},
	file = {:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\YW9IN46R\\9912.html:text/html}
}

@online{noauthor_notitle_nodate-1,
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14170/14086},
	urldate = {2020-08-10},
	file = {:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\SIGV63G7\\14086.html:text/html}
}

@inproceedings{zhu_graphbtm_2018,
	location = {Brussels, Belgium},
	title = {{GraphBTM}: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model},
	url = {http://aclweb.org/anthology/D18-1495},
	doi = {10.18653/v1/D18-1495},
	shorttitle = {{GraphBTM}},
	abstract = {Discovering the latent topics within texts has been a fundamental task for many applications. However, conventional topic models suffer different problems in different settings. The Latent Dirichlet Allocation ({LDA}) may not work well for short texts due to the data sparsity (i.e., the sparse word co-occurrence patterns in short documents). The Biterm Topic Model ({BTM}) learns topics by modeling the word-pairs named biterms in the whole corpus. This assumption is very strong when documents are long with rich topic information and do not exhibit the transitivity of biterms. In this paper, we propose a novel way called {GraphBTM} to represent biterms as graphs and design Graph Convolutional Networks ({GCNs}) with residual connections to extract transitive features from biterms. To overcome the data sparsity of {LDA} and the strong assumption of {BTM}, we sample a ﬁxed number of documents to form a mini-corpus as a training instance. We also propose a dataset called All N ews extracted from (Thompson, 2017), in which documents are much longer than 20 Newsgroups. We present an amortized variational inference method for {GraphBTM}. Our method generates more coherent topics compared with previous approaches. Experiments show that the sampling strategy improves performance by a large margin.},
	eventtitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	pages = {4663--4672},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Zhu, Qile and Feng, Zheng and Li, Xiaolin},
	urldate = {2020-08-10},
	date = {2018},
	langid = {english},
	file = {Zhu et al. - 2018 - GraphBTM Graph Enhanced Autoencoded Variational I.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\Y84PGRJF\\Zhu et al. - 2018 - GraphBTM Graph Enhanced Autoencoded Variational I.pdf:application/pdf}
}

@article{levy_neural_nodate,
	title = {Neural Word Embedding as Implicit Matrix Factorization},
	abstract = {We analyze skip-gram with negative-sampling ({SGNS}), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information ({PMI}) of the respective word and context pairs, shifted by a global constant. We ﬁnd that another embedding method, {NCE}, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive {PMI} word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with {SVD} can achieve solutions that are at least as good as {SGNS}’s solutions for word similarity tasks. On analogy questions {SGNS} remains superior to {SVD}. We conjecture that this stems from the weighted nature of {SGNS}’s factorization.},
	pages = {9},
	author = {Levy, Omer and Goldberg, Yoav},
	langid = {english},
	file = {Levy and Goldberg - Neural Word Embedding as Implicit Matrix Factoriza.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\ADJN6A32\\Levy and Goldberg - Neural Word Embedding as Implicit Matrix Factoriza.pdf:application/pdf}
}

@article{mikolov_distributed_nodate,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	abstract = {The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
	pages = {9},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	langid = {english},
	file = {Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\VSDFL8EM\\Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:application/pdf}
}

@article{blei_variational_2017,
	title = {Variational Inference: A Review for Statisticians},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	shorttitle = {Variational Inference},
	abstract = {One of the core problems of modern statistics is to approximate difﬁcult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference ({VI}), a method from machine learning that approximates probability densities through optimization. {VI} has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind {VI} is to ﬁrst posit a family of densities and then to ﬁnd the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-ﬁeld variational inference, discuss the special case of {VI} applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in {VI} and highlight important open problems. {VI} is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	pages = {859--877},
	number = {518},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and {McAuliffe}, Jon D.},
	urldate = {2020-08-13},
	date = {2017-04-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1601.00670},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Statistics - Computation},
	file = {1601.00670.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\63MQQW57\\1601.00670.pdf:application/pdf}
}

@article{paisley_bayesian_nodate,
	title = {Bayesian Models for Machine Learning},
	pages = {126},
	author = {Paisley, John},
	langid = {english},
	file = {Paisley - Bayesian Models for Machine Learning.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\8SZIPZV3\\Paisley - Bayesian Models for Machine Learning.pdf:application/pdf}
}

@article{resnik_gibbs_nodate,
	title = {{GIBBS} {SAMPLING} {FOR} {THE} {UNINITIATED}},
	abstract = {This document is intended for computer scientists who would like to try out a Markov Chain Monte Carlo ({MCMC}) technique, particularly in order to do inference with Bayesian models on problems related to text processing. We try to keep theory to the absolute minimum needed, though we work through the details much more explicitly than you usually see even in “introductory” explanations. That means we’ve attempted to be ridiculously explicit in our exposition and notation.},
	pages = {23},
	author = {Resnik, Philip and Hardisty, Eric},
	langid = {english},
	file = {Resnik and Hardisty - GIBBS SAMPLING FOR THE UNINITIATED.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\YNAHC6S3\\Resnik and Hardisty - GIBBS SAMPLING FOR THE UNINITIATED.pdf:application/pdf}
}

@article{archambeau_latent_2015,
	title = {Latent {IBP} Compound Dirichlet Allocation},
	volume = {37},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2014.2313122},
	abstract = {We introduce the four-parameter {IBP} compound Dirichlet process ({ICDP}), a stochastic process that generates sparse non-negative vectors with potentially an unbounded number of entries. If we repeatedly sample from the {ICDP} we can generate sparse matrices with an infinite number of columns and power-law characteristics. We apply the four-parameter {ICDP} to sparse nonparametric topic modelling to account for the very large number of topics present in large text corpora and the power-law distribution of the vocabulary of natural languages. The model, which we call latent {IBP} compound Dirichlet allocation ({LIDA}), allows for power-law distributions, both, in the number of topics summarising the documents and in the number of words defining each topic. It can be interpreted as a sparse variant of the hierarchical Pitman-Yor process when applied to topic modelling. We derive an efficient and simple collapsed Gibbs sampler closely related to the collapsed Gibbs sampler of latent Dirichlet allocation ({LDA}), making the model applicable in a wide range of domains. Our nonparametric Bayesian topic model compares favourably to the widely used hierarchical Dirichlet process and its heavy tailed version, the hierarchical Pitman-Yor process, on benchmark corpora. Experiments demonstrate that accounting for the power-distribution of real data is beneficial and that sparsity provides more interpretable results.},
	pages = {321--333},
	number = {2},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Archambeau, Cédric and Lakshminarayanan, Balaji and Bouchard, Guillaume},
	date = {2015-02},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {clustering, Analytical models, Resource management, Vocabulary, Data models, Bayesian nonparametrics, Atomic measurements, bag-of-words representation, Bayes methods, Compounds, Gibbs sampling, power-law distribution, sparse modelling, topic modelling},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\5KXSKMLQ\\6780626.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\ICHKSMTA\\Archambeau et al. - 2015 - Latent IBP Compound Dirichlet Allocation.pdf:application/pdf}
}

@article{chien_hierarchical_2015,
	title = {Hierarchical Pitman–Yor–Dirichlet Language Model},
	volume = {23},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2015.2428632},
	abstract = {Probabilistic models are often viewed as insufficiently expressive because of strong limitation and assumption on the probabilistic distribution and the fixed model complexity. Bayesian nonparametric learning pursues an expressive probabilistic representation based on the nonparametric prior and posterior distributions with less assumption-laden approach to inference. This paper presents a hierarchical Pitman-Yor-Dirichlet ({HPYD}) process as the nonparametric priors to infer the predictive probabilities of the smoothed n-grams with the integrated topic information. A metaphor of hierarchical Chinese restaurant process is proposed to infer the {HPYD} language model ({HPYD}-{LM}) via Gibbs sampling. This process is equivalent to implement the hierarchical Dirichlet process-latent Dirichlet allocation ({HDP}-{LDA}) with the twisted hierarchical Pitman-Yor {LM} ({HPY}-{LM}) as base measures. Accordingly, we produce the power-law distributions and extract the semantic topics to reflect the properties of natural language in the estimated {HPYD}-{LM}. The superiority of {HPYD}-{LM} to {HPY}-{LM} and other language models is demonstrated by the experiments on model perplexity and speech recognition.},
	pages = {1259--1272},
	number = {8},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Chien, Jen-Tzung},
	date = {2015-08},
	note = {Conference Name: {IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	keywords = {Semantics, Data models, Bayesian nonparametrics, topic model, learning (artificial intelligence), unsupervised learning, Bayes methods, Gibbs sampling, Adaptation models, base measures, Bayesian nonparametric learning, Context, expressive probabilistic representation, fixed model complexity, {HDP}-{LDA}, hierarchical Chinese restaurant process, hierarchical Dirichlet process-latent Dirichlet allocation, hierarchical Pitman-Yor-Dirichlet language model, {HPYD} language model, {HPYD}-{LM}, integrated topic information, language model, model perplexity, natural language, nonparametric prior distribution, nonparametric statistics, posterior distributions, power-law distributions, predictive probabilities, probabilistic distribution, probabilistic models, semantic topics, smoothed n-grams, Speech, Speech processing, speech recognition, statistical distributions, twisted hierarchical Pitman-Yor {LM}},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\2HNFKC8U\\7098357.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\PW3R7JN5\\Chien - 2015 - Hierarchical Pitman–Yor–Dirichlet Language Model.pdf:application/pdf}
}

@inproceedings{sato_topic_2010,
	location = {New York, {NY}, {USA}},
	title = {Topic models with power-law using Pitman-Yor process},
	isbn = {978-1-4503-0055-1},
	url = {https://doi.org/10.1145/1835804.1835890},
	doi = {10.1145/1835804.1835890},
	series = {{KDD} '10},
	abstract = {One important approach for knowledge discovery and data mining is to estimate unobserved variables because latent variables can indicate hidden specific properties of observed data. The latent factor model assumes that each item in a record has a latent factor; the co-occurrence of items can then be modeled by latent factors. In document modeling, a record indicates a document represented as a "bag of words," meaning that the order of words is ignored, an item indicates a word and a latent factor indicates a topic. Latent Dirichlet allocation ({LDA}) is a widely used Bayesian topic model applying the Dirichlet distribution over the latent topic distribution of a document having multiple topics. {LDA} assumes that latent topics, i.e., discrete latent variables, are distributed according to a multinomial distribution whose parameters are generated from the Dirichlet distribution. {LDA} also models a word distribution by using a multinomial distribution whose parameters follows the Dirichlet distribution. This Dirichlet-multinomial setting, however, cannot capture the power-law phenomenon of a word distribution, which is known as Zipf's law in linguistics. We therefore propose a novel topic model using the Pitman-Yor({PY}) process, called the {PY} topic model. The {PY} topic model captures two properties of a document; a power-law word distribution and the presence of multiple topics. In an experiment using real data, this model outperformed {LDA} in document modeling in terms of perplexity.},
	pages = {673--682},
	booktitle = {Proceedings of the 16th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Sato, Issei and Nakagawa, Hiroshi},
	urldate = {2020-08-21},
	date = {2010-07-25},
	keywords = {latent dirichlet allocation, topic model, nonparametric bayes, Pitman-Yor process, power-law},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\RJXJXWHA\\Sato and Nakagawa - 2010 - Topic models with power-law using Pitman-Yor proce.pdf:application/pdf}
}

@article{lindsey_phrase-discovering_nodate,
	title = {A Phrase-Discovering Topic Model Using Hierarchical Pitman-Yor Processes},
	abstract = {Topic models traditionally rely on the bagof-words assumption. In data mining applications, this often results in end-users being presented with inscrutable lists of topical unigrams, single words inferred as representative of their topics. In this article, we present a hierarchical generative probabilistic model of topical phrases. The model simultaneously infers the location, length, and topic of phrases within a corpus and relaxes the bagof-words assumption within phrases by using a hierarchy of Pitman-Yor processes. We use Markov chain Monte Carlo techniques for approximate inference in the model and perform slice sampling to learn its hyperparameters. We show via an experiment on human subjects that our model ﬁnds substantially better, more interpretable topical phrases than do competing models.},
	pages = {9},
	author = {Lindsey, Robert and Headden, William and Stipicevic, Michael},
	langid = {english},
	file = {Lindsey et al. - A Phrase-Discovering Topic Model Using Hierarchica.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\C8XDAQDJ\\Lindsey et al. - A Phrase-Discovering Topic Model Using Hierarchica.pdf:application/pdf}
}

@article{ferguson_bayesian_1973,
	title = {A Bayesian Analysis of Some Nonparametric Problems},
	volume = {1},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1176342360},
	doi = {10.1214/aos/1176342360},
	pages = {209--230},
	number = {2},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Ferguson, Thomas S.},
	urldate = {2020-08-21},
	date = {1973-03},
	langid = {english},
	file = {Ferguson - 1973 - A Bayesian Analysis of Some Nonparametric Problems.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\LL9Z2AM5\\Ferguson - 1973 - A Bayesian Analysis of Some Nonparametric Problems.pdf:application/pdf}
}

@article{teh_indian_nodate,
	title = {Indian Buffet Processes with Power-law Behavior},
	abstract = {The Indian buffet process ({IBP}) is an exchangeable distribution over binary matrices used in Bayesian nonparametric featural models. In this paper we propose a three-parameter generalization of the {IBP} exhibiting power-law behavior. We achieve this by generalizing the beta process (the de Finetti measure of the {IBP}) to the stable-beta process and deriving the {IBP} corresponding to it. We ﬁnd interesting relationships between the stable-beta process and the Pitman-Yor process (another stochastic process used in Bayesian nonparametric models with interesting power-law properties). We derive a stick-breaking construction for the stable-beta process, and ﬁnd that our power-law {IBP} is a good model for word occurrences in document corpora.},
	pages = {9},
	author = {Teh, Yee W and Gorur, Dilan},
	langid = {english},
	file = {Teh and Gorur - Indian Buffet Processes with Power-law Behavior.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\AXQZUCJC\\Teh and Gorur - Indian Buffet Processes with Power-law Behavior.pdf:application/pdf}
}

@inproceedings{batmanghelich_nonparametric_2016-1,
	location = {Berlin, Germany},
	title = {Nonparametric Spherical Topic Modeling with Word Embeddings},
	url = {https://www.aclweb.org/anthology/P16-2087},
	doi = {10.18653/v1/P16-2087},
	eventtitle = {{ACL} 2016},
	pages = {537--542},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Batmanghelich, Kayhan and Saeedi, Ardavan and Narasimhan, Karthik and Gershman, Sam},
	urldate = {2020-08-23},
	date = {2016-08},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\IDX938Q8\\Batmanghelich et al. - 2016 - Nonparametric Spherical Topic Modeling with Word E.pdf:application/pdf}
}

@inproceedings{reynolds_definitional_1972,
	location = {Boston, Massachusetts, United States},
	title = {Definitional interpreters for higher-order programming languages},
	volume = {2},
	url = {http://portal.acm.org/citation.cfm?doid=800194.805852},
	doi = {10.1145/800194.805852},
	abstract = {We use Church, a Turing-universal language for stochastic generative processes and the probability distributions they induce, to study and extend several objects in nonparametric Bayesian statistics. We connect exchangeability and de Finetti measures with notions of purity and closures from functional programming. We exploit delayed evaluation to provide ﬁnite, machine-executable representations for various nonparametric Bayesian objects. We relate common uses of the Dirichlet process to a stochastic generalization of memoization, and use this abstraction to compactly describe and extend several nonparametric models. Finally, we brieﬂy discuss issues of computability and inference.},
	eventtitle = {the {ACM} annual conference},
	pages = {717--740},
	booktitle = {Proceedings of the {ACM} annual conference on   - {ACM} '72},
	publisher = {{ACM} Press},
	author = {Reynolds, John C.},
	urldate = {2020-08-23},
	date = {1972},
	langid = {english},
	file = {Reynolds - 1972 - Definitional interpreters for higher-order program.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\H97D22NB\\Reynolds - 1972 - Definitional interpreters for higher-order program.pdf:application/pdf}
}

@online{noauthor_optimising_nodate,
	title = {Optimising topic coherence with Weighted Plya Urn scheme {\textbar} Elsevier Enhanced Reader},
	url = {https://reader.elsevier.com/reader/sd/pii/S092523121931714X?token=339FCEBD78EAE756FA4CE6E89AA78A24ACF5C5AC8CF020ED42DCF4AC12F7B22896454609A2D4E2915E4D168615EE6D5C},
	urldate = {2020-08-25},
	langid = {english},
	doi = {10.1016/j.neucom.2019.12.013},
	file = {Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\BXBLYLGU\\S092523121931714X.html:text/html}
}

@article{alvarez_bayesian_2016,
	title = {Bayesian inference for a covariance matrix},
	url = {http://arxiv.org/abs/1408.4050},
	abstract = {Covariance matrix estimation arises in multivariate problems including multivariate normal sampling models and regression models where random effects are jointly modeled, e.g. random-intercept, random-slope models. A Bayesian analysis of these problems requires a prior on the covariance matrix. Here we assess, through a simulation study and a real data set, the impact this prior choice has on posterior inference of the covariance matrix. Inverse Wishart distribution is the natural choice for a covariance matrix prior because its conjugacy on normal model and simplicity, is usually available in Bayesian statistical software. However inverse Wishart distribution presents some undesirable properties from a modeling point of view. It can be too restrictive because assume the same amount of prior information about every variance parameters and, more important, it shows a prior relationship between the variances and correlations. Some alternatives distributions has been proposed. The scaled inverse Wishart distribution, which give more flexibility on the variance priors conserving the conjugacy property but does not eliminate the prior relationship between variances and correlations. Secondly, it is possible to fit separate priors for individual correlations and standard deviations. This strategy eliminates any prior relationship within the covariance matrix parameters, but it is not conjugate and therefore computationally slow.},
	journaltitle = {{arXiv}:1408.4050 [stat]},
	author = {Alvarez, Ignacio and Niemi, Jarad and Simpson, Matt},
	urldate = {2020-08-26},
	date = {2016-07-08},
	eprinttype = {arxiv},
	eprint = {1408.4050},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\9M3GC9WX\\Alvarez et al. - 2016 - Bayesian inference for a covariance matrix.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\PABIN6FI\\1408.html:text/html}
}

@article{lewandowski_generating_2009,
	title = {Generating random correlation matrices based on vines and extended onion method},
	volume = {100},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X09000876},
	doi = {10.1016/j.jmva.2009.04.008},
	abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, {ACM} Transactions on Modeling and Computer Simulation ({TOMACS}) 13 (3) (2003) 276–294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177–2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.},
	pages = {1989--2001},
	number = {9},
	journaltitle = {Journal of Multivariate Analysis},
	shortjournal = {Journal of Multivariate Analysis},
	author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
	urldate = {2020-08-26},
	date = {2009-10-01},
	langid = {english},
	keywords = {Correlation matrix, Dependence vines, Onion method, Partial correlation},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\YNKQPI54\\Lewandowski et al. - 2009 - Generating random correlation matrices based on vi.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\U9DNU925\\S0047259X09000876.html:text/html}
}

@article{bouma_normalized_nodate,
	title = {Normalized (Pointwise) Mutual Information in Collocation Extraction},
	abstract = {In this paper, we discuss the related information theoretical association measures of mutual information and pointwise mutual information, in the context of collocation extraction. We introduce normalized variants of these measures in order to make them more easily interpretable and at the same time less sensitive to occurrence frequency. We also provide a small empirical study to give more insight into the behaviour of these new measures in a collocation extraction setup.},
	pages = {11},
	author = {Bouma, Gerlof},
	langid = {english},
	file = {Bouma - Normalized (Pointwise) Mutual Information in Collo.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\Y3JGQQ2V\\Bouma - Normalized (Pointwise) Mutual Information in Collo.pdf:application/pdf}
}

@inproceedings{mimno_optimizing_2011,
	location = {Edinburgh, Scotland, {UK}.},
	title = {Optimizing Semantic Coherence in Topic Models},
	url = {https://www.aclweb.org/anthology/D11-1024},
	eventtitle = {{EMNLP} 2011},
	pages = {262--272},
	booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Mimno, David and Wallach, Hanna and Talley, Edmund and Leenders, Miriam and {McCallum}, Andrew},
	urldate = {2020-08-26},
	date = {2011-07},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\U3ST9KII\\Mimno et al. - 2011 - Optimizing Semantic Coherence in Topic Models.pdf:application/pdf}
}

@book{gelman_bayesian_2013,
	title = {Bayesian data analysis},
	publisher = {{CRC} press},
	author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
	date = {2013}
}

@book{mcelreath_statistical_2020,
	title = {Statistical rethinking: A Bayesian course with examples in R and Stan},
	publisher = {{CRC} press},
	author = {{McElreath}, Richard},
	date = {2020}
}

@article{barnard_modeling_2000,
	title = {{MODELING} {COVARIANCE} {MATRICES} {IN} {TERMS} {OF} {STANDARD} {DEVIATIONS} {AND} {CORRELATIONS}, {WITH} {APPLICATION} {TO} {SHRINKAGE}},
	abstract = {The covariance matrix plays an important role in statistical inference, yet modeling a covariance matrix is often a diﬃcult task in practice due to its dimensionality and the non-negative deﬁnite constraint. In order to model a covariance matrix eﬀectively, it is typically broken down into components based on modeling considerations or mathematical convenience. Decompositions that have received recent research attention include variance components, spectral decomposition, Cholesky decomposition, and matrix logarithm. In this paper we study a statistically motivated decomposition which appears to be relatively unexplored for the purpose of modeling. We model a covariance matrix in terms of its corresponding standard deviations and correlation matrix. We discuss two general modeling situations where this approach is useful: shrinkage estimation of regression coeﬃcients, and a general location-scale model for both categorical and continuous variables. We present some simple choices for priors in terms of standard deviations and the correlation matrix, and describe a straightforward computational strategy for obtaining the posterior of the covariance matrix. We apply our method to real and simulated data sets in the context of shrinkage estimation.},
	pages = {31},
	author = {Barnard, John and {McCulloch}, Robert and Meng, Xiao-Li},
	date = {2000},
	langid = {english},
	file = {Barnard et al. - MODELING COVARIANCE MATRICES IN TERMS OF STANDARD .pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\HWJSKQJB\\Barnard et al. - MODELING COVARIANCE MATRICES IN TERMS OF STANDARD .pdf:application/pdf}
}

@inproceedings{katsumata_graph-based_2018,
	location = {Melbourne, Australia},
	title = {Graph-based Filtering of Out-of-Vocabulary Words for Encoder-Decoder Models},
	url = {http://aclweb.org/anthology/P18-3016},
	doi = {10.18653/v1/P18-3016},
	abstract = {Encoder-decoder models typically only employ words that are frequently used in the training corpus to reduce the computational costs and exclude noise. However, this vocabulary set may still include words that interfere with learning in encoder-decoder models. This paper proposes a method for selecting more suitable words for learning encoders by utilizing not only frequency but also cooccurrence information, which we capture using the {HITS} algorithm. We apply our proposed method to two tasks: machine translation and grammatical error correction. For Japanese-to-English translation, this method achieves a {BLEU} score that is 0.56 points more than that of a baseline. Furthermore, it outperforms the baseline method for English grammatical error correction, with an F0.5-measure that is 1.48 points higher.},
	eventtitle = {Proceedings of {ACL} 2018, Student Research Workshop},
	pages = {112--119},
	booktitle = {Proceedings of {ACL} 2018, Student Research Workshop},
	publisher = {Association for Computational Linguistics},
	author = {Katsumata, Satoru and Matsumura, Yukio and Yamagishi, Hayahide and Komachi, Mamoru},
	urldate = {2020-08-27},
	date = {2018},
	langid = {english},
	file = {Katsumata et al. - 2018 - Graph-based Filtering of Out-of-Vocabulary Words f.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\R28EGA2P\\Katsumata et al. - 2018 - Graph-based Filtering of Out-of-Vocabulary Words f.pdf:application/pdf}
}

@article{khan_variational_nodate,
	title = {Variational {EM} Algorithms for Correlated Topic Models},
	abstract = {In this note, we derive a variational {EM} algorithm for correlated topic models. This algorithm was proposed in Blei and Laﬀerty’s original paper [{BL}06] and is based on a simple bound on logarithm. Because of the form of this bound, E-step update are not available in closed form and need to be solved with a coordinate ascent algorithm.},
	pages = {5},
	author = {Khan, Mohammad Emtiyaz and Bouchard, Guillaume},
	langid = {english},
	file = {Khan and Bouchard - Variational EM Algorithms for Correlated Topic Mod.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\66X6NA6N\\Khan and Bouchard - Variational EM Algorithms for Correlated Topic Mod.pdf:application/pdf}
}

@article{rahman_training_2020,
	title = {Training Sensitivity in Graph Isomorphism Network},
	url = {http://arxiv.org/abs/2008.09020},
	abstract = {Graph neural network ({GNN}) is a popular tool to learn the lower-dimensional representation of a graph. It facilitates the applicability of machine learning tasks on graphs by incorporating domain-specific features. There are various options for underlying procedures (such as optimization functions, activation functions, etc.) that can be considered in the implementation of {GNN}. However, most of the existing tools are confined to one approach without any analysis. Thus, this emerging field lacks a robust implementation ignoring the highly irregular structure of the real-world graphs. In this paper, we attempt to fill this gap by studying various alternative functions for a respective module using a diverse set of benchmark datasets. Our empirical results suggest that the generally used underlying techniques do not always perform well to capture the overall structure from a set of graphs.},
	journaltitle = {{arXiv}:2008.09020 [cs, stat]},
	author = {Rahman, Md Khaledur},
	urldate = {2020-08-27},
	date = {2020-08-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.09020},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	file = {Rahman - 2020 - Training Sensitivity in Graph Isomorphism Network.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\GMJM7YYI\\Rahman - 2020 - Training Sensitivity in Graph Isomorphism Network.pdf:application/pdf}
}

@article{hoffmann_community_2020,
	title = {Community detection in networks without observing edges},
	volume = {6},
	issn = {2375-2548},
	url = {https://advances.sciencemag.org/lookup/doi/10.1126/sciadv.aav1478},
	doi = {10.1126/sciadv.aav1478},
	abstract = {We develop a Bayesian hierarchical model to identify communities of time series. Fitting the model provides an end-to-end community detection algorithm that does not extract information as a sequence of point estimates but propagates uncertainties from the raw data to the community labels. Our approach naturally supports multiscale community detection and the selection of an optimal scale using model comparison. We study the properties of the algorithm using synthetic data and apply it to daily returns of constituents of the S\&P100 index and climate data from U.S. cities.},
	pages = {eaav1478},
	number = {4},
	journaltitle = {Science Advances},
	shortjournal = {Sci. Adv.},
	author = {Hoffmann, Till and Peel, Leto and Lambiotte, Renaud and Jones, Nick S.},
	urldate = {2020-08-28},
	date = {2020-01},
	langid = {english},
	file = {Hoffmann et al. - 2020 - Community detection in networks without observing .pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\M893QK5Z\\Hoffmann et al. - 2020 - Community detection in networks without observing .pdf:application/pdf}
}

@article{akinc_bayesian_2018,
	title = {Bayesian estimation of mixed logit models: Selecting an appropriate prior for the covariance matrix},
	volume = {29},
	issn = {17555345},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1755534517301203},
	doi = {10.1016/j.jocm.2017.11.004},
	shorttitle = {Bayesian estimation of mixed logit models},
	abstract = {Maximum likelihood and Bayesian estimation are both frequently used to ﬁt mixed logit models to choice data. The type and the number of quasi-random draws used for simulating the likelihood and the choice of the priors in Bayesian estimation have a big impact on the estimates. We compare the different approaches and compute the relative root mean square errors of the resulting estimates for the mean, covariance matrix and individual parameters in a large simulation study. We focus on the prior for the covariance matrix in Bayesian estimation and investigate the effect of Inverse Wishart priors, the Separation Strategy, Scaled Inverse Wishart and Huang Half-t priors. We show that the default settings in many software packages can lead to very unreliable results and that it is important to check the robustness of the results.},
	pages = {133--151},
	journaltitle = {Journal of Choice Modelling},
	shortjournal = {Journal of Choice Modelling},
	author = {Akinc, Deniz and Vandebroek, Martina},
	urldate = {2020-08-28},
	date = {2018-12},
	langid = {english},
	file = {Akinc and Vandebroek - 2018 - Bayesian estimation of mixed logit models Selecti.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\GDR9Q3MW\\Akinc and Vandebroek - 2018 - Bayesian estimation of mixed logit models Selecti.pdf:application/pdf}
}

@article{wiecek_structure_2020,
	title = {Structure learning of Bayesian networks involving cyclic structures},
	url = {http://arxiv.org/abs/1906.04992},
	abstract = {Many biological networks include cyclic structures. In such cases, Bayesian networks ({BNs}), which must be acyclic, are not sound models for structure learning. Dynamic {BNs} can be used but require relatively large time series data. We discuss an alternative model that embeds cyclic structures within acyclic {BNs}, allowing us to still use the factorization property and informative priors on network structure. We present an implementation in the linear Gaussian case, where cyclic structures are treated as multivariate nodes. We use a Markov Chain Monte Carlo algorithm for inference, allowing us to work with posterior distribution on the space of graphs.},
	journaltitle = {{arXiv}:1906.04992 [stat]},
	author = {Wiecek, Witold and Bois, Frederic Y. and Gayraud, Ghislaine},
	urldate = {2020-08-28},
	date = {2020-02-17},
	eprinttype = {arxiv},
	eprint = {1906.04992},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\V4IHBL8Z\\Wiecek et al. - 2020 - Structure learning of Bayesian networks involving .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\BXBV8XYG\\1906.html:text/html}
}

@inproceedings{sato_knowledge_2007,
	location = {San Jose, California, {USA}},
	title = {Knowledge discovery of multiple-topic document using parametric mixture model with dirichlet prior},
	isbn = {978-1-59593-609-7},
	url = {http://portal.acm.org/citation.cfm?doid=1281192.1281256},
	doi = {10.1145/1281192.1281256},
	abstract = {Documents, such as those seen on Wikipedia and Folksonomy, have tended to be assigned with multiple topics as a meta-data. Therefore, it is more and more important to analyze a relationship between a document and topics assigned to the document. In this paper, we proposed a novel probabilistic generative model of documents with multiple topics as a meta-data. By focusing on modeling the generation process of a document with multiple topics, we can extract speciﬁc properties of documents with multiple topics. Proposed model is an expansion of an existing probabilistic generative model: Parametric Mixture Model ({PMM}). {PMM} models documents with multiple topics by mixing model parameters of each single topic. Since ,however, {PMM} assigns the same mixture ratio to each single topic, {PMM} cannot take into account the bias of each topic within a document. To deal with this problem, we propose a model that considers Dirichlet distribution as a prior distribution of the mixture ratio. We adopt Variational Bayes Method to infer the bias of each topic within a document. We evaluate the proposed model and {PMM} using {MEDLINE} corpus. The results of F-measure , Precision and Recall show that the proposed model is more effective than {PMM} on multiple-topic classiﬁcation. Moreover, we indicate the potential of the proposed model that extracts topics and document-speciﬁc keywords using information about the assigned topics.},
	eventtitle = {the 13th {ACM} {SIGKDD} international conference},
	pages = {590},
	booktitle = {Proceedings of the 13th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining  - {KDD} '07},
	publisher = {{ACM} Press},
	author = {Sato, Issei and Nakagawa, Hiroshi},
	urldate = {2020-08-28},
	date = {2007},
	langid = {english},
	file = {Sato and Nakagawa - 2007 - Knowledge discovery of multiple-topic document usi.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\JSKCKZGH\\Sato and Nakagawa - 2007 - Knowledge discovery of multiple-topic document usi.pdf:application/pdf}
}

@article{gopal_von_nodate,
	title = {Von Mises-Fisher Clustering Models},
	abstract = {This paper proposes a suite of models for clustering high-dimensional data on a unit sphere based on von Mises-Fisher ({vMF}) distribution and for discovering more intuitive clusters than existing approaches. The proposed models include a) A Bayesian formulation of {vMF} mixture that enables information sharing among clusters, b) a Hierarchical {vMF} mixture that provides multiscale shrinkage and tree structured view of the data and c) a Temporal {vMF} mixture that captures evolution of clusters in temporal data. For posterior inference, we develop fast variational methods as well as collapsed Gibbs sampling techniques for all three models. Our experiments on six datasets provide strong empirical support in favour of {vMF} based clustering models over other popular tools such as K-means, Multinomial Mixtures and Latent Dirichlet Allocation.},
	pages = {9},
	author = {Gopal, Siddharth and Yang, Yiming},
	langid = {english},
	file = {Gopal and Yang - Von Mises-Fisher Clustering Models.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\BM9BCPCD\\Gopal and Yang - Von Mises-Fisher Clustering Models.pdf:application/pdf}
}

@article{teh_dirichlet_nodate,
	title = {Dirichlet Processes: Tutorial and Practical Course},
	pages = {147},
	author = {Teh, Yee Whye},
	langid = {english},
	file = {Teh - Dirichlet Processes Tutorial and Practical Course.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\82EZ8374\\Teh - Dirichlet Processes Tutorial and Practical Course.pdf:application/pdf}
}

@article{blei_latent_nodate,
	title = {Latent Dirichlet Allocation},
	abstract = {We describe latent Dirichlet allocation ({LDA}), a generative probabilistic model for collections of discrete data such as text corpora. {LDA} is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an {EM} algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic {LSI} model.},
	pages = {30},
	author = {Blei, David M},
	langid = {english},
	file = {Blei - Latent Dirichlet Allocation.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\4QVCI92G\\Blei - Latent Dirichlet Allocation.pdf:application/pdf}
}

@article{frederic_technical_nodate,
	title = {A Technical Note on the Logitnormal Distribution},
	abstract = {We present the technical details of the logitnormal distribution of a variable supported on the open unit-interval. Although this distribution has been used widely in a variety of statistical applications under a transformation, its direct analysis has not been exposited. The density function can be expressed algebraically, and the moments can be computed easily by numerical integration as functions of the parameters of the transformed normal distribution. The family of densities has characteristics diﬀerent from the Beta family especially near 0 and 1, although some members of each family resemble each other. Limiting members of the family provide examples of distributions with adherent masses at 0 and 1. We also examine the bivariate exchangeable logitnormal distribution.},
	pages = {8},
	author = {Frederic, Patrizio and Lad, Frank},
	langid = {english},
	file = {Frederic and Lad - A Technical Note on the Logitnormal Distribution.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\CHEASRA8\\Frederic and Lad - A Technical Note on the Logitnormal Distribution.pdf:application/pdf}
}

@article{murphy_conjugate_nodate,
	title = {Conjugate Bayesian analysis of the Gaussian distribution},
	pages = {29},
	author = {Murphy, Kevin P},
	langid = {english},
	file = {Murphy - Conjugate Bayesian analysis of the Gaussian distri.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\R5CEWETT\\Murphy - Conjugate Bayesian analysis of the Gaussian distri.pdf:application/pdf}
}

@article{kucukelbir_automatic_2015,
	title = {Automatic Variational Inference in Stan},
	url = {http://arxiv.org/abs/1506.03431},
	abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-speciﬁc calculations; this makes it diﬃcult to automate. We propose an automatic variational inference algorithm, automatic diﬀerentiation variational inference (advi). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement advi in Stan (code available now), a probabilistic programming framework. We compare advi to mcmc sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With advi we can use variational inference on any model we write in Stan.},
	journaltitle = {{arXiv}:1506.03431 [stat]},
	author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
	urldate = {2020-11-15},
	date = {2015-06-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.03431},
	keywords = {Statistics - Machine Learning},
	file = {Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\FC4XYMC9\\Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:application/pdf}
}

@article{kingma_introduction_2019,
	title = {An Introduction to Variational Autoencoders},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1906.02691},
	doi = {10.1561/2200000056},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	pages = {307--392},
	number = {4},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{FNT} in Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2020-11-15},
	date = {2019},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.02691},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {1906.02691.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\FXYIDVVU\\1906.02691.pdf:application/pdf}
}

@article{murphy_conjugate_nodate-1,
	title = {Conjugate Bayesian analysis of the Gaussian distribution},
	pages = {29},
	author = {Murphy, Kevin P},
	langid = {english},
	file = {Murphy - Conjugate Bayesian analysis of the Gaussian distri.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\NKARK5T2\\Murphy - Conjugate Bayesian analysis of the Gaussian distri.pdf:application/pdf}
}

@article{doersch_tutorial_2016,
	title = {Tutorial on Variational Autoencoders},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders ({VAEs}) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. {VAEs} are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. {VAEs} have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, {CIFAR} images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind {VAEs}, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	journaltitle = {{arXiv}:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	urldate = {2020-11-15},
	date = {2016-08-13},
	eprinttype = {arxiv},
	eprint = {1606.05908},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\SS6Q4JID\\Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\RYCQQ4WX\\1606.html:text/html}
}

@article{srivastava_autoencoding_2017,
	title = {Autoencoding Variational Inference For Topic Models},
	url = {http://arxiv.org/abs/1703.01488},
	abstract = {Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes ({AEVB}), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective {AEVB} based inference method for latent Dirichlet allocation ({LDA}), which we call Autoencoded Variational Inference For Topic Model ({AVITM}). This model tackles the problems caused for {AEVB} by the Dirichlet prior and by component collapsing. We find that {AVITM} matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because {AVITM} is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called {ProdLDA}, that replaces the mixture model in {LDA} with a product of experts. By changing only one line of code from {LDA}, we find that {ProdLDA} yields much more interpretable topics, even if {LDA} is trained via collapsed Gibbs sampling.},
	journaltitle = {{arXiv}:1703.01488 [stat]},
	author = {Srivastava, Akash and Sutton, Charles},
	urldate = {2020-11-15},
	date = {2017-03-04},
	eprinttype = {arxiv},
	eprint = {1703.01488},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\BKUNXK85\\Srivastava and Sutton - 2017 - Autoencoding Variational Inference For Topic Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\XFCDDERV\\1703.html:text/html}
}

@article{wang_split-merge_2012,
	title = {A Split-Merge {MCMC} Algorithm for the Hierarchical Dirichlet Process},
	url = {http://arxiv.org/abs/1201.1657},
	abstract = {The hierarchical Dirichlet process ({HDP}) has become an important Bayesian nonparametric model for grouped data, such as document collections. The {HDP} is used to construct a ﬂexible mixed-membership model where the number of components is determined by the data. As for most Bayesian nonparametric models, exact posterior inference is intractable—practitioners use Markov chain Monte Carlo ({MCMC}) or variational inference. Inspired by the split-merge {MCMC} algorithm for the Dirichlet process ({DP}) mixture model, we describe a novel split-merge {MCMC} sampling algorithm for posterior inference in the {HDP}. We study its properties on both synthetic data and text corpora. We ﬁnd that split-merge {MCMC} for the {HDP} can provide signiﬁcant improvements over traditional Gibbs sampling, and we give some understanding of the data properties that give rise to larger improvements.},
	journaltitle = {{arXiv}:1201.1657 [cs, stat]},
	author = {Wang, Chong and Blei, David M.},
	urldate = {2021-04-23},
	date = {2012-01-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1201.1657},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Wang and Blei - 2012 - A Split-Merge MCMC Algorithm for the Hierarchical .pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\6RICXGLB\\Wang and Blei - 2012 - A Split-Merge MCMC Algorithm for the Hierarchical .pdf:application/pdf}
}

@article{roberts_model_2016,
	title = {A Model of Text for Experimentation in the Social Sciences},
	volume = {111},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2016.1141684},
	doi = {10.1080/01621459.2016.1141684},
	abstract = {Statistical models of text have become increasingly popular in statistics and computer science as a method of exploring large document collections. Social scientists often want to move beyond exploration, to measurement and experimentation, and make inference about social and political processes that drive discourse and content. In this article, we develop a model of text data that supports this type of substantive research. Our approach is to posit a hierarchical mixed membership model for analyzing topical content of documents, in which mixing weights are parameterized by observed covariates. In this model, topical prevalence and topical content are specified as a simple generalized linear model on an arbitrary number of documentlevel covariates, such as news source and time of release, enabling researchers to introduce elements of the experimental design that informed document collection into the model, within a generally applicable framework. We demonstrate the proposed methodology by analyzing a collection of news reports about China, where we allow the prevalence of topics to evolve over time and vary across newswire services. Our methods quantify the effect of news wire source on both the frequency and nature of topic coverage. Supplementary materials for this article are available online.},
	pages = {988--1003},
	number = {515},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Roberts, Margaret E. and Stewart, Brandon M. and Airoldi, Edoardo M.},
	urldate = {2021-04-23},
	date = {2016-07-02},
	langid = {english},
	file = {Roberts et al. - 2016 - A Model of Text for Experimentation in the Social .pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\9WDAK3BV\\Roberts et al. - 2016 - A Model of Text for Experimentation in the Social .pdf:application/pdf}
}

@article{kingma_auto-encoding_2014,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	journaltitle = {{arXiv}:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2021-05-20},
	date = {2014-05-01},
	eprinttype = {arxiv},
	eprint = {1312.6114},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\3P8IT7UD\\Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\XPQW8VA4\\1312.html:text/html}
}

@article{srivastava_autoencoding_2017-1,
	title = {Autoencoding Variational Inference For Topic Models},
	url = {http://arxiv.org/abs/1703.01488},
	abstract = {Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes ({AEVB}), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective {AEVB} based inference method for latent Dirichlet allocation ({LDA}), which we call Autoencoded Variational Inference For Topic Model ({AVITM}). This model tackles the problems caused for {AEVB} by the Dirichlet prior and by component collapsing. We find that {AVITM} matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because {AVITM} is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called {ProdLDA}, that replaces the mixture model in {LDA} with a product of experts. By changing only one line of code from {LDA}, we find that {ProdLDA} yields much more interpretable topics, even if {LDA} is trained via collapsed Gibbs sampling.},
	journaltitle = {{arXiv}:1703.01488 [stat]},
	author = {Srivastava, Akash and Sutton, Charles},
	urldate = {2021-05-20},
	date = {2017-03-04},
	eprinttype = {arxiv},
	eprint = {1703.01488},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\P8RI525V\\Srivastava and Sutton - 2017 - Autoencoding Variational Inference For Topic Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\V38SEE3M\\1703.html:text/html}
}

@inproceedings{wallach_topic_2006,
	location = {Pittsburgh, Pennsylvania},
	title = {Topic modeling: beyond bag-of-words},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143967},
	doi = {10.1145/1143844.1143967},
	shorttitle = {Topic modeling},
	abstract = {Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the “bag-of-words” assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs {EM} algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful.},
	eventtitle = {the 23rd international conference},
	pages = {977--984},
	booktitle = {Proceedings of the 23rd international conference on Machine learning  - {ICML} '06},
	publisher = {{ACM} Press},
	author = {Wallach, Hanna M.},
	urldate = {2021-08-27},
	date = {2006},
	langid = {english},
	file = {Wallach - 2006 - Topic modeling beyond bag-of-words.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\83KGWCUY\\Wallach - 2006 - Topic modeling beyond bag-of-words.pdf:application/pdf}
}

@inproceedings{wang_topical_2007,
	title = {Topical N-Grams: Phrase and Topic Discovery, with an Application to Information Retrieval},
	doi = {10.1109/ICDM.2007.86},
	shorttitle = {Topical N-Grams},
	abstract = {Most topic models, such as latent Dirichlet allocation, rely on the bag-of-words assumption. However, word order and phrases are often critical to capturing the meaning of text in many text mining tasks. This paper presents topical n-grams, a topic model that discovers topics as well as topical phrases. The probabilistic model generates words in their textual order by, for each word, first sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Thus our model can model "white house" as a special meaning phrase in the 'politics' topic, but not in the 'real estate' topic. Successive bigrams form longer phrases. We present experiments showing meaningful phrases and more interpretable topics from the {NIPS} data and improved information retrieval performance on a {TREC} collection.},
	eventtitle = {Seventh {IEEE} International Conference on Data Mining ({ICDM} 2007)},
	pages = {697--702},
	booktitle = {Seventh {IEEE} International Conference on Data Mining ({ICDM} 2007)},
	author = {Wang, Xuerui and {McCallum}, Andrew and Wei, Xing},
	date = {2007-10},
	note = {{ISSN}: 2374-8486},
	keywords = {Artificial neural networks, Biological neural networks, Context modeling, Data mining, Information retrieval, Natural language processing, Neuroscience, Sampling methods, Text mining, Vocabulary},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\KYXQFWVF\\4470313.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\YTCM8LPV\\Wang et al. - 2007 - Topical N-Grams Phrase and Topic Discovery, with .pdf:application/pdf}
}

@article{bengio_neural_nodate,
	title = {A Neural Probabilistic Language Model},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difﬁcult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to ﬁght the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a signiﬁcant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach signiﬁcantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	pages = {19},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	langid = {english},
	file = {Bengio et al. - A Neural Probabilistic Language Model.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\N47M5TXV\\Bengio et al. - A Neural Probabilistic Language Model.pdf:application/pdf}
}

@inproceedings{wang_topical_2007-1,
	location = {Omaha, {NE}, {USA}},
	title = {Topical N-Grams: Phrase and Topic Discovery, with an Application to Information Retrieval},
	isbn = {978-0-7695-3018-5},
	url = {http://ieeexplore.ieee.org/document/4470313/},
	doi = {10.1109/ICDM.2007.86},
	shorttitle = {Topical N-Grams},
	abstract = {Most topic models, such as latent Dirichlet allocation, rely on the bag-of-words assumption. However, word order and phrases are often critical to capturing the meaning of text in many text mining tasks. This paper presents topical n-grams, a topic model that discovers topics as well as topical phrases. The probabilistic model generates words in their textual order by, for each word, ﬁrst sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-speciﬁc unigram or bigram distribution. Thus our model can model “white house” as a special meaning phrase in the ‘politics’ topic, but not in the ‘real estate’ topic. Successive bigrams form longer phrases. We present experiments showing meaningful phrases and more interpretable topics from the {NIPS} data and improved information retrieval performance on a {TREC} collection.},
	eventtitle = {Seventh {IEEE} International Conference on Data Mining ({ICDM} 2007)},
	pages = {697--702},
	booktitle = {Seventh {IEEE} International Conference on Data Mining ({ICDM} 2007)},
	publisher = {{IEEE}},
	author = {Wang, Xuerui and {McCallum}, Andrew and Wei, Xing},
	urldate = {2021-08-30},
	date = {2007-10},
	langid = {english},
	file = {Wang et al. - 2007 - Topical N-Grams Phrase and Topic Discovery, with .pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\EM6BTMW9\\Wang et al. - 2007 - Topical N-Grams Phrase and Topic Discovery, with .pdf:application/pdf}
}

@inproceedings{wallach_topic_2006-1,
	location = {Pittsburgh, Pennsylvania},
	title = {Topic modeling: beyond bag-of-words},
	isbn = {978-1-59593-383-6},
	url = {http://portal.acm.org/citation.cfm?doid=1143844.1143967},
	doi = {10.1145/1143844.1143967},
	shorttitle = {Topic modeling},
	abstract = {Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the “bag-of-words” assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs {EM} algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful.},
	eventtitle = {the 23rd international conference},
	pages = {977--984},
	booktitle = {Proceedings of the 23rd international conference on Machine learning  - {ICML} '06},
	publisher = {{ACM} Press},
	author = {Wallach, Hanna M.},
	urldate = {2021-08-30},
	date = {2006},
	langid = {english},
	file = {Wallach - 2006 - Topic modeling beyond bag-of-words.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\237ASAFS\\Wallach - 2006 - Topic modeling beyond bag-of-words.pdf:application/pdf}
}

@article{wallach_rethinking_nodate,
	title = {Rethinking {LDA}: Why Priors Matter},
	abstract = {Implementations of topic models typically use symmetric Dirichlet priors with ﬁxed concentration parameters, with the implicit assumption that such “smoothing parameters” have little practical effect. In this paper, we explore several classes of structured priors for topic models. We ﬁnd that an asymmetric Dirichlet prior over the document–topic distributions has substantial advantages over a symmetric prior, while an asymmetric prior over the topic–word distributions provides no real beneﬁt. Approximation of this prior structure through simple, efﬁcient hyperparameter optimization steps is sufﬁcient to achieve these performance gains. The prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language. Since this prior structure can be implemented using efﬁcient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.},
	pages = {11},
	author = {Wallach, Hanna M and Mimno, David and {McCallum}, Andrew},
	langid = {english},
	file = {Wallach et al. - Rethinking LDA Why Priors Matter.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\SPQC2YVR\\Wallach et al. - Rethinking LDA Why Priors Matter.pdf:application/pdf}
}

@article{li_dirichlet_nodate,
	title = {Dirichlet Graph Variational Autoencoder},
	abstract = {Graph Neural Networks ({GNNs}) and Variational Autoencoders ({VAEs}) have been widely used in modeling and generating graphs with latent factors. However, there is no clear explanation of what these latent factors are and why they perform well. In this work, we present Dirichlet Graph Variational Autoencoder ({DGVAE}) with graph cluster memberships as latent factors. Our study connects {VAEs} based graph generation and balanced graph cut, and provides a new way to understand and improve the internal mechanism of {VAEs} based graph generation. Speciﬁcally, we ﬁrst interpret the reconstruction term of {DGVAE} as balanced graph cut in a principled way. Furthermore, motivated by the low pass characteristics in balanced graph cut, we propose a new variant of {GNN} named Heatts to encode the input graph into cluster memberships. Heatts utilizes the Taylor series for fast computation of heat kernels and has better low pass characteristics than Graph Convolutional Networks ({GCN}). Through experiments on graph generation and graph clustering, we demonstrate the effectiveness of our proposed framework.},
	pages = {10},
	author = {Li, Jia and Yu, Jianwei and Li, Jiajin and Zhang, Honglei and Zhao, Kangfei and Rong, Yu and Cheng, Hong and Huang, Junzhou},
	langid = {english},
	file = {Li et al. - Dirichlet Graph Variational Autoencoder.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\XBCYGBA7\\Li et al. - Dirichlet Graph Variational Autoencoder.pdf:application/pdf}
}

@article{cohen_logistic_nodate,
	title = {Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction},
	abstract = {We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational {EM} algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use diﬀerent priors.},
	pages = {8},
	author = {Cohen, Shay B and Gimpel, Kevin and Smith, Noah A},
	langid = {english},
	file = {Cohen et al. - Logistic Normal Priors for Unsupervised Probabilis.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\9VMT9HTD\\Cohen et al. - Logistic Normal Priors for Unsupervised Probabilis.pdf:application/pdf}
}

@article{tran_bayesian_2019,
	title = {Bayesian Layers: A Module for Neural Network Uncertainty},
	url = {http://arxiv.org/abs/1812.03973},
	shorttitle = {Bayesian Layers},
	abstract = {We describe Bayesian Layers, a module designed for fast experimentation with neural network uncertainty. It extends neural network libraries with drop-in replacements for common layers. This enables composition via a unified abstraction over deterministic and stochastic functions and allows for scalability via the underlying system. These layers capture uncertainty over weights (Bayesian neural nets), pre-activation units (dropout), activations ("stochastic output layers"), or the function itself (Gaussian processes). They can also be reversible to propagate uncertainty from input to output. We include code examples for common architectures such as Bayesian {LSTMs}, deep {GPs}, and flow-based models. As demonstration, we fit a 5-billion parameter "Bayesian Transformer" on 512 {TPUv}2 cores for uncertainty in machine translation and a Bayesian dynamics model for model-based planning. Finally, we show how Bayesian Layers can be used within the Edward2 probabilistic programming language for probabilistic programs with stochastic processes.},
	journaltitle = {{arXiv}:1812.03973 [cs, stat]},
	author = {Tran, Dustin and Dusenberry, Michael W. and van der Wilk, Mark and Hafner, Danijar},
	urldate = {2021-08-30},
	date = {2019-03-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.03973},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning},
	file = {Tran et al. - 2019 - Bayesian Layers A Module for Neural Network Uncer.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\2XTBF2UF\\Tran et al. - 2019 - Bayesian Layers A Module for Neural Network Uncer.pdf:application/pdf}
}

@article{xue_bayesian_2021,
	title = {Bayesian Transformer Language Models for Speech Recognition},
	url = {http://arxiv.org/abs/2102.04754},
	abstract = {State-of-the-art neural language models ({LMs}) represented by Transformers are highly complex. Their use of ﬁxed, deterministic parameter estimates fail to account for model uncertainty and lead to over-ﬁtting and poor generalization when given limited training data. In order to address these issues, this paper proposes a full Bayesian learning framework for Transformer {LM} estimation. Efﬁcient variational inference based approaches are used to estimate the latent parameter posterior distributions associated with different parts of the Transformer model architecture including multi-head self-attention, feed forward and embedding layers. Statistically signiﬁcant word error rate ({WER}) reductions up to 0.5\% absolute (3.18\% relative) and consistent perplexity gains were obtained over the baseline Transformer {LMs} on state-of-the-art Switchboard corpus trained {LF}-{MMI} factored {TDNN} systems with i-Vector speaker adaptation. Performance improvements were also obtained on a cross domain {LM} adaptation task requiring porting a Transformer {LM} trained on the Switchboard and Fisher data to a low-resource {DementiaBank} elderly speech corpus.},
	journaltitle = {{arXiv}:2102.04754 [cs]},
	author = {Xue, Boyang and Yu, Jianwei and Xu, Junhao and Liu, Shansong and Hu, Shoukang and Ye, Zi and Geng, Mengzhe and Liu, Xunying and Meng, Helen},
	urldate = {2021-08-30},
	date = {2021-02-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2102.04754},
	keywords = {Computer Science - Computation and Language},
	file = {Xue et al. - 2021 - Bayesian Transformer Language Models for Speech Re.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\NMUWM9KH\\Xue et al. - 2021 - Bayesian Transformer Language Models for Speech Re.pdf:application/pdf}
}

@inproceedings{hennig_kernel_2012,
	title = {Kernel Topic Models},
	url = {https://proceedings.mlr.press/v22/hennig12.html},
	eventtitle = {Artificial Intelligence and Statistics},
	pages = {511--519},
	booktitle = {Artificial Intelligence and Statistics},
	publisher = {{PMLR}},
	author = {Hennig, Philipp and Stern, David and Herbrich, Ralf and Graepel, Thore},
	urldate = {2021-08-30},
	date = {2012-03-21},
	langid = {english},
	note = {{ISSN}: 1938-7228},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\J2YMRMWP\\Hennig et al. - 2012 - Kernel Topic Models.pdf:application/pdf}
}

@article{mikolov_distributed_nodate-1,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	abstract = {The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
	pages = {9},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	langid = {english},
	file = {Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\BD35TVWW\\Mikolov et al. - Distributed Representations of Words and Phrases a.pdf:application/pdf}
}

@article{lee_learning_1999,
	title = {Learning the parts of objects by non-negative matrix factorization},
	volume = {401},
	rights = {1999 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/44565},
	doi = {10.1038/44565},
	abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	pages = {788--791},
	number = {6755},
	journaltitle = {Nature},
	author = {Lee, Daniel D. and Seung, H. Sebastian},
	urldate = {2021-08-30},
	date = {1999-10},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6755
Primary\_atype: Research
Publisher: Nature Publishing Group},
	file = {Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\WNBXMAPW\\44565.html:text/html}
}

@article{vaswani_attention_nodate,
	title = {Attention is All you Need},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.0 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature.},
	pages = {11},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	langid = {english},
	file = {Vaswani et al. - Attention is All you Need.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\CNM9Y5E2\\Vaswani et al. - Attention is All you Need.pdf:application/pdf}
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model ({biLM}), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging {NLP} problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	journaltitle = {{arXiv}:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	urldate = {2021-08-30},
	date = {2018-03-22},
	eprinttype = {arxiv},
	eprint = {1802.05365},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\TFJBY678\\Peters et al. - 2018 - Deep contextualized word representations.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\PZHI5G44\\1802.html:text/html}
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	url = {http://arxiv.org/abs/1907.11692},
	shorttitle = {{RoBERTa}},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of {BERT} pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that {BERT} was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on {GLUE}, {RACE} and {SQuAD}. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	journaltitle = {{arXiv}:1907.11692 [cs]},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	urldate = {2021-08-30},
	date = {2019-07-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.11692},
	keywords = {Computer Science - Computation and Language},
	file = {Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\8ZDR6GVB\\Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf}
}

@article{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2021-08-30},
	date = {2019-05-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\RLJ4KB9C\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@article{gerlach_network_2018,
	title = {A network approach to topic models},
	volume = {4},
	issn = {2375-2548},
	url = {https://advances.sciencemag.org/lookup/doi/10.1126/sciadv.aaq1360},
	doi = {10.1126/sciadv.aaq1360},
	pages = {eaaq1360},
	number = {7},
	journaltitle = {Science Advances},
	shortjournal = {Sci. Adv.},
	author = {Gerlach, Martin and Peixoto, Tiago P. and Altmann, Eduardo G.},
	urldate = {2021-08-30},
	date = {2018-07},
	langid = {english},
	file = {Gerlach et al. - 2018 - A network approach to topic models.pdf:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\YBVPGIAQ\\Gerlach et al. - 2018 - A network approach to topic models.pdf:application/pdf}
}

@article{kingma_auto-encoding_2014-1,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	journaltitle = {{arXiv}:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2021-08-30},
	date = {2014-05-01},
	eprinttype = {arxiv},
	eprint = {1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\EJASH3VA\\Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\3QL4NXES\\1312.html:text/html}
}

@article{bingham_pyro_2019,
	title = {Pyro: Deep Universal Probabilistic Programming},
	volume = {20},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v20/18-403.html},
	shorttitle = {Pyro},
	abstract = {Pyro is a probabilistic programming language built on Python as a platform for developing advanced probabilistic models in {AI} research. To scale to large data sets and high-dimensional models, Pyro uses stochastic variational inference algorithms and probability distributions built on top of {PyTorch}, a modern {GPU}-accelerated deep learning framework. To accommodate complex or model-specific algorithmic behavior, Pyro leverages Poutine, a library of composable building blocks for modifying the behavior of probabilistic programs.},
	pages = {1--6},
	number = {28},
	journaltitle = {Journal of Machine Learning Research},
	author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
	urldate = {2021-09-05},
	date = {2019},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\B7PTZMD7\\Bingham et al. - 2019 - Pyro Deep Universal Probabilistic Programming.pdf:application/pdf;Source Code:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\5NEJ9CC9\\pyro.html:text/html}
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	url = {https://openreview.net/forum?id=BJJsrmfCZ},
	abstract = {A summary of automatic differentiation techniques employed in {PyTorch} library, including novelties like support for in-place modification in presence of objects aliasing the same data, performance...},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and {DeVito}, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	urldate = {2021-09-05},
	date = {2017-10-28},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\AE4UGZ9H\\Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf:application/pdf;Snapshot:C\:\\Users\\HP-\\Google Drive\\Temp\\Zotero\\storage\\7EIM6QZP\\forum.html:text/html}
}