{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-25T12:06:57.613275Z","iopub.execute_input":"2021-12-25T12:06:57.613621Z","iopub.status.idle":"2021-12-25T12:06:57.689639Z","shell.execute_reply.started":"2021-12-25T12:06:57.613525Z","shell.execute_reply":"2021-12-25T12:06:57.688886Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/un-general-debates/un-general-debates.csv\n/kaggle/input/stopwords/stops.txt\n/kaggle/input/nips-papers-1987-2019-updated/papers.csv\n/kaggle/input/nips-papers-1987-2019-updated/authors.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import csv\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport random\nfrom scipy import sparse\nimport itertools\nimport re\nfrom scipy.io import savemat, loadmat\nimport torch\nimport string\nimport os\n\n# Maximum / minimum document frequency\nmax_df = 0.5\nmin_df = 100  # choose desired value for min_df\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Read meta-data\nprint('reading meta-data...')\n# data = pd.read_csv('../input/un-general-debates/un-general-debates.csv')\n# data = data[~data.text.isnull()]\n# docs = data.text.values\ndata = pd.read_csv('../input/nips-papers-1987-2019-updated/papers.csv')\ndata = data[~data.full_text.isnull()]\ndocs = data.full_text.values  ## bows\ntimes = data.year.unique()\ntimes.sort()\n\nts = torch.from_numpy(data.year.to_numpy()).to(device)  ## timestamp\nts = (ts==ts.unique()[:,None]).nonzero().transpose(1,0)[0].to(device)\n\ndef remove_not_printable(in_str):\n    return \"\".join([c for c in in_str if c in string.printable])\n\ndef contains_punctuation(w):\n    return any(char in string.punctuation for char in w)\n\ndef contains_numeric(w):\n    return any(char.isdigit() for char in w)\n\n# document preprocessing\ninit_docs = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]|[\\n]+''', doc) for doc in docs]\ninit_docs = [[w.lower() for w in init_docs[doc] if not contains_punctuation(w)] for doc in range(len(init_docs))]\ninit_docs = [[w for w in init_docs[doc] if not contains_numeric(w)] for doc in range(len(init_docs))]\ninit_docs = [[w for w in init_docs[doc] if len(w) > 1] for doc in range(len(init_docs))]\ninit_docs = [\" \".join(init_docs[doc]) for doc in range(len(init_docs))]","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:06:57.691011Z","iopub.execute_input":"2021-12-25T12:06:57.691233Z","iopub.status.idle":"2021-12-25T12:11:00.013551Z","shell.execute_reply.started":"2021-12-25T12:06:57.691202Z","shell.execute_reply":"2021-12-25T12:11:00.012374Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"reading meta-data...\n","output_type":"stream"}]},{"cell_type":"code","source":"timestamps = data.year.values","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:11:00.015356Z","iopub.execute_input":"2021-12-25T12:11:00.015563Z","iopub.status.idle":"2021-12-25T12:11:00.021195Z","shell.execute_reply.started":"2021-12-25T12:11:00.015540Z","shell.execute_reply":"2021-12-25T12:11:00.020093Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\n# Read raw data\nprint('reading raw data...')\n# docs = []\n# not_found = []\n# timestamps = []\n# for (pid, tt) in zip(all_pids, all_timestamps):\n#     path_read = 'raw/acl_abstracts/acl_data-combined/all_papers'\n#     path_read = os.path.join(path_read, pid + '.txt')\n#     if not os.path.isfile(path_read):\n#         not_found.append(pid)\n#     else:\n#         with open(path_read, 'rb') as f:\n#             doc = f.read().decode('utf-8', 'ignore')\n#             doc = doc.lower().replace('\\n', ' ').replace(\"’\", \" \").replace(\"'\", \" \").translate(str.maketrans(string.punctuation + \"0123456789\", ' '*len(string.punctuation + \"0123456789\"))).split()\n#         doc = [remove_not_printable(w) for w in doc if len(w)>1]\n#         if len(doc) > 1:\n#             doc = \" \".join(doc)\n#             docs.append(doc)\n#             timestamps.append(tt)\n\n# Write as raw text\n# print('writing to text file...')\n# out_filename = './docs_processed.txt'\n# print('writing to text file...')\n# with open(out_filename, 'w') as f:\n#     for line in docs:\n#         f.write(line + '\\n')\n\n# Read stopwords\nwith open('../input/stopwords/stops.txt', 'r') as f:\n    stops = f.read().split('\\n')\n\n# Create count vectorizer\nprint('counting document frequency of words...')\ncvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=frozenset(stops))\ncvz = cvectorizer.fit_transform(init_docs).sign()\n\n# Get vocabulary\nprint('building the vocabulary...')\nsum_counts = cvz.sum(axis=0)\nv_size = sum_counts.shape[1]\nsum_counts_np = np.zeros(v_size, dtype=int)\nfor v in range(v_size):\n    sum_counts_np[v] = sum_counts[0,v]\nword2id = dict([(w, cvectorizer.vocabulary_.get(w)) for w in cvectorizer.vocabulary_])\nid2word = dict([(cvectorizer.vocabulary_.get(w), w) for w in cvectorizer.vocabulary_])\n#del cvectorizer\nprint('  initial vocabulary size: {}'.format(v_size))\n\n# Sort elements in vocabulary\nidx_sort = np.argsort(sum_counts_np)\nvocab_aux = [id2word[idx_sort[cc]] for cc in range(v_size)]\n\n# Filter out stopwords (if any)\nvocab_aux = [w for w in vocab_aux if w not in stops]\nprint('  vocabulary size after removing stopwords from list: {}'.format(len(vocab_aux)))\n\n# Create dictionary and inverse dictionary\nvocab = vocab_aux\ndel vocab_aux\nword2id = dict([(w, j) for j, w in enumerate(vocab)])\nid2word = dict([(j, w) for j, w in enumerate(vocab)])\n\n# Create mapping of timestamps\nall_times = sorted(set(timestamps))\ntime2id = dict([(t, i) for i, t in enumerate(all_times)])\nid2time = dict([(i, t) for i, t in enumerate(all_times)])\ntime_list = [id2time[i] for i in range(len(all_times))]\n\n# Split in train/test/valid\nprint('tokenizing documents and splitting into train/test/valid...')\nnum_docs = cvz.shape[0]\ntrSize = int(np.floor(0.85*num_docs))\ntsSize = int(np.floor(0.10*num_docs))\nvaSize = int(num_docs - trSize - tsSize)\n#del cvz\nidx_permute = np.random.permutation(num_docs).astype(int)\n\n# Remove words not in train_data\nvocab = list(set([w for idx_d in range(trSize) for w in docs[idx_permute[idx_d]].split() if w in word2id]))\nword2id = dict([(w, j) for j, w in enumerate(vocab)])\nid2word = dict([(j, w) for j, w in enumerate(vocab)])\nprint('  vocabulary after removing words not in train: {}'.format(len(vocab)))\n\ndocs_tr = [[word2id[w] for w in docs[idx_permute[idx_d]].split() if w in word2id] for idx_d in range(trSize)]\ntimestamps_tr = [time2id[timestamps[idx_permute[idx_d]]] for idx_d in range(trSize)]\ndocs_ts = [[word2id[w] for w in docs[idx_permute[idx_d+trSize]].split() if w in word2id] for idx_d in range(tsSize)]\ntimestamps_ts = [time2id[timestamps[idx_permute[idx_d+trSize]]] for idx_d in range(tsSize)]\ndocs_va = [[word2id[w] for w in docs[idx_permute[idx_d+trSize+tsSize]].split() if w in word2id] for idx_d in range(vaSize)]\ntimestamps_va = [time2id[timestamps[idx_permute[idx_d+trSize+tsSize]]] for idx_d in range(vaSize)]\n\nprint('  number of documents (train): {} [this should be equal to {} and {}]'.format(len(docs_tr), trSize, len(timestamps_tr)))\nprint('  number of documents (test): {} [this should be equal to {} and {}]'.format(len(docs_ts), tsSize, len(timestamps_ts)))\nprint('  number of documents (valid): {} [this should be equal to {} and {}]'.format(len(docs_va), vaSize, len(timestamps_va)))\n\n# Remove empty documents\nprint('removing empty documents...')\n\ndef remove_empty(in_docs, in_timestamps):\n    out_docs = []\n    out_timestamps = []\n    for ii, doc in enumerate(in_docs):\n        if(doc!=[]):\n            out_docs.append(doc)\n            out_timestamps.append(in_timestamps[ii])\n    return out_docs, out_timestamps\n\ndef remove_by_threshold(in_docs, in_timestamps, thr):\n    out_docs = []\n    out_timestamps = []\n    for ii, doc in enumerate(in_docs):\n        if(len(doc)>thr):\n            out_docs.append(doc)\n            out_timestamps.append(in_timestamps[ii])\n    return out_docs, out_timestamps\n\ndocs_tr, timestamps_tr = remove_empty(docs_tr, timestamps_tr)\ndocs_ts, timestamps_ts = remove_empty(docs_ts, timestamps_ts)\ndocs_va, timestamps_va = remove_empty(docs_va, timestamps_va)\n\n# Remove test documents with length=1\ndocs_ts, timestamps_ts = remove_by_threshold(docs_ts, timestamps_ts, 1)\n\n# Split test set in 2 halves\nprint('splitting test documents in 2 halves...')\ndocs_ts_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in docs_ts]\ndocs_ts_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in docs_ts]\n\n# Getting lists of words and doc_indices\nprint('creating lists of words...')\n\ndef create_list_words(in_docs):\n    return [x for y in in_docs for x in y]\n\nwords_tr = create_list_words(docs_tr)\nwords_ts = create_list_words(docs_ts)\nwords_ts_h1 = create_list_words(docs_ts_h1)\nwords_ts_h2 = create_list_words(docs_ts_h2)\nwords_va = create_list_words(docs_va)\n\nprint('  len(words_tr): ', len(words_tr))\nprint('  len(words_ts): ', len(words_ts))\nprint('  len(words_ts_h1): ', len(words_ts_h1))\nprint('  len(words_ts_h2): ', len(words_ts_h2))\nprint('  len(words_va): ', len(words_va))\n\n# Get doc indices\nprint('getting doc indices...')\n\ndef create_doc_indices(in_docs):\n    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n    return [int(x) for y in aux for x in y]\n\ndoc_indices_tr = create_doc_indices(docs_tr)\ndoc_indices_ts = create_doc_indices(docs_ts)\ndoc_indices_ts_h1 = create_doc_indices(docs_ts_h1)\ndoc_indices_ts_h2 = create_doc_indices(docs_ts_h2)\ndoc_indices_va = create_doc_indices(docs_va)\n\nprint('  len(np.unique(doc_indices_tr)): {} [this should be {}]'.format(len(np.unique(doc_indices_tr)), len(docs_tr)))\nprint('  len(np.unique(doc_indices_ts)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts)), len(docs_ts)))\nprint('  len(np.unique(doc_indices_ts_h1)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h1)), len(docs_ts_h1)))\nprint('  len(np.unique(doc_indices_ts_h2)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h2)), len(docs_ts_h2)))\nprint('  len(np.unique(doc_indices_va)): {} [this should be {}]'.format(len(np.unique(doc_indices_va)), len(docs_va)))\n\n# Number of documents in each set\nn_docs_tr = len(docs_tr)\nn_docs_ts = len(docs_ts)\nn_docs_ts_h1 = len(docs_ts_h1)\nn_docs_ts_h2 = len(docs_ts_h2)\nn_docs_va = len(docs_va)\n\n# Remove unused variables\ndel docs_tr\ndel docs_ts\ndel docs_ts_h1\ndel docs_ts_h2\ndel docs_va\n\n# Create bow representation\nprint('creating bow representation...')\n\ndef create_bow(doc_indices, words, n_docs, vocab_size):\n    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n\nbow_tr = create_bow(doc_indices_tr, words_tr, n_docs_tr, len(vocab))\nbow_ts = create_bow(doc_indices_ts, words_ts, n_docs_ts, len(vocab))\nbow_ts_h1 = create_bow(doc_indices_ts_h1, words_ts_h1, n_docs_ts_h1, len(vocab))\nbow_ts_h2 = create_bow(doc_indices_ts_h2, words_ts_h2, n_docs_ts_h2, len(vocab))\nbow_va = create_bow(doc_indices_va, words_va, n_docs_va, len(vocab))\n\ndel words_tr\ndel words_ts\ndel words_ts_h1\ndel words_ts_h2\ndel words_va\ndel doc_indices_tr\ndel doc_indices_ts\ndel doc_indices_ts_h1\ndel doc_indices_ts_h2\ndel doc_indices_va\n\n# Write files for LDA C++ code\ndef write_lda_file(filename, timestamps_in, time_list_in, bow_in):\n    idxSort = np.argsort(timestamps_in)\n    \n    with open(filename, \"w\") as f:\n        for row in idxSort:\n            x = bow_in.getrow(row)\n            n_elems = x.count_nonzero()\n            f.write(str(n_elems))\n            if(n_elems != len(x.indices) or n_elems != len(x.data)):\n                raise ValueError(\"[ERR] THIS SHOULD NOT HAPPEN\")\n            for ii, dd in zip(x.indices, x.data):\n                f.write(' ' + str(ii) + ':' + str(dd))\n            f.write('\\n')\n            \n    with open(filename.replace(\"-mult\", \"-seq\"), \"w\") as f:\n        f.write(str(len(time_list_in)) + '\\n')\n        for idx_t, _ in enumerate(time_list_in):\n            n_elem = len([t for t in timestamps_in if t==idx_t])\n            f.write(str(n_elem) + '\\n')\n            \n\npath_save = './min_df_' + str(min_df) + '/'\nif not os.path.isdir(path_save):\n    os.system('mkdir -p ' + path_save)\n\n# Write files for LDA C++ code\nprint('saving LDA files for C++ code...')\nwrite_lda_file(path_save + 'dtm_tr-mult.dat', timestamps_tr, time_list, bow_tr)\nwrite_lda_file(path_save + 'dtm_ts-mult.dat', timestamps_ts, time_list, bow_ts)\nwrite_lda_file(path_save + 'dtm_ts_h1-mult.dat', timestamps_ts, time_list, bow_ts_h1)\nwrite_lda_file(path_save + 'dtm_ts_h2-mult.dat', timestamps_ts, time_list, bow_ts_h2)\nwrite_lda_file(path_save + 'dtm_va-mult.dat', timestamps_va, time_list, bow_va)\n\n# Also write the vocabulary and timestamps\nwith open(path_save + 'vocab.txt', \"w\") as f:\n    for v in vocab:\n        f.write(v + '\\n')\n\nwith open(path_save + 'timestamps.txt', \"w\") as f:\n    for t in time_list:\n        f.write(str(t) + '\\n')\n\nwith open(path_save + 'vocab.pkl', 'wb') as f:\n    pickle.dump(vocab, f)\ndel vocab\n\nwith open(path_save + 'timestamps.pkl', 'wb') as f:\n    pickle.dump(time_list, f)\n\n# Save timestamps alone\nsavemat(path_save + 'bow_tr_timestamps', {'timestamps': timestamps_tr}, do_compression=True)\nsavemat(path_save + 'bow_ts_timestamps', {'timestamps': timestamps_ts}, do_compression=True)\nsavemat(path_save + 'bow_va_timestamps', {'timestamps': timestamps_va}, do_compression=True)\n\n# Split bow intro token/value pairs\nprint('splitting bow intro token/value pairs and saving to disk...')\n\ndef split_bow(bow_in, n_docs):\n    indices = [[w for w in bow_in[doc,:].indices] for doc in range(n_docs)]\n    counts = [[c for c in bow_in[doc,:].data] for doc in range(n_docs)]\n    return indices, counts\n\nbow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_docs_tr)\nsavemat(path_save + 'bow_tr_tokens', {'tokens': bow_tr_tokens}, do_compression=True)\nsavemat(path_save + 'bow_tr_counts', {'counts': bow_tr_counts}, do_compression=True)\ndel bow_tr\ndel bow_tr_tokens\ndel bow_tr_counts\n\nbow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_docs_ts)\nsavemat(path_save + 'bow_ts_tokens', {'tokens': bow_ts_tokens}, do_compression=True)\nsavemat(path_save + 'bow_ts_counts', {'counts': bow_ts_counts}, do_compression=True)\ndel bow_ts\ndel bow_ts_tokens\ndel bow_ts_counts\n\nbow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_docs_ts_h1)\nsavemat(path_save + 'bow_ts_h1_tokens', {'tokens': bow_ts_h1_tokens}, do_compression=True)\nsavemat(path_save + 'bow_ts_h1_counts', {'counts': bow_ts_h1_counts}, do_compression=True)\ndel bow_ts_h1\ndel bow_ts_h1_tokens\ndel bow_ts_h1_counts\n\nbow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_docs_ts_h2)\nsavemat(path_save + 'bow_ts_h2_tokens', {'tokens': bow_ts_h2_tokens}, do_compression=True)\nsavemat(path_save + 'bow_ts_h2_counts', {'counts': bow_ts_h2_counts}, do_compression=True)\ndel bow_ts_h2\ndel bow_ts_h2_tokens\ndel bow_ts_h2_counts\n\nbow_va_tokens, bow_va_counts = split_bow(bow_va, n_docs_va)\nsavemat(path_save + 'bow_va_tokens', {'tokens': bow_va_tokens}, do_compression=True)\nsavemat(path_save + 'bow_va_counts', {'counts': bow_va_counts}, do_compression=True)\ndel bow_va\ndel bow_va_tokens\ndel bow_va_counts\n\nprint('Data ready !!')\nprint('*************')","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:11:00.023217Z","iopub.execute_input":"2021-12-25T12:11:00.023434Z","iopub.status.idle":"2021-12-25T12:12:58.344830Z","shell.execute_reply.started":"2021-12-25T12:11:00.023411Z","shell.execute_reply":"2021-12-25T12:12:58.344068Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"reading raw data...\ncounting document frequency of words...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'll', 'mon', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n  'stop_words.' % sorted(inconsistent))\n","output_type":"stream"},{"name":"stdout","text":"building the vocabulary...\n  initial vocabulary size: 9179\n  vocabulary size after removing stopwords from list: 9179\ntokenizing documents and splitting into train/test/valid...\n  vocabulary after removing words not in train: 7756\n  number of documents (train): 8225 [this should be equal to 8225 and 8225]\n  number of documents (test): 967 [this should be equal to 967 and 967]\n  number of documents (valid): 485 [this should be equal to 485 and 485]\nremoving empty documents...\nsplitting test documents in 2 halves...\ncreating lists of words...\n  len(words_tr):  7602764\n  len(words_ts):  888447\n  len(words_ts_h1):  443992\n  len(words_ts_h2):  444455\n  len(words_va):  448550\ngetting doc indices...\n  len(np.unique(doc_indices_tr)): 8179 [this should be 8179]\n  len(np.unique(doc_indices_ts)): 960 [this should be 960]\n  len(np.unique(doc_indices_ts_h1)): 960 [this should be 960]\n  len(np.unique(doc_indices_ts_h2)): 960 [this should be 960]\n  len(np.unique(doc_indices_va)): 483 [this should be 483]\ncreating bow representation...\nsaving LDA files for C++ code...\nsplitting bow intro token/value pairs and saving to disk...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  return array(a, dtype, copy=False, order=order, subok=True)\n","output_type":"stream"},{"name":"stdout","text":"Data ready !!\n*************\n","output_type":"stream"}]},{"cell_type":"markdown","source":"kl divergence","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport torch \nimport numpy as np\nimport bokeh.plotting as bp\n\nfrom bokeh.plotting import save\nfrom bokeh.models import HoverTool\nimport matplotlib.pyplot as plt \nimport matplotlib \n\ntiny = 1e-6\n\ndef _reparameterize(mu, logvar, num_samples):\n    \"\"\"Applies the reparameterization trick to return samples from a given q\"\"\"\n    std = torch.exp(0.5 * logvar) \n    bsz, zdim = logvar.size()\n    eps = torch.randn(num_samples, bsz, zdim).to(mu.device)\n    mu = mu.unsqueeze(0)\n    std = std.unsqueeze(0)\n    res = eps.mul_(std).add_(mu)\n    return res\n\ndef get_document_frequency(data, wi, wj=None):\n    if wj is None:\n        D_wi = 0\n        for l in range(len(data)):\n            doc = data[l].squeeze(0)\n            if len(doc) == 1: \n                continue\n                #doc = [doc.squeeze()]\n            else:\n                doc = doc.squeeze()\n            if wi in doc:\n                D_wi += 1\n        return D_wi\n    D_wj = 0\n    D_wi_wj = 0\n    for l in range(len(data)):\n        doc = data[l].squeeze(0)\n        if len(doc) == 1: \n            doc = [doc.squeeze()]\n        else:\n            doc = doc.squeeze()\n        if wj in doc:\n            D_wj += 1\n            if wi in doc:\n                D_wi_wj += 1\n    return D_wj, D_wi_wj \n\ndef get_topic_coherence(beta, data, vocab):\n    D = len(data) ## number of docs...data is list of documents\n    print('D: ', D)\n    TC = []\n    num_topics = len(beta)\n    for k in range(num_topics):\n        print('k: {}/{}'.format(k, num_topics))\n        top_10 = list(beta[k].argsort()[-11:][::-1])\n        top_words = [vocab[a] for a in top_10]\n        TC_k = 0\n        counter = 0\n        for i, word in enumerate(top_10):\n            # get D(w_i)\n            D_wi = get_document_frequency(data, word)\n            j = i + 1\n            tmp = 0\n            while j < len(top_10) and j > i:\n                # get D(w_j) and D(w_i, w_j)\n                D_wj, D_wi_wj = get_document_frequency(data, word, top_10[j])\n                # get f(w_i, w_j)\n                if D_wi_wj == 0:\n                    f_wi_wj = -1\n                else:\n                    f_wi_wj = -1 + ( np.log(D_wi) + np.log(D_wj)  - 2.0 * np.log(D) ) / ( np.log(D_wi_wj) - np.log(D) )\n                # update tmp: \n                tmp += f_wi_wj\n                j += 1\n                counter += 1\n            # update TC_k\n            TC_k += tmp \n        TC.append(TC_k)\n    print('counter: ', counter)\n    print('num topics: ', len(TC))\n    #TC = np.mean(TC) / counter\n    print('Topic Coherence is: {}'.format(TC))\n    return TC, counter\n\ndef log_gaussian(z, mu=None, logvar=None):\n    sz = z.size()\n    d = z.size(2)\n    bsz = z.size(1)\n    if mu is None or logvar is None:\n        mu = torch.zeros(bsz, d).to(z.device)\n        logvar = torch.zeros(bsz, d).to(z.device)\n    mu = mu.unsqueeze(0)\n    logvar = logvar.unsqueeze(0)\n    var = logvar.exp()\n    log_density = ((z - mu)**2 / (var+tiny)).sum(2) # b\n    log_det = logvar.sum(2) # b\n    log_density = log_density + log_det + d*np.log(2*np.pi)\n    return -0.5*log_density\n\ndef logsumexp(x, dim=0):\n    d = torch.max(x, dim)[0]   \n    if x.dim() == 1:\n        return torch.log(torch.exp(x - d).sum(dim)) + d\n    else:\n        return torch.log(torch.exp(x - d.unsqueeze(dim).expand_as(x)).sum(dim) + tiny) + d\n\ndef flatten_docs(docs): #to get words and doc_indices\n    words = [x for y in docs for x in y]\n    doc_indices = [[j for _ in doc] for j, doc in enumerate(docs)]\n    doc_indices = [x for y in doc_indices for x in y]\n    return words, doc_indices\n    \ndef onehot(data, min_length):\n    return list(np.bincount(data, minlength=min_length))\n\ndef nearest_neighbors(word, embeddings, vocab, num_words):\n    vectors = embeddings.cpu().numpy() \n    index = vocab.index(word)\n    query = embeddings[index].cpu().numpy() \n    ranks = vectors.dot(query).squeeze()\n    denom = query.T.dot(query).squeeze()\n    denom = denom * np.sum(vectors**2, 1)\n    denom = np.sqrt(denom)\n    ranks = ranks / denom\n    mostSimilar = []\n    [mostSimilar.append(idx) for idx in ranks.argsort()[::-1]]\n    nearest_neighbors = mostSimilar[:num_words]\n    nearest_neighbors = [vocab[comp] for comp in nearest_neighbors]\n    return nearest_neighbors\n\ndef visualize(docs, _lda_keys, topics, theta):\n    tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n    # project to 2D\n    tsne_lda = tsne_model.fit_transform(theta)\n    colormap = []\n    for name, hex in matplotlib.colors.cnames.items():\n        colormap.append(hex)\n\n    colormap = colormap[:len(theta[0, :])]\n    colormap = np.array(colormap)\n\n    title = '20 newsgroups TE embedding V viz'\n    num_example = len(docs)\n\n    plot_lda = bp.figure(plot_width=1400, plot_height=1100,\n                     title=title,\n                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n                     x_axis_type=None, y_axis_type=None, min_border=1)\n\n    plt.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1],\n                 color=colormap[_lda_keys][:num_example])\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:12:58.346255Z","iopub.execute_input":"2021-12-25T12:12:58.346522Z","iopub.status.idle":"2021-12-25T12:12:59.489324Z","shell.execute_reply.started":"2021-12-25T12:12:58.346483Z","shell.execute_reply":"2021-12-25T12:12:59.488504Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\n\nimport numpy as np\nimport scipy.io\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef _fetch(path, name):\n    if name == 'train':\n        token_file = os.path.join(path, 'bow_tr_tokens')\n        count_file = os.path.join(path, 'bow_tr_counts')\n    elif name == 'valid':\n        token_file = os.path.join(path, 'bow_va_tokens')\n        count_file = os.path.join(path, 'bow_va_counts')\n    else:\n        token_file = os.path.join(path, 'bow_ts_tokens')\n        count_file = os.path.join(path, 'bow_ts_counts')\n    tokens = scipy.io.loadmat(token_file)['tokens'].squeeze()\n    counts = scipy.io.loadmat(count_file)['counts'].squeeze()\n    if name == 'test':\n        token_1_file = os.path.join(path, 'bow_ts_h1_tokens')\n        count_1_file = os.path.join(path, 'bow_ts_h1_counts')\n        token_2_file = os.path.join(path, 'bow_ts_h2_tokens')\n        count_2_file = os.path.join(path, 'bow_ts_h2_counts')\n        tokens_1 = scipy.io.loadmat(token_1_file)['tokens'].squeeze()\n        counts_1 = scipy.io.loadmat(count_1_file)['counts'].squeeze()\n        tokens_2 = scipy.io.loadmat(token_2_file)['tokens'].squeeze()\n        counts_2 = scipy.io.loadmat(count_2_file)['counts'].squeeze()\n        return {'tokens': tokens, 'counts': counts, 'tokens_1': tokens_1, 'counts_1': counts_1, 'tokens_2': tokens_2,\n                'counts_2': counts_2}\n    return {'tokens': tokens, 'counts': counts}\n\n\ndef _fetch_temporal(path, name):\n    if name == 'train':\n        token_file = os.path.join(path, 'bow_tr_tokens')\n        count_file = os.path.join(path, 'bow_tr_counts')\n        time_file = os.path.join(path, 'bow_tr_timestamps')\n    elif name == 'valid':\n        token_file = os.path.join(path, 'bow_va_tokens')\n        count_file = os.path.join(path, 'bow_va_counts')\n        time_file = os.path.join(path, 'bow_va_timestamps')\n    else:\n        token_file = os.path.join(path, 'bow_ts_tokens')\n        count_file = os.path.join(path, 'bow_ts_counts')\n        time_file = os.path.join(path, 'bow_ts_timestamps')\n    tokens = scipy.io.loadmat(token_file)['tokens'].squeeze()\n    counts = scipy.io.loadmat(count_file)['counts'].squeeze()\n    times = scipy.io.loadmat(time_file)['timestamps'].squeeze()\n    if name == 'test':\n        token_1_file = os.path.join(path, 'bow_ts_h1_tokens')\n        count_1_file = os.path.join(path, 'bow_ts_h1_counts')\n        token_2_file = os.path.join(path, 'bow_ts_h2_tokens')\n        count_2_file = os.path.join(path, 'bow_ts_h2_counts')\n        tokens_1 = scipy.io.loadmat(token_1_file)['tokens'].squeeze()\n        counts_1 = scipy.io.loadmat(count_1_file)['counts'].squeeze()\n        tokens_2 = scipy.io.loadmat(token_2_file)['tokens'].squeeze()\n        counts_2 = scipy.io.loadmat(count_2_file)['counts'].squeeze()\n        return {'tokens': tokens, 'counts': counts, 'times': times,\n                'tokens_1': tokens_1, 'counts_1': counts_1,\n                'tokens_2': tokens_2, 'counts_2': counts_2}\n    return {'tokens': tokens, 'counts': counts, 'times': times}\n\n\ndef get_data(path, temporal=False):\n    ### load vocabulary\n    with open(os.path.join(path, 'vocab.pkl'), 'rb') as f:\n        vocab = pickle.load(f)\n\n    if not temporal:\n        train = _fetch(path, 'train')\n        valid = _fetch(path, 'valid')\n        test = _fetch(path, 'test')\n    else:\n        train = _fetch_temporal(path, 'train')\n        valid = _fetch_temporal(path, 'valid')\n        test = _fetch_temporal(path, 'test')\n\n    return vocab, train, valid, test\n\n\ndef get_batch(tokens, counts, ind, vocab_size, temporal=False, times=None):\n    \"\"\"fetch input data by batch.\"\"\"\n    batch_size = len(ind)\n    data_batch = np.zeros((batch_size, vocab_size))\n    if temporal:\n        times_batch = np.zeros((batch_size,))\n    for i, doc_id in enumerate(ind):\n        doc = tokens[doc_id]\n        count = counts[doc_id]\n        if temporal:\n            timestamp = times[doc_id]\n            times_batch[i] = timestamp\n        L = count.shape[1]\n        if len(doc) == 1:\n            doc = [doc.squeeze()]\n            count = [count.squeeze()]\n        else:\n            doc = doc.squeeze()\n            count = count.squeeze()\n        if doc_id != -1:\n            for j, word in enumerate(doc):\n                data_batch[i, word] = count[j]\n    data_batch = torch.from_numpy(data_batch).float().to(device)\n    if temporal:\n        times_batch = torch.from_numpy(times_batch).to(device)\n        return data_batch, times_batch\n    return data_batch\n\n\ndef get_rnn_input(tokens, counts, times, num_times, vocab_size, num_docs):\n    # shuffle\n    indices = torch.randperm(num_docs)\n    # split to 1000 sets\n    indices = torch.split(indices, 1000)\n    # TxV\n    rnn_input = torch.zeros(num_times, vocab_size).to(device)\n    # times count\n    cnt = torch.zeros(num_times, ).to(device)\n    for idx, ind in enumerate(indices):\n        data_batch, times_batch = get_batch(tokens, counts, ind, vocab_size, temporal=True, times=times)\n        for t in range(num_times):\n            tmp = (times_batch == t).nonzero()\n            docs = data_batch[tmp].squeeze().sum(0)\n            rnn_input[t] += docs\n            cnt[t] += len(tmp)\n        if idx % 20 == 0:\n            print('idx: {}/{}'.format(idx, len(indices)))\n    rnn_input = rnn_input / cnt.unsqueeze(1)\n    return rnn_input\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:12:59.491056Z","iopub.execute_input":"2021-12-25T12:12:59.491338Z","iopub.status.idle":"2021-12-25T12:12:59.533626Z","shell.execute_reply.started":"2021-12-25T12:12:59.491301Z","shell.execute_reply":"2021-12-25T12:12:59.531908Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"This file defines a dynamic etm object.\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass DETM(nn.Module):\n    def __init__(self, args, embeddings):\n        super(DETM, self).__init__()\n\n        ## define hyperparameters\n        self.num_topics = args.num_topics\n\n        self.num_times = args.num_times\n        self.vocab_size = args.vocab_size\n        self.t_hidden_size = args.t_hidden_size\n        self.eta_hidden_size = args.eta_hidden_size\n        self.rho_size = args.rho_size\n        self.emsize = args.emb_size\n        self.enc_drop = args.enc_drop\n        self.eta_nlayers = args.eta_nlayers\n        self.t_drop = nn.Dropout(args.enc_drop)\n        self.delta = args.delta\n        self.train_embeddings = args.train_embeddings\n\n        self.theta_act = self.get_activation(args.theta_act)\n\n        ## define the word embedding matrix \\rho\n        if args.train_embeddings:\n            self.rho = nn.Linear(args.rho_size, args.vocab_size, bias=False)\n        else:\n            num_embeddings, emsize = embeddings.size()\n            rho = nn.Embedding(num_embeddings, emsize)\n            rho.weight.data = embeddings\n            self.rho = rho.weight.data.clone().float().to(device)\n\n        ## define the variational parameters for the topic embeddings over time (alpha) ... alpha is K x T x L\n        self.mu_q_alpha = nn.Parameter(torch.randn(args.num_topics, args.num_times, args.rho_size))\n        self.logsigma_q_alpha = nn.Parameter(torch.randn(args.num_topics, args.num_times, args.rho_size))\n\n        ## define variational distribution for \\eta via amortizartion... eta is K x T\n        ## Eta\n        ## V->L\n        ## L+K->K\n        self.q_eta_map = nn.Linear(args.vocab_size, args.eta_hidden_size)\n        self.q_eta = nn.LSTM(args.eta_hidden_size, args.eta_hidden_size, args.eta_nlayers, dropout=args.eta_dropout)\n\n        self.mu_q_eta = nn.Linear(args.eta_hidden_size + args.num_topics, args.num_topics, bias=True)\n        self.logsigma_q_eta = nn.Linear(args.eta_hidden_size + args.num_topics, args.num_topics, bias=True)\n\n        ## define variational distribution for \\theta_{1:D} via amortizartion... theta is K x D\n        ## Theta\n        ## V+K->K\n        self.q_theta = nn.Sequential(\n            nn.Linear(args.vocab_size + args.num_topics, args.t_hidden_size),\n            self.theta_act,\n            nn.Linear(args.t_hidden_size, args.t_hidden_size),\n            self.theta_act,\n        )\n        self.logsigma_q_theta = nn.Linear(args.t_hidden_size, args.num_topics, bias=True)\n        self.mu_q_theta = nn.Linear(args.t_hidden_size, args.num_topics, bias=True)\n\n    def get_activation(self, act):\n        if act == 'tanh':\n            act = nn.Tanh()\n        elif act == 'relu':\n            act = nn.ReLU()\n        elif act == 'softplus':\n            act = nn.Softplus()\n        elif act == 'rrelu':\n            act = nn.RReLU()\n        elif act == 'leakyrelu':\n            act = nn.LeakyReLU()\n        elif act == 'elu':\n            act = nn.ELU()\n        elif act == 'selu':\n            act = nn.SELU()\n        elif act == 'glu':\n            act = nn.GLU()\n        else:\n            print('Defaulting to tanh activations...')\n            act = nn.Tanh()\n        return act\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"Returns a sample from a Gaussian distribution via reparameterization.\n        \"\"\"\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std)\n            return eps.mul_(std).add_(mu)\n        else:\n            return mu\n\n    def get_kl(self, q_mu, q_logsigma, p_mu=None, p_logsigma=None):\n        \"\"\" Gaussian KL Divergence\n        Returns KL( N(q_mu, q_logsigma) || N(p_mu, p_logsigma) ).\n        \"\"\"\n        if p_mu is not None and p_logsigma is not None:\n            sigma_q_sq = torch.exp(q_logsigma)\n            sigma_p_sq = torch.exp(p_logsigma)\n            kl = (sigma_q_sq + (q_mu - p_mu) ** 2) / (sigma_p_sq + 1e-6)\n            kl = kl - 1 + p_logsigma - q_logsigma\n            kl = 0.5 * torch.sum(kl, dim=-1)\n        else:\n            kl = -0.5 * torch.sum(1 + q_logsigma - q_mu.pow(2) - q_logsigma.exp(), dim=-1)\n        return kl\n\n    # Compute α[t] ~ N(α[t-1],γ^2*I), with reparam trick\n    # Compute KL(N(μσ),N())\n    # alpha TxKxL\n    def get_alpha(self):  ## mean field\n        # TxKxL\n        alphas = torch.zeros(self.num_times, self.num_topics, self.rho_size).to(device)\n        kl_alpha = []\n        # rt\n        alphas[0] = self.reparameterize(self.mu_q_alpha[:, 0, :], self.logsigma_q_alpha[:, 0, :])\n        p_mu_0 = torch.zeros(self.num_topics, self.rho_size).to(device)\n        logsigma_p_0 = torch.zeros(self.num_topics, self.rho_size).to(device)\n        # kl-divergence for 0\n        kl_0 = self.get_kl(self.mu_q_alpha[:, 0, :], self.logsigma_q_alpha[:, 0, :], p_mu_0, logsigma_p_0)\n        kl_alpha.append(kl_0)\n        # for each\n        # rt\n        for t in range(1, self.num_times):\n            alphas[t] = self.reparameterize(self.mu_q_alpha[:, t, :], self.logsigma_q_alpha[:, t, :])\n            # kl-divergence  || N(a[t-1],s^2*I)\n            p_mu_t = alphas[t - 1]\n            logsigma_p_t = torch.log(self.delta * torch.ones(self.num_topics, self.rho_size).to(device))\n            kl_t = self.get_kl(self.mu_q_alpha[:, t, :], self.logsigma_q_alpha[:, t, :], p_mu_t, logsigma_p_t)\n            kl_alpha.append(kl_t)\n        kl_alpha = torch.stack(kl_alpha).sum()\n        return alphas, kl_alpha.sum()\n\n    # Compute η[t]~N(η[t-1], δ^2*I), η[0]=\n    def get_eta(self, rnn_inp):  ## structured amortized inference\n        inp = self.q_eta_map(rnn_inp).unsqueeze(1)\n        hidden = self.init_hidden()\n        output, _ = self.q_eta(inp, hidden)\n        output = output.squeeze()\n\n        etas = torch.zeros(self.num_times, self.num_topics).to(device)\n        kl_eta = []\n        # Compute η[0]\n        inp_0 = torch.cat([output[0], torch.zeros(self.num_topics, ).to(device)], dim=0)\n        mu_0 = self.mu_q_eta(inp_0)\n        logsigma_0 = self.logsigma_q_eta(inp_0)\n        etas[0] = self.reparameterize(mu_0, logsigma_0)\n\n        p_mu_0 = torch.zeros(self.num_topics, ).to(device)\n        logsigma_p_0 = torch.zeros(self.num_topics, ).to(device)\n        kl_0 = self.get_kl(mu_0, logsigma_0, p_mu_0, logsigma_p_0)\n        kl_eta.append(kl_0)\n\n        # for t:[1,T]\n        for t in range(1, self.num_times):\n            inp_t = torch.cat([output[t], etas[t - 1]], dim=0)\n            mu_t = self.mu_q_eta(inp_t)\n            logsigma_t = self.logsigma_q_eta(inp_t)\n            etas[t] = self.reparameterize(mu_t, logsigma_t)\n\n            logsigma_p_t = torch.log(self.delta * torch.ones(self.num_topics, ).to(device))\n            kl_t = self.get_kl(mu_t, logsigma_t, etas[t - 1], logsigma_p_t)\n            kl_eta.append(kl_t)\n        kl_eta = torch.stack(kl_eta).sum()\n        return etas, kl_eta\n\n    # θ~LN(η[t],α^2*I)\n    # get theta according to the timestamp\n    # input:\n    def get_theta(self, eta, bows, times):  # amortized inference\n        \"\"\"Returns the topic proportions.\"\"\"\n        eta_td = eta[times.type('torch.LongTensor')]\n        inp = torch.cat([bows, eta_td], dim=1)\n        q_theta = self.q_theta(inp)\n        if self.enc_drop > 0:\n            q_theta = self.t_drop(q_theta)\n        mu_theta = self.mu_q_theta(q_theta)\n        logsigma_theta = self.logsigma_q_theta(q_theta)\n        z = self.reparameterize(mu_theta, logsigma_theta)\n        theta = F.softmax(z, dim=-1)\n        kl_theta = self.get_kl(mu_theta, logsigma_theta, eta_td, torch.zeros(self.num_topics).to(device))\n        return theta, kl_theta\n\n    # w_d~softmax(ρ*α[t_d])\n    # return\n    def get_beta(self, alpha):\n        \"\"\"Returns the topic matrix \\beta of shape K x V\"\"\"\n        if self.train_embeddings:\n            logit = self.rho(alpha.view(alpha.size(0) * alpha.size(1), self.rho_size))\n        else:\n            tmp = alpha.view(alpha.size(0) * alpha.size(1), self.rho_size)\n            logit = torch.mm(tmp, self.rho.permute(1, 0))\n        logit = logit.view(alpha.size(0), alpha.size(1), -1)\n        beta = F.softmax(logit, dim=-1)\n        return beta\n\n    def get_nll(self, theta, beta, bows):\n        theta = theta.unsqueeze(1)\n        loglik = torch.bmm(theta, beta).squeeze(1)\n        loglik = torch.log(loglik + 1e-6)\n        nll = -loglik * bows\n        nll = nll.sum(-1)\n        return nll\n\n    def forward(self, bows, normalized_bows, times, rnn_inp, num_docs):\n        bsz = normalized_bows.size(0)\n        coeff = num_docs / bsz\n        # 1. get alpha TxKxL\n        alpha, kl_alpha = self.get_alpha()\n        # 2. get eta TxK\n        eta, kl_eta = self.get_eta(rnn_inp)\n        # 3. get theta DxK\n        theta, kl_theta = self.get_theta(eta, normalized_bows, times)\n        kl_theta = kl_theta.sum() * coeff\n        # 4. get beta (rho*alpha)\n        beta = self.get_beta(alpha)\n        beta = beta[times.type('torch.LongTensor')]\n        # 5. get nll loss\n        nll = self.get_nll(theta, beta, bows)\n        nll = nll.sum() * coeff\n        nelbo = nll + kl_alpha + kl_eta + kl_theta\n        return nelbo, nll, kl_alpha, kl_eta, kl_theta\n\n    def init_hidden(self):\n        \"\"\"Initializes the first hidden state of the RNN used as inference network for \\eta.\n        \"\"\"\n        weight = next(self.parameters())\n        nlayers = self.eta_nlayers\n        nhid = self.eta_hidden_size\n        return (weight.new_zeros(nlayers, 1, nhid), weight.new_zeros(nlayers, 1, nhid))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:12:59.535556Z","iopub.execute_input":"2021-12-25T12:12:59.535824Z","iopub.status.idle":"2021-12-25T12:12:59.601606Z","shell.execute_reply.started":"2021-12-25T12:12:59.535794Z","shell.execute_reply":"2021-12-25T12:12:59.601019Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#/usr/bin/python\n\nfrom __future__ import print_function\n\nimport argparse\nimport torch\nimport pickle \nimport numpy as np \nimport os \nimport math \nimport random \nimport sys\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport scipy.io\n\n\nfrom sklearn.decomposition import PCA\nfrom torch import nn, optim\nfrom torch.nn import functional as F\n\nparser = argparse.ArgumentParser(description='The Embedded Topic Model')\n\n### data and file related arguments\nparser.add_argument('--dataset', type=str, default='un', help='name of corpus')\nparser.add_argument('--data_path', type=str, default='./', help='directory containing data')\nparser.add_argument('--emb_path', type=str, default='skipgram/embeddings.txt', help='directory containing embeddings')\nparser.add_argument('--save_path', type=str, default='./results', help='path to save results')\nparser.add_argument('--batch_size', type=int, default=1000, help='number of documents in a batch for training')\nparser.add_argument('--min_df', type=int, default=100, help='to get the right data..minimum document frequency')\n\n### model-related arguments\nparser.add_argument('--num_topics', type=int, default=30, help='number of topics')\nparser.add_argument('--rho_size', type=int, default=300, help='dimension of rho')\nparser.add_argument('--emb_size', type=int, default=300, help='dimension of embeddings')\nparser.add_argument('--t_hidden_size', type=int, default=800, help='dimension of hidden space of q(theta)')\nparser.add_argument('--theta_act', type=str, default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)')\nparser.add_argument('--train_embeddings', type=int, default=1, help='whether to fix rho or train it')\nparser.add_argument('--eta_nlayers', type=int, default=3, help='number of layers for eta')\nparser.add_argument('--eta_hidden_size', type=int, default=200, help='number of hidden units for rnn')\nparser.add_argument('--delta', type=float, default=0.005, help='prior variance')\n\n### optimization-related arguments\nparser.add_argument('--lr', type=float, default=0.005, help='learning rate')\nparser.add_argument('--lr_factor', type=float, default=4.0, help='divide learning rate by this')\nparser.add_argument('--epochs', type=int, default=100, help='number of epochs to train')\nparser.add_argument('--mode', type=str, default='train', help='train or eval model')\nparser.add_argument('--optimizer', type=str, default='adam', help='choice of optimizer')\nparser.add_argument('--seed', type=int, default=2019, help='random seed (default: 1)')\nparser.add_argument('--enc_drop', type=float, default=0.0, help='dropout rate on encoder')\nparser.add_argument('--eta_dropout', type=float, default=0.0, help='dropout rate on rnn for eta')\nparser.add_argument('--clip', type=float, default=0.0, help='gradient clipping')\nparser.add_argument('--nonmono', type=int, default=10, help='number of bad hits allowed')\nparser.add_argument('--wdecay', type=float, default=1.2e-6, help='some l2 regularization')\nparser.add_argument('--anneal_lr', type=int, default=0, help='whether to anneal the learning rate or not')\nparser.add_argument('--bow_norm', type=int, default=0, help='normalize the bows or not')\n\n### evaluation, visualization, and logging-related arguments\nparser.add_argument('--num_words', type=int, default=20, help='number of words for topic viz')\nparser.add_argument('--log_interval', type=int, default=10, help='when to log training')\nparser.add_argument('--visualize_every', type=int, default=1, help='when to visualize results')\nparser.add_argument('--eval_batch_size', type=int, default=1000, help='input batch size for evaluation')\nparser.add_argument('--load_from', type=str, default='', help='the name of the ckpt to eval from')\nparser.add_argument('--tc', type=int, default=0, help='whether to compute tc or not')\n\nargs, _ = parser.parse_known_args()\n\npca = PCA(n_components=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n## set seed\nnp.random.seed(args.seed)\ntorch.backends.cudnn.deterministic = True\ntorch.manual_seed(args.seed)\n\n## get data\n# 1. vocabulary\nprint('Getting vocabulary ...')\ndata_file = os.path.join(args.data_path, 'min_df_{}'.format(args.min_df))\nvocab, train, valid, test = get_data(data_file, temporal=True)\nvocab_size = len(vocab)\nargs.vocab_size = vocab_size\n\n# 1. training data\nprint('Getting training data ...')\ntrain_tokens = train['tokens']\ntrain_counts = train['counts']\ntrain_times = train['times']\nargs.num_times = len(np.unique(train_times))\nargs.num_docs_train = len(train_tokens)\ntrain_rnn_inp = get_rnn_input(\n    train_tokens, train_counts, train_times, args.num_times, args.vocab_size, args.num_docs_train)\n\n# 2. dev set\nprint('Getting validation data ...')\nvalid_tokens = valid['tokens']\nvalid_counts = valid['counts']\nvalid_times = valid['times']\nargs.num_docs_valid = len(valid_tokens)\nvalid_rnn_inp = get_rnn_input(\n    valid_tokens, valid_counts, valid_times, args.num_times, args.vocab_size, args.num_docs_valid)\n\n# 3. test data\nprint('Getting testing data ...')\ntest_tokens = test['tokens']\ntest_counts = test['counts']\ntest_times = test['times']\nargs.num_docs_test = len(test_tokens)\ntest_rnn_inp = get_rnn_input(\n    test_tokens, test_counts, test_times, args.num_times, args.vocab_size, args.num_docs_test)\n\ntest_1_tokens = test['tokens_1']\ntest_1_counts = test['counts_1']\ntest_1_times = test_times\nargs.num_docs_test_1 = len(test_1_tokens)\ntest_1_rnn_inp = get_rnn_input(\n    test_1_tokens, test_1_counts, test_1_times, args.num_times, args.vocab_size, args.num_docs_test)\n\ntest_2_tokens = test['tokens_2']\ntest_2_counts = test['counts_2']\ntest_2_times = test_times\nargs.num_docs_test_2 = len(test_2_tokens)\ntest_2_rnn_inp = get_rnn_input(\n    test_2_tokens, test_2_counts, test_2_times, args.num_times, args.vocab_size, args.num_docs_test)\n\n## get embeddings \n# print('Getting embeddings ...')\n# emb_path = args.emb_path\n# vect_path = os.path.join(args.data_path.split('/')[0], 'embeddings.pkl')   \n# vectors = {}\n# with open(emb_path, 'rb') as f:\n#     for l in f:\n#         line = l.decode().split()\n#         word = line[0]\n#         if word in vocab:\n#             vect = np.array(line[1:]).astype(np.float)\n#             vectors[word] = vect\nembeddings = np.zeros((vocab_size, args.emb_size))\nwords_found = 0\nfor i, word in enumerate(vocab):\n    try: \n        embeddings[i] = vectors[word]\n        words_found += 1\n    except KeyError:\n        embeddings[i] = np.random.normal(scale=0.6, size=(args.emb_size, ))\nembeddings = torch.from_numpy(embeddings).to(device)\nargs.embeddings_dim = embeddings.size()\n\nprint('\\n')\nprint('=*'*100)\nprint('Training a Dynamic Embedded Topic Model on {} with the following settings: {}'.format(args.dataset.upper(), args))\nprint('=*'*100)\n\n## define checkpoint\nif not os.path.exists(args.save_path):\n    os.makedirs(args.save_path)\n\nif args.mode == 'eval':\n    ckpt = args.load_from\nelse:\n    ckpt = os.path.join(args.save_path, \n        'detm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_L_{}_minDF_{}_trainEmbeddings_{}'.format(\n        args.dataset, args.num_topics, args.t_hidden_size, args.optimizer, args.clip, args.theta_act, \n            args.lr, args.batch_size, args.rho_size, args.eta_nlayers, args.min_df, args.train_embeddings))\n\n## define model and optimizer\nif args.load_from != '':\n    print('Loading checkpoint from {}'.format(args.load_from))\n    with open(args.load_from, 'rb') as f:\n        model = torch.load(f)\nelse:\n    model = DETM(args, embeddings)\nprint('\\nDETM architecture: {}'.format(model))\nmodel.to(device)\n\nif args.optimizer == 'adam':\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\nelif args.optimizer == 'adagrad':\n    optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\nelif args.optimizer == 'adadelta':\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\nelif args.optimizer == 'rmsprop':\n    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\nelif args.optimizer == 'asgd':\n    optimizer = optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\nelse:\n    print('Defaulting to vanilla SGD')\n    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n\ndef train(epoch):\n    \"\"\"Train DETM on data for one epoch.\n    \"\"\"\n    model.train()\n    acc_loss = 0\n    acc_nll = 0\n    acc_kl_theta_loss = 0\n    acc_kl_eta_loss = 0\n    acc_kl_alpha_loss = 0\n    cnt = 0\n    indices = torch.randperm(args.num_docs_train)\n    indices = torch.split(indices, args.batch_size) \n    for idx, ind in enumerate(indices):\n        optimizer.zero_grad()\n        model.zero_grad()\n        data_batch, times_batch = get_batch(\n            train_tokens, train_counts, ind, args.vocab_size, temporal=True, times=train_times)\n        sums = data_batch.sum(1).unsqueeze(1)\n        if args.bow_norm:\n            normalized_data_batch = data_batch / sums\n        else:\n            normalized_data_batch = data_batch\n\n        loss, nll, kl_alpha, kl_eta, kl_theta = model(data_batch, normalized_data_batch, times_batch, train_rnn_inp, args.num_docs_train)\n        loss.backward()\n        if args.clip > 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n        optimizer.step()\n\n        acc_loss += torch.sum(loss).item()\n        acc_nll += torch.sum(nll).item()\n        acc_kl_theta_loss += torch.sum(kl_theta).item()\n        acc_kl_eta_loss += torch.sum(kl_eta).item()\n        acc_kl_alpha_loss += torch.sum(kl_alpha).item()\n        cnt += 1\n\n        if idx % args.log_interval == 0 and idx > 0:\n            cur_loss = round(acc_loss / cnt, 2) \n            cur_nll = round(acc_nll / cnt, 2) \n            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n            cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n            cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n            lr = optimizer.param_groups[0]['lr']\n            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n                epoch, idx, len(indices), lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n    \n    cur_loss = round(acc_loss / cnt, 2) \n    cur_nll = round(acc_nll / cnt, 2) \n    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n    cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n    cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n    lr = optimizer.param_groups[0]['lr']\n    print('*'*100)\n    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n            epoch, lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n    print('*'*100)\n\ndef visualize():\n    \"\"\"Visualizes topics and embeddings and word usage evolution.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        alpha = model.mu_q_alpha\n        beta = model.get_beta(alpha)\n        print('beta: ', beta.size())\n        print('\\n')\n        print('#'*100)\n        print('Visualize topics...')\n        times = [0, 10, 20]\n        topics_words = []\n        for k in range(args.num_topics):\n            for t in times:\n                #print(beta.shape)\n                gamma = beta[k, t, :]\n                top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n                topic_words = [vocab[a] for a in top_words]\n                topics_words.append(' '.join(topic_words))\n                print('Topic {} .. Time: {} ===> {}'.format(k, t, topic_words)) \n\n        print('\\n')\n        print('Visualize word embeddings ...')\n        queries = ['economic', 'assembly', 'security', 'management', 'debt', 'rights',  'africa']\n        try:\n            embeddings = model.rho.weight  # Vocab_size x E\n        except:\n            embeddings = model.rho         # Vocab_size x E\n        neighbors = []\n        for word in queries:\n            print('word: {} .. neighbors: {}'.format(\n                word, nearest_neighbors(word, embeddings, vocab, args.num_words)))\n        print('#'*100)\n\n        # print('\\n')\n        # print('Visualize word evolution ...')\n        # topic_0 = None ### k \n        # queries_0 = ['woman', 'gender', 'man', 'mankind', 'humankind'] ### v \n\n        # topic_1 = None\n        # queries_1 = ['africa', 'colonial', 'racist', 'democratic']\n\n        # topic_2 = None\n        # queries_2 = ['poverty', 'sustainable', 'trade']\n\n        # topic_3 = None\n        # queries_3 = ['soviet', 'convention', 'iran']\n\n        # topic_4 = None # climate\n        # queries_4 = ['environment', 'impact', 'threats', 'small', 'global', 'climate']\n\ndef _eta_helper(rnn_inp):\n    inp = model.q_eta_map(rnn_inp).unsqueeze(1)\n    hidden = model.init_hidden()\n    output, _ = model.q_eta(inp, hidden)\n    output = output.squeeze()\n    etas = torch.zeros(model.num_times, model.num_topics).to(device)\n    inp_0 = torch.cat([output[0], torch.zeros(model.num_topics,).to(device)], dim=0)\n    etas[0] = model.mu_q_eta(inp_0)\n    for t in range(1, model.num_times):\n        inp_t = torch.cat([output[t], etas[t-1]], dim=0)\n        etas[t] = model.mu_q_eta(inp_t)\n    return etas\n\ndef get_eta(source):\n    model.eval()\n    with torch.no_grad():\n        if source == 'val':\n            rnn_inp = valid_rnn_inp\n            return _eta_helper(rnn_inp)\n        else:\n            rnn_1_inp = test_1_rnn_inp\n            return _eta_helper(rnn_1_inp)\n\ndef get_theta(eta, bows):\n    model.eval()\n    with torch.no_grad():\n        inp = torch.cat([bows, eta], dim=1)\n        q_theta = model.q_theta(inp)\n        mu_theta = model.mu_q_theta(q_theta)\n        theta = F.softmax(mu_theta, dim=-1)\n        return theta    \n\ndef get_completion_ppl(source):\n    \"\"\"Returns document completion perplexity.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        alpha = model.mu_q_alpha\n        if source == 'val':\n            indices = torch.split(torch.tensor(range(args.num_docs_valid)), args.eval_batch_size)\n            tokens = valid_tokens\n            counts = valid_counts\n            times = valid_times\n            eta = get_eta('val')\n\n            acc_loss = 0\n            cnt = 0\n            for idx, ind in enumerate(indices):\n                data_batch, times_batch = get_batch(\n                    tokens, counts, ind, args.vocab_size, temporal=True, times=times)\n                sums = data_batch.sum(1).unsqueeze(1)\n                if args.bow_norm:\n                    normalized_data_batch = data_batch / sums\n                else:\n                    normalized_data_batch = data_batch\n\n                # 1. select time D[T]xTxK\n                eta_td = eta[times_batch.type('torch.LongTensor')]\n                # 2. get theta (eta_t) D[T]\n                theta = get_theta(eta_td, normalized_data_batch)\n                # 3. get alpha_t (KxD[T]xL)\n                alpha_td = alpha[:, times_batch.type('torch.LongTensor'), :]\n                # 4. get beta\n                ## alpha(KxD[T]xV) -> D[T]xKxV\n                beta = model.get_beta(alpha_td).permute(1, 0, 2)\n                # 5. get log-likelihood\n                # DxKx1 * D[T]xKxV\n                loglik = theta.unsqueeze(2) * beta\n                # D[T]xKxV\n                loglik = loglik.sum(1)\n                loglik = torch.log(loglik)\n                # 6. calculate perplexity\n                ## log()*databatch, sum(1)\n                nll = -loglik * data_batch\n                nll = nll.sum(-1)\n                loss = nll / sums.squeeze()\n                loss = loss.mean().item()\n                acc_loss += loss\n                cnt += 1\n            cur_loss = acc_loss / cnt\n            ppl_all = round(math.exp(cur_loss), 1)\n            print('*'*100)\n            print('{} PPL: {}'.format(source.upper(), ppl_all))\n            print('*'*100)\n            return ppl_all\n        else: \n            indices = torch.split(torch.tensor(range(args.num_docs_test)), args.eval_batch_size)\n            tokens_1 = test_1_tokens\n            counts_1 = test_1_counts\n\n            tokens_2 = test_2_tokens\n            counts_2 = test_2_counts\n\n            eta_1 = get_eta('test')\n\n            acc_loss = 0\n            cnt = 0\n            indices = torch.split(torch.tensor(range(args.num_docs_test)), args.eval_batch_size)\n            for idx, ind in enumerate(indices):\n                data_batch_1, times_batch_1 = get_batch(\n                    tokens_1, counts_1, ind, args.vocab_size, temporal=True, times=test_times)\n                sums_1 = data_batch_1.sum(1).unsqueeze(1)\n                if args.bow_norm:\n                    normalized_data_batch_1 = data_batch_1 / sums_1\n                else:\n                    normalized_data_batch_1 = data_batch_1\n                # DTxTxK\n                eta_td_1 = eta_1[times_batch_1.type('torch.LongTensor')]\n\n                theta = get_theta(eta_td_1, normalized_data_batch_1)\n\n                data_batch_2, times_batch_2 = get_batch(\n                    tokens_2, counts_2, ind, args.vocab_size, temporal=True, times=test_times)\n                sums_2 = data_batch_2.sum(1).unsqueeze(1)\n\n                alpha_td = alpha[:, times_batch_2.type('torch.LongTensor'), :]\n                beta = model.get_beta(alpha_td).permute(1, 0, 2)\n                loglik = theta.unsqueeze(2) * beta\n                loglik = loglik.sum(1)\n                loglik = torch.log(loglik)\n                nll = -loglik * data_batch_2\n                nll = nll.sum(-1)\n                loss = nll / sums_2.squeeze()\n                loss = loss.mean().item()\n                acc_loss += loss\n                cnt += 1\n            cur_loss = acc_loss / cnt\n            ppl_dc = round(math.exp(cur_loss), 1)\n            print('*'*100)\n            print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n            print('*'*100)\n            return ppl_dc\n\ndef _diversity_helper(beta, num_tops):\n    list_w = np.zeros((args.num_topics, num_tops))\n    for k in range(args.num_topics):\n        gamma = beta[k, :]\n        top_words = gamma.cpu().numpy().argsort()[-num_tops:][::-1]\n        list_w[k, :] = top_words\n    list_w = np.reshape(list_w, (-1))\n    list_w = list(list_w)\n    n_unique = len(np.unique(list_w))\n    diversity = n_unique / (args.num_topics * num_tops)\n    return diversity\n\ndef get_topic_quality():\n    \"\"\"Returns topic coherence and topic diversity.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        alpha = model.mu_q_alpha\n        beta = model.get_beta(alpha) \n        print('beta: ', beta.size())\n\n        print('\\n')\n        print('#'*100)\n        print('Get topic diversity...')\n        num_tops = 25\n        TD_all = np.zeros((args.num_times,))\n        for tt in range(args.num_times):\n            TD_all[tt] = _diversity_helper(beta[:, tt, :], num_tops)\n        TD = np.mean(TD_all)\n        print('Topic Diversity is: {}'.format(TD))\n\n        print('\\n')\n        print('Get topic coherence...')\n        print('train_tokens: ', train_tokens[0])\n        TC_all = []\n        cnt_all = []\n        for tt in range(args.num_times):\n            tc, cnt = get_topic_coherence(beta[:, tt, :].cpu().numpy(), train_tokens, vocab)\n            TC_all.append(tc)\n            cnt_all.append(cnt)\n        print('TC_all: ', TC_all)\n        TC_all = torch.tensor(TC_all)\n        print('TC_all: ', TC_all.size())\n        print('\\n')\n        print('Get topic quality...')\n        quality = tc * diversity\n        print('Topic Quality is: {}'.format(quality))\n        print('#'*100)\n\nif args.mode == 'train':\n    ## train model on data by looping through multiple epochs\n    best_epoch = 0\n    best_val_ppl = 1e9\n    all_val_ppls = []\n    for epoch in range(1, args.epochs):\n        train(epoch)\n#         if epoch % args.visualize_every == 0:\n#             visualize()\n        val_ppl = get_completion_ppl('val')\n        print('val_ppl: ', val_ppl)\n        if val_ppl < best_val_ppl:\n            with open(ckpt, 'wb') as f:\n                torch.save(model, f)\n            best_epoch = epoch\n            best_val_ppl = val_ppl\n        else:\n            ## check whether to anneal lr\n            lr = optimizer.param_groups[0]['lr']\n            if args.anneal_lr and (len(all_val_ppls) > args.nonmono and val_ppl > min(all_val_ppls[:-args.nonmono]) and lr > 1e-5):\n                optimizer.param_groups[0]['lr'] /= args.lr_factor\n        all_val_ppls.append(val_ppl)\n    with open(ckpt, 'rb') as f:\n        model = torch.load(f)\n    model = model.to(device)\n    model.eval()\n    with torch.no_grad():\n        print('saving topic matrix beta...')\n        alpha = model.mu_q_alpha\n        beta = model.get_beta(alpha).cpu().numpy()\n        scipy.io.savemat(ckpt+'_beta.mat', {'values': beta}, do_compression=True)\n        if args.train_embeddings:\n            print('saving word embedding matrix rho...')\n            rho = model.rho.weight.cpu().numpy()\n            scipy.io.savemat(ckpt+'_rho.mat', {'values': rho}, do_compression=True)\n        print('computing validation perplexity...')\n        val_ppl = get_completion_ppl('val')\n        print('computing test perplexity...')\n        test_ppl = get_completion_ppl('test')\nelse: \n    with open(ckpt, 'rb') as f:\n        model = torch.load(f)\n    model = model.to(device)\n        \n    print('saving alpha...')\n    with torch.no_grad():\n        alpha = model.mu_q_alpha.cpu().numpy()\n        scipy.io.savemat(ckpt+'_alpha.mat', {'values': alpha}, do_compression=True)\n\n    print('computing validation perplexity...')\n    val_ppl = get_completion_ppl('val')\n    print('computing test perplexity...')\n    test_ppl = get_completion_ppl('test')\n    print('computing topic coherence and topic diversity...')\n    get_topic_quality()\n    print('visualizing topics and embeddings...')\n    visualize()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:12:59.603057Z","iopub.execute_input":"2021-12-25T12:12:59.603496Z","iopub.status.idle":"2021-12-25T12:13:00.606938Z","shell.execute_reply.started":"2021-12-25T12:12:59.603468Z","shell.execute_reply":"2021-12-25T12:13:00.605725Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Getting vocabulary ...\nGetting training data ...\nidx: 0/9\nGetting validation data ...\nidx: 0/1\nGetting testing data ...\nidx: 0/1\nidx: 0/1\nidx: 0/1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_60/2170529085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mwords_found\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"],"ename":"NameError","evalue":"name 'vectors' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport torch \nimport numpy as np\nimport bokeh.plotting as bp\n\ndef get_df(data, wi, wj=None):\n    \"\"\"\n    Obtain the document frequency\n    :param data: document vocabulary matrix\n    :param wi: word index w_i\n    :param wj: word index w_j\n    :return: document frequency for word w_i , w_i ∩ w_j\n    \"\"\"\n    if wj is None:\n        return torch.where(data[:, wi] > 0, 1, 0).sum(-1)\n    else:\n        df_wi = torch.where(data[:, wi] > 0, 1, 0)\n        df_wj = torch.where(data[:, wj] > 0, 1, 0)\n        return df_wj.sum(-1), (df_wi & df_wj).sum(-1)\n\n\ndef get_topic_coherence(beta, data):\n    D = torch.tensor(len(data)) ## number of docs...data is list of documents\n    TC = []\n    num_topics = len(beta)\n    counter = 0\n    for k in range(num_topics):\n        top_10 = list(torch.flip(beta[k].argsort()[-11:],[0]))\n        TC_k = 0\n        counter = 0\n        for i, word in enumerate(top_10):\n            D_wi = get_df(data, word)\n            j = i + 1\n            tmp = 0\n            while j < len(top_10) and j > i:\n                D_wj, D_wi_wj = get_df(data, word, top_10[j])\n                if D_wi_wj == 0:\n                    f_wi_wj = -1\n                else:\n                    f_wi_wj = -1 + (torch.log(D_wi)+torch.log(D_wj)-2.0*torch.log(D))/(torch.log(D_wi_wj)-torch.log(D))\n                tmp += f_wi_wj\n                j += 1\n                counter += 1\n            TC_k += tmp \n        TC.append(TC_k.detach().cpu().numpy())\n    TC = np.mean(TC) / counter\n    #print('Topic coherence is: {}'.format(TC))\n    return TC\n\n\ndef visualize(docs, _lda_keys, topics, theta):\n    tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n    # project to 2D\n    tsne_lda = tsne_model.fit_transform(theta)\n    colormap = []\n    for name, hex in matplotlib.colors.cnames.items():\n        colormap.append(hex)\n\n    colormap = colormap[:len(theta[0, :])]\n    colormap = np.array(colormap)\n\n    title = '20 newsgroups TE embedding V viz'\n    num_example = len(docs)\n\n    plot_lda = bp.figure(plot_width=1400, plot_height=1100,\n                     title=title,\n                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n                     x_axis_type=None, y_axis_type=None, min_border=1)\n\n    plt.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1],\n                 color=colormap[_lda_keys][:num_example])\n    plt.show()\n\n    \ndef get_rnn_input(dataloader, num_times, vocab_size):\n    # TxV\n    rnn_input = torch.zeros(num_times, vocab_size).to(device)\n    # times count\n    cnt = torch.zeros(num_times, ).to(device)\n    # create data loader\n    for data in dataloader:\n        data_batch = cvz[data['index'].long() - 1,:].to(device)\n        times_batch = ts[data['index'].long() - 1].to(device)\n        for t in range(num_times):\n            # check times\n            tmp = (times_batch == t).nonzero()\n            # sum the vocabulary in time t\n            docs = data_batch[tmp].squeeze().sum(0)\n            # feed in the vocabulary count in time t\n            rnn_input[t] += docs\n            # sum up the count\n            cnt[t] += tmp.shape[0]\n            # check the epoch\n    rnn_input = rnn_input / cnt.unsqueeze(1)\n    return rnn_input","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:13:00.607948Z","iopub.status.idle":"2021-12-25T12:13:00.609672Z","shell.execute_reply.started":"2021-12-25T12:13:00.609217Z","shell.execute_reply":"2021-12-25T12:13:00.609287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    alpha = model.mu_q_alpha\n    beta = model.get_beta(alpha)\n    \nbeta = beta.permute(1,0,2)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:13:00.611861Z","iopub.status.idle":"2021-12-25T12:13:00.612289Z","shell.execute_reply.started":"2021-12-25T12:13:00.612105Z","shell.execute_reply":"2021-12-25T12:13:00.612128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = torch.from_numpy(np.array(\n    get_batch(train_tokens, train_counts, range(train_tokens.shape[0]), args.vocab_size, temporal=True, times=train_times)[0]\n))","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:13:00.613194Z","iopub.status.idle":"2021-12-25T12:13:00.613502Z","shell.execute_reply.started":"2021-12-25T12:13:00.613335Z","shell.execute_reply":"2021-12-25T12:13:00.613356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:13:00.614925Z","iopub.status.idle":"2021-12-25T12:13:00.615229Z","shell.execute_reply.started":"2021-12-25T12:13:00.615073Z","shell.execute_reply":"2021-12-25T12:13:00.615090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = cvectorizer.vocabulary_\ncvz = cvz.todense()","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:13:00.616267Z","iopub.status.idle":"2021-12-25T12:13:00.616560Z","shell.execute_reply.started":"2021-12-25T12:13:00.616408Z","shell.execute_reply":"2021-12-25T12:13:00.616424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KxTxL\n# alpha = model.mu_q_alpha.clone().contiguous()\n# alpha = alpha.permute(1,0,2)\n# #alpha = model.get_alpha()[0]\n# beta = model.get_beta(alpha,torch.arange(0,ts.unique().shape[0]))\n\ncnt = 0\ntc = 0\nfor time in range(0,beta.shape[0]):\n    beta_t = beta[time,:,:]\n    cnt+=1\n    tc+=get_topic_coherence(beta_t, ds)\n\ntc/=cnt\nprint(f'tc: {tc}')\n\n#print_top_words(beta[:,:,:-3],vocab)\ndef _diversity_helper(beta, num_tops):\n    list_w = torch.zeros((int(beta.shape[0]), num_tops))\n    for k in range(int(beta.shape[0])):\n        gamma = beta[k, :]\n        top_words = gamma.argsort()[-num_tops:]\n        list_w[k, :] = top_words\n    list_w = list_w.reshape(-1)\n    n_unique = len(list_w.unique())\n    diversity = n_unique / (beta.shape[0] * num_tops)\n    return diversity\n\ntd = 0\nfor t in range(beta.shape[0]):\n    d=_diversity_helper(beta[t],25)\n    td+=d\n    print(d)\nprint(f'TD: {td/beta.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2021-12-25T12:13:00.617384Z","iopub.status.idle":"2021-12-25T12:13:00.617662Z","shell.execute_reply.started":"2021-12-25T12:13:00.617513Z","shell.execute_reply":"2021-12-25T12:13:00.617528Z"},"trusted":true},"execution_count":null,"outputs":[]}]}