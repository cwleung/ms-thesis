{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-25T09:49:20.052924Z","iopub.execute_input":"2021-12-25T09:49:20.053261Z","iopub.status.idle":"2021-12-25T09:49:20.080839Z","shell.execute_reply.started":"2021-12-25T09:49:20.053226Z","shell.execute_reply":"2021-12-25T09:49:20.079814Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"/kaggle/input/stopwords/stops.txt\n/kaggle/input/nips-papers-1987-2019-updated/papers.csv\n/kaggle/input/nips-papers-1987-2019-updated/authors.csv\n/kaggle/input/un-general-debates/un-general-debates.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import csv\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport random\nfrom scipy import sparse\nimport itertools\nimport re\nfrom scipy.io import savemat, loadmat\nimport torch\nimport string\nimport os\n\n# Maximum / minimum document frequency\nmax_df = 0.5\nmin_df = 100  # choose desired value for min_df\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Read meta-data\nprint('reading meta-data...')\ndata = pd.read_csv('../input/un-general-debates/un-general-debates.csv')\ndata = data[~data.text.isnull()]\ndocs = data.text.values\n# data = pd.read_csv('../input/nips-papers-1987-2019-updated/papers.csv')\n# data = data[~data.full_text.isnull()]\n# docs = data.full_text.values  ## bows\ntimes = data.year.unique()\ntimes.sort()\n\nts = torch.from_numpy(data.year.to_numpy()).to(device)  ## timestamp\nts = (ts==ts.unique()[:,None]).nonzero().transpose(1,0)[0].to(device)\n\ndef remove_not_printable(in_str):\n    return \"\".join([c for c in in_str if c in string.printable])\n\ndef contains_punctuation(w):\n    return any(char in string.punctuation for char in w)\n\ndef contains_numeric(w):\n    return any(char.isdigit() for char in w)\n\n# document preprocessing\ninit_docs = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]|[\\n]+''', doc) for doc in docs]\ninit_docs = [[w.lower() for w in init_docs[doc] if not contains_punctuation(w)] for doc in range(len(init_docs))]\ninit_docs = [[w for w in init_docs[doc] if not contains_numeric(w)] for doc in range(len(init_docs))]\ninit_docs = [[w for w in init_docs[doc] if len(w) > 1] for doc in range(len(init_docs))]\ninit_docs = [\" \".join(init_docs[doc]) for doc in range(len(init_docs))]","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:49:20.083485Z","iopub.execute_input":"2021-12-25T09:49:20.084609Z","iopub.status.idle":"2021-12-25T09:50:49.243255Z","shell.execute_reply.started":"2021-12-25T09:49:20.084558Z","shell.execute_reply":"2021-12-25T09:50:49.242251Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"reading meta-data...\n","output_type":"stream"}]},{"cell_type":"code","source":"timestamps = data.year.values","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:50:49.245126Z","iopub.execute_input":"2021-12-25T09:50:49.245368Z","iopub.status.idle":"2021-12-25T09:50:49.250021Z","shell.execute_reply.started":"2021-12-25T09:50:49.245328Z","shell.execute_reply":"2021-12-25T09:50:49.249064Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"\n# Read raw data\nprint('reading raw data...')\n# docs = []\n# not_found = []\n# timestamps = []\n# for (pid, tt) in zip(all_pids, all_timestamps):\n#     path_read = 'raw/acl_abstracts/acl_data-combined/all_papers'\n#     path_read = os.path.join(path_read, pid + '.txt')\n#     if not os.path.isfile(path_read):\n#         not_found.append(pid)\n#     else:\n#         with open(path_read, 'rb') as f:\n#             doc = f.read().decode('utf-8', 'ignore')\n#             doc = doc.lower().replace('\\n', ' ').replace(\"’\", \" \").replace(\"'\", \" \").translate(str.maketrans(string.punctuation + \"0123456789\", ' '*len(string.punctuation + \"0123456789\"))).split()\n#         doc = [remove_not_printable(w) for w in doc if len(w)>1]\n#         if len(doc) > 1:\n#             doc = \" \".join(doc)\n#             docs.append(doc)\n#             timestamps.append(tt)\n\n# Write as raw text\n# print('writing to text file...')\n# out_filename = './docs_processed.txt'\n# print('writing to text file...')\n# with open(out_filename, 'w') as f:\n#     for line in docs:\n#         f.write(line + '\\n')\n\n# Read stopwords\nwith open('../input/stopwords/stops.txt', 'r') as f:\n    stops = f.read().split('\\n')\n\n# Create count vectorizer\nprint('counting document frequency of words...')\ncvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=frozenset(stops))\ncvz = cvectorizer.fit_transform(init_docs).sign()\n\n# Get vocabulary\nprint('building the vocabulary...')\nsum_counts = cvz.sum(axis=0)\nv_size = sum_counts.shape[1]\nsum_counts_np = np.zeros(v_size, dtype=int)\nfor v in range(v_size):\n    sum_counts_np[v] = sum_counts[0,v]\nword2id = dict([(w, cvectorizer.vocabulary_.get(w)) for w in cvectorizer.vocabulary_])\nid2word = dict([(cvectorizer.vocabulary_.get(w), w) for w in cvectorizer.vocabulary_])\n#del cvectorizer\nprint('  initial vocabulary size: {}'.format(v_size))\n\n# Sort elements in vocabulary\nidx_sort = np.argsort(sum_counts_np)\nvocab_aux = [id2word[idx_sort[cc]] for cc in range(v_size)]\n\n# Filter out stopwords (if any)\nvocab_aux = [w for w in vocab_aux if w not in stops]\nprint('  vocabulary size after removing stopwords from list: {}'.format(len(vocab_aux)))\n\n# Create dictionary and inverse dictionary\nvocab = vocab_aux\ndel vocab_aux\nword2id = dict([(w, j) for j, w in enumerate(vocab)])\nid2word = dict([(j, w) for j, w in enumerate(vocab)])\n\n# Create mapping of timestamps\nall_times = sorted(set(timestamps))\ntime2id = dict([(t, i) for i, t in enumerate(all_times)])\nid2time = dict([(i, t) for i, t in enumerate(all_times)])\ntime_list = [id2time[i] for i in range(len(all_times))]\n\n# Split in train/test/valid\nprint('tokenizing documents and splitting into train/test/valid...')\nnum_docs = cvz.shape[0]\ntrSize = int(np.floor(0.85*num_docs))\ntsSize = int(np.floor(0.10*num_docs))\nvaSize = int(num_docs - trSize - tsSize)\n#del cvz\nidx_permute = np.random.permutation(num_docs).astype(int)\n\n# Remove words not in train_data\nvocab = list(set([w for idx_d in range(trSize) for w in docs[idx_permute[idx_d]].split() if w in word2id]))\nword2id = dict([(w, j) for j, w in enumerate(vocab)])\nid2word = dict([(j, w) for j, w in enumerate(vocab)])\nprint('  vocabulary after removing words not in train: {}'.format(len(vocab)))\n\ndocs_tr = [[word2id[w] for w in docs[idx_permute[idx_d]].split() if w in word2id] for idx_d in range(trSize)]\ntimestamps_tr = [time2id[timestamps[idx_permute[idx_d]]] for idx_d in range(trSize)]\ndocs_ts = [[word2id[w] for w in docs[idx_permute[idx_d+trSize]].split() if w in word2id] for idx_d in range(tsSize)]\ntimestamps_ts = [time2id[timestamps[idx_permute[idx_d+trSize]]] for idx_d in range(tsSize)]\ndocs_va = [[word2id[w] for w in docs[idx_permute[idx_d+trSize+tsSize]].split() if w in word2id] for idx_d in range(vaSize)]\ntimestamps_va = [time2id[timestamps[idx_permute[idx_d+trSize+tsSize]]] for idx_d in range(vaSize)]\n\nprint('  number of documents (train): {} [this should be equal to {} and {}]'.format(len(docs_tr), trSize, len(timestamps_tr)))\nprint('  number of documents (test): {} [this should be equal to {} and {}]'.format(len(docs_ts), tsSize, len(timestamps_ts)))\nprint('  number of documents (valid): {} [this should be equal to {} and {}]'.format(len(docs_va), vaSize, len(timestamps_va)))\n\n# Remove empty documents\nprint('removing empty documents...')\n\ndef remove_empty(in_docs, in_timestamps):\n    out_docs = []\n    out_timestamps = []\n    for ii, doc in enumerate(in_docs):\n        if(doc!=[]):\n            out_docs.append(doc)\n            out_timestamps.append(in_timestamps[ii])\n    return out_docs, out_timestamps\n\ndef remove_by_threshold(in_docs, in_timestamps, thr):\n    out_docs = []\n    out_timestamps = []\n    for ii, doc in enumerate(in_docs):\n        if(len(doc)>thr):\n            out_docs.append(doc)\n            out_timestamps.append(in_timestamps[ii])\n    return out_docs, out_timestamps\n\ndocs_tr, timestamps_tr = remove_empty(docs_tr, timestamps_tr)\ndocs_ts, timestamps_ts = remove_empty(docs_ts, timestamps_ts)\ndocs_va, timestamps_va = remove_empty(docs_va, timestamps_va)\n\n# Remove test documents with length=1\ndocs_ts, timestamps_ts = remove_by_threshold(docs_ts, timestamps_ts, 1)\n\n# Split test set in 2 halves\nprint('splitting test documents in 2 halves...')\ndocs_ts_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in docs_ts]\ndocs_ts_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in docs_ts]\n\n# Getting lists of words and doc_indices\nprint('creating lists of words...')\n\ndef create_list_words(in_docs):\n    return [x for y in in_docs for x in y]\n\nwords_tr = create_list_words(docs_tr)\nwords_ts = create_list_words(docs_ts)\nwords_ts_h1 = create_list_words(docs_ts_h1)\nwords_ts_h2 = create_list_words(docs_ts_h2)\nwords_va = create_list_words(docs_va)\n\nprint('  len(words_tr): ', len(words_tr))\nprint('  len(words_ts): ', len(words_ts))\nprint('  len(words_ts_h1): ', len(words_ts_h1))\nprint('  len(words_ts_h2): ', len(words_ts_h2))\nprint('  len(words_va): ', len(words_va))\n\n# Get doc indices\nprint('getting doc indices...')\n\ndef create_doc_indices(in_docs):\n    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n    return [int(x) for y in aux for x in y]\n\ndoc_indices_tr = create_doc_indices(docs_tr)\ndoc_indices_ts = create_doc_indices(docs_ts)\ndoc_indices_ts_h1 = create_doc_indices(docs_ts_h1)\ndoc_indices_ts_h2 = create_doc_indices(docs_ts_h2)\ndoc_indices_va = create_doc_indices(docs_va)\n\nprint('  len(np.unique(doc_indices_tr)): {} [this should be {}]'.format(len(np.unique(doc_indices_tr)), len(docs_tr)))\nprint('  len(np.unique(doc_indices_ts)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts)), len(docs_ts)))\nprint('  len(np.unique(doc_indices_ts_h1)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h1)), len(docs_ts_h1)))\nprint('  len(np.unique(doc_indices_ts_h2)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h2)), len(docs_ts_h2)))\nprint('  len(np.unique(doc_indices_va)): {} [this should be {}]'.format(len(np.unique(doc_indices_va)), len(docs_va)))\n\n# Number of documents in each set\nn_docs_tr = len(docs_tr)\nn_docs_ts = len(docs_ts)\nn_docs_ts_h1 = len(docs_ts_h1)\nn_docs_ts_h2 = len(docs_ts_h2)\nn_docs_va = len(docs_va)\n\n# Remove unused variables\ndel docs_tr\ndel docs_ts\ndel docs_ts_h1\ndel docs_ts_h2\ndel docs_va\n\n# Create bow representation\nprint('creating bow representation...')\n\ndef create_bow(doc_indices, words, n_docs, vocab_size):\n    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n\nbow_tr = create_bow(doc_indices_tr, words_tr, n_docs_tr, len(vocab))\nbow_ts = create_bow(doc_indices_ts, words_ts, n_docs_ts, len(vocab))\nbow_ts_h1 = create_bow(doc_indices_ts_h1, words_ts_h1, n_docs_ts_h1, len(vocab))\nbow_ts_h2 = create_bow(doc_indices_ts_h2, words_ts_h2, n_docs_ts_h2, len(vocab))\nbow_va = create_bow(doc_indices_va, words_va, n_docs_va, len(vocab))\n\ndel words_tr\ndel words_ts\ndel words_ts_h1\ndel words_ts_h2\ndel words_va\ndel doc_indices_tr\ndel doc_indices_ts\ndel doc_indices_ts_h1\ndel doc_indices_ts_h2\ndel doc_indices_va\n\n# Write files for LDA C++ code\ndef write_lda_file(filename, timestamps_in, time_list_in, bow_in):\n    idxSort = np.argsort(timestamps_in)\n    \n    with open(filename, \"w\") as f:\n        for row in idxSort:\n            x = bow_in.getrow(row)\n            n_elems = x.count_nonzero()\n            f.write(str(n_elems))\n            if(n_elems != len(x.indices) or n_elems != len(x.data)):\n                raise ValueError(\"[ERR] THIS SHOULD NOT HAPPEN\")\n            for ii, dd in zip(x.indices, x.data):\n                f.write(' ' + str(ii) + ':' + str(dd))\n            f.write('\\n')\n            \n    with open(filename.replace(\"-mult\", \"-seq\"), \"w\") as f:\n        f.write(str(len(time_list_in)) + '\\n')\n        for idx_t, _ in enumerate(time_list_in):\n            n_elem = len([t for t in timestamps_in if t==idx_t])\n            f.write(str(n_elem) + '\\n')\n            \n\npath_save = './min_df_' + str(min_df) + '/'\nif not os.path.isdir(path_save):\n    os.system('mkdir -p ' + path_save)\n\n# Write files for LDA C++ code\nprint('saving LDA files for C++ code...')\nwrite_lda_file(path_save + 'dtm_tr-mult.dat', timestamps_tr, time_list, bow_tr)\nwrite_lda_file(path_save + 'dtm_ts-mult.dat', timestamps_ts, time_list, bow_ts)\nwrite_lda_file(path_save + 'dtm_ts_h1-mult.dat', timestamps_ts, time_list, bow_ts_h1)\nwrite_lda_file(path_save + 'dtm_ts_h2-mult.dat', timestamps_ts, time_list, bow_ts_h2)\nwrite_lda_file(path_save + 'dtm_va-mult.dat', timestamps_va, time_list, bow_va)\n\n# Also write the vocabulary and timestamps\nwith open(path_save + 'vocab.txt', \"w\") as f:\n    for v in vocab:\n        f.write(v + '\\n')\n\nwith open(path_save + 'timestamps.txt', \"w\") as f:\n    for t in time_list:\n        f.write(str(t) + '\\n')\n\nwith open(path_save + 'vocab.pkl', 'wb') as f:\n    pickle.dump(vocab, f)\ndel vocab\n\nwith open(path_save + 'timestamps.pkl', 'wb') as f:\n    pickle.dump(time_list, f)\n\n# Save timestamps alone\nsavemat(path_save + 'bow_tr_timestamps', {'timestamps': timestamps_tr}, do_compression=True)\nsavemat(path_save + 'bow_ts_timestamps', {'timestamps': timestamps_ts}, do_compression=True)\nsavemat(path_save + 'bow_va_timestamps', {'timestamps': timestamps_va}, do_compression=True)\n\n# Split bow intro token/value pairs\nprint('splitting bow intro token/value pairs and saving to disk...')\n\ndef split_bow(bow_in, n_docs):\n    indices = [[w for w in bow_in[doc,:].indices] for doc in range(n_docs)]\n    counts = [[c for c in bow_in[doc,:].data] for doc in range(n_docs)]\n    return indices, counts\n\nbow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_docs_tr)\nsavemat(path_save + 'bow_tr_tokens', {'tokens': bow_tr_tokens}, do_compression=True)\nsavemat(path_save + 'bow_tr_counts', {'counts': bow_tr_counts}, do_compression=True)\ndel bow_tr\ndel bow_tr_tokens\ndel bow_tr_counts\n\nbow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_docs_ts)\nsavemat(path_save + 'bow_ts_tokens', {'tokens': bow_ts_tokens}, do_compression=True)\nsavemat(path_save + 'bow_ts_counts', {'counts': bow_ts_counts}, do_compression=True)\ndel bow_ts\ndel bow_ts_tokens\ndel bow_ts_counts\n\nbow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_docs_ts_h1)\nsavemat(path_save + 'bow_ts_h1_tokens', {'tokens': bow_ts_h1_tokens}, do_compression=True)\nsavemat(path_save + 'bow_ts_h1_counts', {'counts': bow_ts_h1_counts}, do_compression=True)\ndel bow_ts_h1\ndel bow_ts_h1_tokens\ndel bow_ts_h1_counts\n\nbow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_docs_ts_h2)\nsavemat(path_save + 'bow_ts_h2_tokens', {'tokens': bow_ts_h2_tokens}, do_compression=True)\nsavemat(path_save + 'bow_ts_h2_counts', {'counts': bow_ts_h2_counts}, do_compression=True)\ndel bow_ts_h2\ndel bow_ts_h2_tokens\ndel bow_ts_h2_counts\n\nbow_va_tokens, bow_va_counts = split_bow(bow_va, n_docs_va)\nsavemat(path_save + 'bow_va_tokens', {'tokens': bow_va_tokens}, do_compression=True)\nsavemat(path_save + 'bow_va_counts', {'counts': bow_va_counts}, do_compression=True)\ndel bow_va\ndel bow_va_tokens\ndel bow_va_counts\n\nprint('Data ready !!')\nprint('*************')","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:50:49.252715Z","iopub.execute_input":"2021-12-25T09:50:49.252974Z","iopub.status.idle":"2021-12-25T09:51:46.493332Z","shell.execute_reply.started":"2021-12-25T09:50:49.252943Z","shell.execute_reply":"2021-12-25T09:51:46.492054Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"reading raw data...\ncounting document frequency of words...\nbuilding the vocabulary...\n  initial vocabulary size: 6703\n  vocabulary size after removing stopwords from list: 6703\ntokenizing documents and splitting into train/test/valid...\n  vocabulary after removing words not in train: 6185\n  number of documents (train): 6380 [this should be equal to 6380 and 6380]\n  number of documents (test): 750 [this should be equal to 750 and 750]\n  number of documents (valid): 377 [this should be equal to 377 and 377]\nremoving empty documents...\nsplitting test documents in 2 halves...\ncreating lists of words...\n  len(words_tr):  3471030\n  len(words_ts):  413760\n  len(words_ts_h1):  206685\n  len(words_ts_h2):  207075\n  len(words_va):  204735\ngetting doc indices...\n  len(np.unique(doc_indices_tr)): 6380 [this should be 6380]\n  len(np.unique(doc_indices_ts)): 750 [this should be 750]\n  len(np.unique(doc_indices_ts_h1)): 750 [this should be 750]\n  len(np.unique(doc_indices_ts_h2)): 750 [this should be 750]\n  len(np.unique(doc_indices_va)): 377 [this should be 377]\ncreating bow representation...\nsaving LDA files for C++ code...\nsplitting bow intro token/value pairs and saving to disk...\nData ready !!\n*************\n","output_type":"stream"}]},{"cell_type":"markdown","source":"kl divergence","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport torch \nimport numpy as np\nimport bokeh.plotting as bp\n\nfrom bokeh.plotting import save\nfrom bokeh.models import HoverTool\nimport matplotlib.pyplot as plt \nimport matplotlib \n\ntiny = 1e-6\n\ndef _reparameterize(mu, logvar, num_samples):\n    \"\"\"Applies the reparameterization trick to return samples from a given q\"\"\"\n    std = torch.exp(0.5 * logvar) \n    bsz, zdim = logvar.size()\n    eps = torch.randn(num_samples, bsz, zdim).to(mu.device)\n    mu = mu.unsqueeze(0)\n    std = std.unsqueeze(0)\n    res = eps.mul_(std).add_(mu)\n    return res\n\ndef get_document_frequency(data, wi, wj=None):\n    if wj is None:\n        D_wi = 0\n        for l in range(len(data)):\n            doc = data[l].squeeze(0)\n            if len(doc) == 1: \n                continue\n                #doc = [doc.squeeze()]\n            else:\n                doc = doc.squeeze()\n            if wi in doc:\n                D_wi += 1\n        return D_wi\n    D_wj = 0\n    D_wi_wj = 0\n    for l in range(len(data)):\n        doc = data[l].squeeze(0)\n        if len(doc) == 1: \n            doc = [doc.squeeze()]\n        else:\n            doc = doc.squeeze()\n        if wj in doc:\n            D_wj += 1\n            if wi in doc:\n                D_wi_wj += 1\n    return D_wj, D_wi_wj \n\ndef get_topic_coherence(beta, data, vocab):\n    D = len(data) ## number of docs...data is list of documents\n    print('D: ', D)\n    TC = []\n    num_topics = len(beta)\n    for k in range(num_topics):\n        print('k: {}/{}'.format(k, num_topics))\n        top_10 = list(beta[k].argsort()[-11:][::-1])\n        top_words = [vocab[a] for a in top_10]\n        TC_k = 0\n        counter = 0\n        for i, word in enumerate(top_10):\n            # get D(w_i)\n            D_wi = get_document_frequency(data, word)\n            j = i + 1\n            tmp = 0\n            while j < len(top_10) and j > i:\n                # get D(w_j) and D(w_i, w_j)\n                D_wj, D_wi_wj = get_document_frequency(data, word, top_10[j])\n                # get f(w_i, w_j)\n                if D_wi_wj == 0:\n                    f_wi_wj = -1\n                else:\n                    f_wi_wj = -1 + ( np.log(D_wi) + np.log(D_wj)  - 2.0 * np.log(D) ) / ( np.log(D_wi_wj) - np.log(D) )\n                # update tmp: \n                tmp += f_wi_wj\n                j += 1\n                counter += 1\n            # update TC_k\n            TC_k += tmp \n        TC.append(TC_k)\n    print('counter: ', counter)\n    print('num topics: ', len(TC))\n    #TC = np.mean(TC) / counter\n    print('Topic Coherence is: {}'.format(TC))\n    return TC, counter\n\ndef log_gaussian(z, mu=None, logvar=None):\n    sz = z.size()\n    d = z.size(2)\n    bsz = z.size(1)\n    if mu is None or logvar is None:\n        mu = torch.zeros(bsz, d).to(z.device)\n        logvar = torch.zeros(bsz, d).to(z.device)\n    mu = mu.unsqueeze(0)\n    logvar = logvar.unsqueeze(0)\n    var = logvar.exp()\n    log_density = ((z - mu)**2 / (var+tiny)).sum(2) # b\n    log_det = logvar.sum(2) # b\n    log_density = log_density + log_det + d*np.log(2*np.pi)\n    return -0.5*log_density\n\ndef logsumexp(x, dim=0):\n    d = torch.max(x, dim)[0]   \n    if x.dim() == 1:\n        return torch.log(torch.exp(x - d).sum(dim)) + d\n    else:\n        return torch.log(torch.exp(x - d.unsqueeze(dim).expand_as(x)).sum(dim) + tiny) + d\n\ndef flatten_docs(docs): #to get words and doc_indices\n    words = [x for y in docs for x in y]\n    doc_indices = [[j for _ in doc] for j, doc in enumerate(docs)]\n    doc_indices = [x for y in doc_indices for x in y]\n    return words, doc_indices\n    \ndef onehot(data, min_length):\n    return list(np.bincount(data, minlength=min_length))\n\ndef nearest_neighbors(word, embeddings, vocab, num_words):\n    vectors = embeddings.cpu().numpy() \n    index = vocab.index(word)\n    query = embeddings[index].cpu().numpy() \n    ranks = vectors.dot(query).squeeze()\n    denom = query.T.dot(query).squeeze()\n    denom = denom * np.sum(vectors**2, 1)\n    denom = np.sqrt(denom)\n    ranks = ranks / denom\n    mostSimilar = []\n    [mostSimilar.append(idx) for idx in ranks.argsort()[::-1]]\n    nearest_neighbors = mostSimilar[:num_words]\n    nearest_neighbors = [vocab[comp] for comp in nearest_neighbors]\n    return nearest_neighbors\n\ndef visualize(docs, _lda_keys, topics, theta):\n    tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n    # project to 2D\n    tsne_lda = tsne_model.fit_transform(theta)\n    colormap = []\n    for name, hex in matplotlib.colors.cnames.items():\n        colormap.append(hex)\n\n    colormap = colormap[:len(theta[0, :])]\n    colormap = np.array(colormap)\n\n    title = '20 newsgroups TE embedding V viz'\n    num_example = len(docs)\n\n    plot_lda = bp.figure(plot_width=1400, plot_height=1100,\n                     title=title,\n                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n                     x_axis_type=None, y_axis_type=None, min_border=1)\n\n    plt.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1],\n                 color=colormap[_lda_keys][:num_example])\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:51:46.495133Z","iopub.execute_input":"2021-12-25T09:51:46.495494Z","iopub.status.idle":"2021-12-25T09:51:46.534613Z","shell.execute_reply.started":"2021-12-25T09:51:46.495458Z","shell.execute_reply":"2021-12-25T09:51:46.533802Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\n\nimport numpy as np\nimport scipy.io\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef _fetch(path, name):\n    if name == 'train':\n        token_file = os.path.join(path, 'bow_tr_tokens')\n        count_file = os.path.join(path, 'bow_tr_counts')\n    elif name == 'valid':\n        token_file = os.path.join(path, 'bow_va_tokens')\n        count_file = os.path.join(path, 'bow_va_counts')\n    else:\n        token_file = os.path.join(path, 'bow_ts_tokens')\n        count_file = os.path.join(path, 'bow_ts_counts')\n    tokens = scipy.io.loadmat(token_file)['tokens'].squeeze()\n    counts = scipy.io.loadmat(count_file)['counts'].squeeze()\n    if name == 'test':\n        token_1_file = os.path.join(path, 'bow_ts_h1_tokens')\n        count_1_file = os.path.join(path, 'bow_ts_h1_counts')\n        token_2_file = os.path.join(path, 'bow_ts_h2_tokens')\n        count_2_file = os.path.join(path, 'bow_ts_h2_counts')\n        tokens_1 = scipy.io.loadmat(token_1_file)['tokens'].squeeze()\n        counts_1 = scipy.io.loadmat(count_1_file)['counts'].squeeze()\n        tokens_2 = scipy.io.loadmat(token_2_file)['tokens'].squeeze()\n        counts_2 = scipy.io.loadmat(count_2_file)['counts'].squeeze()\n        return {'tokens': tokens, 'counts': counts, 'tokens_1': tokens_1, 'counts_1': counts_1, 'tokens_2': tokens_2,\n                'counts_2': counts_2}\n    return {'tokens': tokens, 'counts': counts}\n\n\ndef _fetch_temporal(path, name):\n    if name == 'train':\n        token_file = os.path.join(path, 'bow_tr_tokens')\n        count_file = os.path.join(path, 'bow_tr_counts')\n        time_file = os.path.join(path, 'bow_tr_timestamps')\n    elif name == 'valid':\n        token_file = os.path.join(path, 'bow_va_tokens')\n        count_file = os.path.join(path, 'bow_va_counts')\n        time_file = os.path.join(path, 'bow_va_timestamps')\n    else:\n        token_file = os.path.join(path, 'bow_ts_tokens')\n        count_file = os.path.join(path, 'bow_ts_counts')\n        time_file = os.path.join(path, 'bow_ts_timestamps')\n    tokens = scipy.io.loadmat(token_file)['tokens'].squeeze()\n    counts = scipy.io.loadmat(count_file)['counts'].squeeze()\n    times = scipy.io.loadmat(time_file)['timestamps'].squeeze()\n    if name == 'test':\n        token_1_file = os.path.join(path, 'bow_ts_h1_tokens')\n        count_1_file = os.path.join(path, 'bow_ts_h1_counts')\n        token_2_file = os.path.join(path, 'bow_ts_h2_tokens')\n        count_2_file = os.path.join(path, 'bow_ts_h2_counts')\n        tokens_1 = scipy.io.loadmat(token_1_file)['tokens'].squeeze()\n        counts_1 = scipy.io.loadmat(count_1_file)['counts'].squeeze()\n        tokens_2 = scipy.io.loadmat(token_2_file)['tokens'].squeeze()\n        counts_2 = scipy.io.loadmat(count_2_file)['counts'].squeeze()\n        return {'tokens': tokens, 'counts': counts, 'times': times,\n                'tokens_1': tokens_1, 'counts_1': counts_1,\n                'tokens_2': tokens_2, 'counts_2': counts_2}\n    return {'tokens': tokens, 'counts': counts, 'times': times}\n\n\ndef get_data(path, temporal=False):\n    ### load vocabulary\n    with open(os.path.join(path, 'vocab.pkl'), 'rb') as f:\n        vocab = pickle.load(f)\n\n    if not temporal:\n        train = _fetch(path, 'train')\n        valid = _fetch(path, 'valid')\n        test = _fetch(path, 'test')\n    else:\n        train = _fetch_temporal(path, 'train')\n        valid = _fetch_temporal(path, 'valid')\n        test = _fetch_temporal(path, 'test')\n\n    return vocab, train, valid, test\n\n\ndef get_batch(tokens, counts, ind, vocab_size, temporal=False, times=None):\n    \"\"\"fetch input data by batch.\"\"\"\n    batch_size = len(ind)\n    data_batch = np.zeros((batch_size, vocab_size))\n    if temporal:\n        times_batch = np.zeros((batch_size,))\n    for i, doc_id in enumerate(ind):\n        doc = tokens[doc_id]\n        count = counts[doc_id]\n        if temporal:\n            timestamp = times[doc_id]\n            times_batch[i] = timestamp\n        L = count.shape[1]\n        if len(doc) == 1:\n            doc = [doc.squeeze()]\n            count = [count.squeeze()]\n        else:\n            doc = doc.squeeze()\n            count = count.squeeze()\n        if doc_id != -1:\n            for j, word in enumerate(doc):\n                data_batch[i, word] = count[j]\n    data_batch = torch.from_numpy(data_batch).float().to(device)\n    if temporal:\n        times_batch = torch.from_numpy(times_batch).to(device)\n        return data_batch, times_batch\n    return data_batch\n\n\ndef get_rnn_input(tokens, counts, times, num_times, vocab_size, num_docs):\n    # shuffle\n    indices = torch.randperm(num_docs)\n    # split to 1000 sets\n    indices = torch.split(indices, 1000)\n    # TxV\n    rnn_input = torch.zeros(num_times, vocab_size).to(device)\n    # times count\n    cnt = torch.zeros(num_times, ).to(device)\n    for idx, ind in enumerate(indices):\n        data_batch, times_batch = get_batch(tokens, counts, ind, vocab_size, temporal=True, times=times)\n        for t in range(num_times):\n            tmp = (times_batch == t).nonzero()\n            docs = data_batch[tmp].squeeze().sum(0)\n            rnn_input[t] += docs\n            cnt[t] += len(tmp)\n        if idx % 20 == 0:\n            print('idx: {}/{}'.format(idx, len(indices)))\n    rnn_input = rnn_input / cnt.unsqueeze(1)\n    return rnn_input\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:51:46.536216Z","iopub.execute_input":"2021-12-25T09:51:46.536468Z","iopub.status.idle":"2021-12-25T09:51:46.574199Z","shell.execute_reply.started":"2021-12-25T09:51:46.536437Z","shell.execute_reply":"2021-12-25T09:51:46.573238Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"\"\"\"This file defines a dynamic etm object.\n\"\"\"\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass DETM(nn.Module):\n    def __init__(self, args, embeddings):\n        super(DETM, self).__init__()\n\n        ## define hyperparameters\n        self.num_topics = args.num_topics\n\n        self.num_times = args.num_times\n        self.vocab_size = args.vocab_size\n        self.t_hidden_size = args.t_hidden_size\n        self.eta_hidden_size = args.eta_hidden_size\n        self.rho_size = args.rho_size\n        self.emsize = args.emb_size\n        self.enc_drop = args.enc_drop\n        self.eta_nlayers = args.eta_nlayers\n        self.t_drop = nn.Dropout(args.enc_drop)\n        self.delta = args.delta\n        self.train_embeddings = args.train_embeddings\n\n        self.theta_act = self.get_activation(args.theta_act)\n\n        ## define the word embedding matrix \\rho\n        if args.train_embeddings:\n            self.rho = nn.Linear(args.rho_size, args.vocab_size, bias=False)\n        else:\n            num_embeddings, emsize = embeddings.size()\n            rho = nn.Embedding(num_embeddings, emsize)\n            rho.weight.data = embeddings\n            self.rho = rho.weight.data.clone().float().to(device)\n\n        ## define the variational parameters for the topic embeddings over time (alpha) ... alpha is K x T x L\n        self.mu_q_alpha = nn.Parameter(torch.randn(args.num_topics, args.num_times, args.rho_size))\n        self.logsigma_q_alpha = nn.Parameter(torch.randn(args.num_topics, args.num_times, args.rho_size))\n\n        ## define variational distribution for \\eta via amortizartion... eta is K x T\n        ## Eta\n        ## V->L\n        ## L+K->K\n        self.q_eta_map = nn.Linear(args.vocab_size, args.eta_hidden_size)\n        self.q_eta = nn.LSTM(args.eta_hidden_size, args.eta_hidden_size, args.eta_nlayers, dropout=args.eta_dropout)\n\n        self.mu_q_eta = nn.Linear(args.eta_hidden_size + args.num_topics, args.num_topics, bias=True)\n        self.logsigma_q_eta = nn.Linear(args.eta_hidden_size + args.num_topics, args.num_topics, bias=True)\n\n        ## define variational distribution for \\theta_{1:D} via amortizartion... theta is K x D\n        ## Theta\n        ## V+K->K\n        self.q_theta = nn.Sequential(\n            nn.Linear(args.vocab_size + args.num_topics, args.t_hidden_size),\n            self.theta_act,\n            nn.Linear(args.t_hidden_size, args.t_hidden_size),\n            self.theta_act,\n        )\n        self.logsigma_q_theta = nn.Linear(args.t_hidden_size, args.num_topics, bias=True)\n        self.mu_q_theta = nn.Linear(args.t_hidden_size, args.num_topics, bias=True)\n\n    def get_activation(self, act):\n        if act == 'tanh':\n            act = nn.Tanh()\n        elif act == 'relu':\n            act = nn.ReLU()\n        elif act == 'softplus':\n            act = nn.Softplus()\n        elif act == 'rrelu':\n            act = nn.RReLU()\n        elif act == 'leakyrelu':\n            act = nn.LeakyReLU()\n        elif act == 'elu':\n            act = nn.ELU()\n        elif act == 'selu':\n            act = nn.SELU()\n        elif act == 'glu':\n            act = nn.GLU()\n        else:\n            print('Defaulting to tanh activations...')\n            act = nn.Tanh()\n        return act\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"Returns a sample from a Gaussian distribution via reparameterization.\n        \"\"\"\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std)\n            return eps.mul_(std).add_(mu)\n        else:\n            return mu\n\n    def get_kl(self, q_mu, q_logsigma, p_mu=None, p_logsigma=None):\n        \"\"\" Gaussian KL Divergence\n        Returns KL( N(q_mu, q_logsigma) || N(p_mu, p_logsigma) ).\n        \"\"\"\n        if p_mu is not None and p_logsigma is not None:\n            sigma_q_sq = torch.exp(q_logsigma)\n            sigma_p_sq = torch.exp(p_logsigma)\n            kl = (sigma_q_sq + (q_mu - p_mu) ** 2) / (sigma_p_sq + 1e-6)\n            kl = kl - 1 + p_logsigma - q_logsigma\n            kl = 0.5 * torch.sum(kl, dim=-1)\n        else:\n            kl = -0.5 * torch.sum(1 + q_logsigma - q_mu.pow(2) - q_logsigma.exp(), dim=-1)\n        return kl\n\n    # Compute α[t] ~ N(α[t-1],γ^2*I), with reparam trick\n    # Compute KL(N(μσ),N())\n    # alpha TxKxL\n    def get_alpha(self):  ## mean field\n        # TxKxL\n        alphas = torch.zeros(self.num_times, self.num_topics, self.rho_size).to(device)\n        kl_alpha = []\n        # rt\n        alphas[0] = self.reparameterize(self.mu_q_alpha[:, 0, :], self.logsigma_q_alpha[:, 0, :])\n        p_mu_0 = torch.zeros(self.num_topics, self.rho_size).to(device)\n        logsigma_p_0 = torch.zeros(self.num_topics, self.rho_size).to(device)\n        # kl-divergence for 0\n        kl_0 = self.get_kl(self.mu_q_alpha[:, 0, :], self.logsigma_q_alpha[:, 0, :], p_mu_0, logsigma_p_0)\n        kl_alpha.append(kl_0)\n        # for each\n        # rt\n        for t in range(1, self.num_times):\n            alphas[t] = self.reparameterize(self.mu_q_alpha[:, t, :], self.logsigma_q_alpha[:, t, :])\n            # kl-divergence  || N(a[t-1],s^2*I)\n            p_mu_t = alphas[t - 1]\n            logsigma_p_t = torch.log(self.delta * torch.ones(self.num_topics, self.rho_size).to(device))\n            kl_t = self.get_kl(self.mu_q_alpha[:, t, :], self.logsigma_q_alpha[:, t, :], p_mu_t, logsigma_p_t)\n            kl_alpha.append(kl_t)\n        kl_alpha = torch.stack(kl_alpha).sum()\n        return alphas, kl_alpha.sum()\n\n    # Compute η[t]~N(η[t-1], δ^2*I), η[0]=\n    def get_eta(self, rnn_inp):  ## structured amortized inference\n        inp = self.q_eta_map(rnn_inp).unsqueeze(1)\n        hidden = self.init_hidden()\n        output, _ = self.q_eta(inp, hidden)\n        output = output.squeeze()\n\n        etas = torch.zeros(self.num_times, self.num_topics).to(device)\n        kl_eta = []\n        # Compute η[0]\n        inp_0 = torch.cat([output[0], torch.zeros(self.num_topics, ).to(device)], dim=0)\n        mu_0 = self.mu_q_eta(inp_0)\n        logsigma_0 = self.logsigma_q_eta(inp_0)\n        etas[0] = self.reparameterize(mu_0, logsigma_0)\n\n        p_mu_0 = torch.zeros(self.num_topics, ).to(device)\n        logsigma_p_0 = torch.zeros(self.num_topics, ).to(device)\n        kl_0 = self.get_kl(mu_0, logsigma_0, p_mu_0, logsigma_p_0)\n        kl_eta.append(kl_0)\n\n        # for t:[1,T]\n        for t in range(1, self.num_times):\n            inp_t = torch.cat([output[t], etas[t - 1]], dim=0)\n            mu_t = self.mu_q_eta(inp_t)\n            logsigma_t = self.logsigma_q_eta(inp_t)\n            etas[t] = self.reparameterize(mu_t, logsigma_t)\n\n            logsigma_p_t = torch.log(self.delta * torch.ones(self.num_topics, ).to(device))\n            kl_t = self.get_kl(mu_t, logsigma_t, etas[t - 1], logsigma_p_t)\n            kl_eta.append(kl_t)\n        kl_eta = torch.stack(kl_eta).sum()\n        return etas, kl_eta\n\n    # θ~LN(η[t],α^2*I)\n    # get theta according to the timestamp\n    # input:\n    def get_theta(self, eta, bows, times):  # amortized inference\n        \"\"\"Returns the topic proportions.\"\"\"\n        eta_td = eta[times.type('torch.LongTensor')]\n        inp = torch.cat([bows, eta_td], dim=1)\n        q_theta = self.q_theta(inp)\n        if self.enc_drop > 0:\n            q_theta = self.t_drop(q_theta)\n        mu_theta = self.mu_q_theta(q_theta)\n        logsigma_theta = self.logsigma_q_theta(q_theta)\n        z = self.reparameterize(mu_theta, logsigma_theta)\n        theta = F.softmax(z, dim=-1)\n        kl_theta = self.get_kl(mu_theta, logsigma_theta, eta_td, torch.zeros(self.num_topics).to(device))\n        return theta, kl_theta\n\n    # w_d~softmax(ρ*α[t_d])\n    # return\n    def get_beta(self, alpha):\n        \"\"\"Returns the topic matrix \\beta of shape K x V\"\"\"\n        if self.train_embeddings:\n            logit = self.rho(alpha.view(alpha.size(0) * alpha.size(1), self.rho_size))\n        else:\n            tmp = alpha.view(alpha.size(0) * alpha.size(1), self.rho_size)\n            logit = torch.mm(tmp, self.rho.permute(1, 0))\n        logit = logit.view(alpha.size(0), alpha.size(1), -1)\n        beta = F.softmax(logit, dim=-1)\n        return beta\n\n    def get_nll(self, theta, beta, bows):\n        theta = theta.unsqueeze(1)\n        loglik = torch.bmm(theta, beta).squeeze(1)\n        loglik = torch.log(loglik + 1e-6)\n        nll = -loglik * bows\n        nll = nll.sum(-1)\n        return nll\n\n    def forward(self, bows, normalized_bows, times, rnn_inp, num_docs):\n        bsz = normalized_bows.size(0)\n        coeff = num_docs / bsz\n        # 1. get alpha TxKxL\n        alpha, kl_alpha = self.get_alpha()\n        # 2. get eta TxK\n        eta, kl_eta = self.get_eta(rnn_inp)\n        # 3. get theta DxK\n        theta, kl_theta = self.get_theta(eta, normalized_bows, times)\n        kl_theta = kl_theta.sum() * coeff\n        # 4. get beta (rho*alpha)\n        beta = self.get_beta(alpha)\n        beta = beta[times.type('torch.LongTensor')]\n        # 5. get nll loss\n        nll = self.get_nll(theta, beta, bows)\n        nll = nll.sum() * coeff\n        nelbo = nll + kl_alpha + kl_eta + kl_theta\n        return nelbo, nll, kl_alpha, kl_eta, kl_theta\n\n    def init_hidden(self):\n        \"\"\"Initializes the first hidden state of the RNN used as inference network for \\eta.\n        \"\"\"\n        weight = next(self.parameters())\n        nlayers = self.eta_nlayers\n        nhid = self.eta_hidden_size\n        return (weight.new_zeros(nlayers, 1, nhid), weight.new_zeros(nlayers, 1, nhid))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:51:46.575828Z","iopub.execute_input":"2021-12-25T09:51:46.576277Z","iopub.status.idle":"2021-12-25T09:51:46.635695Z","shell.execute_reply.started":"2021-12-25T09:51:46.576235Z","shell.execute_reply":"2021-12-25T09:51:46.634349Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"#/usr/bin/python\n\nfrom __future__ import print_function\n\nimport argparse\nimport torch\nimport pickle \nimport numpy as np \nimport os \nimport math \nimport random \nimport sys\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport scipy.io\n\n\nfrom sklearn.decomposition import PCA\nfrom torch import nn, optim\nfrom torch.nn import functional as F\n\nparser = argparse.ArgumentParser(description='The Embedded Topic Model')\n\n### data and file related arguments\nparser.add_argument('--dataset', type=str, default='un', help='name of corpus')\nparser.add_argument('--data_path', type=str, default='./', help='directory containing data')\nparser.add_argument('--emb_path', type=str, default='skipgram/embeddings.txt', help='directory containing embeddings')\nparser.add_argument('--save_path', type=str, default='./results', help='path to save results')\nparser.add_argument('--batch_size', type=int, default=1000, help='number of documents in a batch for training')\nparser.add_argument('--min_df', type=int, default=100, help='to get the right data..minimum document frequency')\n\n### model-related arguments\nparser.add_argument('--num_topics', type=int, default=30, help='number of topics')\nparser.add_argument('--rho_size', type=int, default=300, help='dimension of rho')\nparser.add_argument('--emb_size', type=int, default=300, help='dimension of embeddings')\nparser.add_argument('--t_hidden_size', type=int, default=800, help='dimension of hidden space of q(theta)')\nparser.add_argument('--theta_act', type=str, default='relu', help='tanh, softplus, relu, rrelu, leakyrelu, elu, selu, glu)')\nparser.add_argument('--train_embeddings', type=int, default=1, help='whether to fix rho or train it')\nparser.add_argument('--eta_nlayers', type=int, default=3, help='number of layers for eta')\nparser.add_argument('--eta_hidden_size', type=int, default=200, help='number of hidden units for rnn')\nparser.add_argument('--delta', type=float, default=0.005, help='prior variance')\n\n### optimization-related arguments\nparser.add_argument('--lr', type=float, default=0.005, help='learning rate')\nparser.add_argument('--lr_factor', type=float, default=4.0, help='divide learning rate by this')\nparser.add_argument('--epochs', type=int, default=100, help='number of epochs to train')\nparser.add_argument('--mode', type=str, default='train', help='train or eval model')\nparser.add_argument('--optimizer', type=str, default='adam', help='choice of optimizer')\nparser.add_argument('--seed', type=int, default=2019, help='random seed (default: 1)')\nparser.add_argument('--enc_drop', type=float, default=0.0, help='dropout rate on encoder')\nparser.add_argument('--eta_dropout', type=float, default=0.0, help='dropout rate on rnn for eta')\nparser.add_argument('--clip', type=float, default=0.0, help='gradient clipping')\nparser.add_argument('--nonmono', type=int, default=10, help='number of bad hits allowed')\nparser.add_argument('--wdecay', type=float, default=1.2e-6, help='some l2 regularization')\nparser.add_argument('--anneal_lr', type=int, default=0, help='whether to anneal the learning rate or not')\nparser.add_argument('--bow_norm', type=int, default=0, help='normalize the bows or not')\n\n### evaluation, visualization, and logging-related arguments\nparser.add_argument('--num_words', type=int, default=20, help='number of words for topic viz')\nparser.add_argument('--log_interval', type=int, default=10, help='when to log training')\nparser.add_argument('--visualize_every', type=int, default=1, help='when to visualize results')\nparser.add_argument('--eval_batch_size', type=int, default=1000, help='input batch size for evaluation')\nparser.add_argument('--load_from', type=str, default='', help='the name of the ckpt to eval from')\nparser.add_argument('--tc', type=int, default=0, help='whether to compute tc or not')\n\nargs, _ = parser.parse_known_args()\n\npca = PCA(n_components=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n## set seed\nnp.random.seed(args.seed)\ntorch.backends.cudnn.deterministic = True\ntorch.manual_seed(args.seed)\n\n## get data\n# 1. vocabulary\nprint('Getting vocabulary ...')\ndata_file = os.path.join(args.data_path, 'min_df_{}'.format(args.min_df))\nvocab, train, valid, test = get_data(data_file, temporal=True)\nvocab_size = len(vocab)\nargs.vocab_size = vocab_size\n\n# 1. training data\nprint('Getting training data ...')\ntrain_tokens = train['tokens']\ntrain_counts = train['counts']\ntrain_times = train['times']\nargs.num_times = len(np.unique(train_times))\nargs.num_docs_train = len(train_tokens)\ntrain_rnn_inp = get_rnn_input(\n    train_tokens, train_counts, train_times, args.num_times, args.vocab_size, args.num_docs_train)\n\n# 2. dev set\nprint('Getting validation data ...')\nvalid_tokens = valid['tokens']\nvalid_counts = valid['counts']\nvalid_times = valid['times']\nargs.num_docs_valid = len(valid_tokens)\nvalid_rnn_inp = get_rnn_input(\n    valid_tokens, valid_counts, valid_times, args.num_times, args.vocab_size, args.num_docs_valid)\n\n# 3. test data\nprint('Getting testing data ...')\ntest_tokens = test['tokens']\ntest_counts = test['counts']\ntest_times = test['times']\nargs.num_docs_test = len(test_tokens)\ntest_rnn_inp = get_rnn_input(\n    test_tokens, test_counts, test_times, args.num_times, args.vocab_size, args.num_docs_test)\n\ntest_1_tokens = test['tokens_1']\ntest_1_counts = test['counts_1']\ntest_1_times = test_times\nargs.num_docs_test_1 = len(test_1_tokens)\ntest_1_rnn_inp = get_rnn_input(\n    test_1_tokens, test_1_counts, test_1_times, args.num_times, args.vocab_size, args.num_docs_test)\n\ntest_2_tokens = test['tokens_2']\ntest_2_counts = test['counts_2']\ntest_2_times = test_times\nargs.num_docs_test_2 = len(test_2_tokens)\ntest_2_rnn_inp = get_rnn_input(\n    test_2_tokens, test_2_counts, test_2_times, args.num_times, args.vocab_size, args.num_docs_test)\n\n## get embeddings \n# print('Getting embeddings ...')\n# emb_path = args.emb_path\n# vect_path = os.path.join(args.data_path.split('/')[0], 'embeddings.pkl')   \n# vectors = {}\n# with open(emb_path, 'rb') as f:\n#     for l in f:\n#         line = l.decode().split()\n#         word = line[0]\n#         if word in vocab:\n#             vect = np.array(line[1:]).astype(np.float)\n#             vectors[word] = vect\nembeddings = np.zeros((vocab_size, args.emb_size))\nwords_found = 0\nfor i, word in enumerate(vocab):\n    try: \n        embeddings[i] = vectors[word]\n        words_found += 1\n    except KeyError:\n        embeddings[i] = np.random.normal(scale=0.6, size=(args.emb_size, ))\nembeddings = torch.from_numpy(embeddings).to(device)\nargs.embeddings_dim = embeddings.size()\n\nprint('\\n')\nprint('=*'*100)\nprint('Training a Dynamic Embedded Topic Model on {} with the following settings: {}'.format(args.dataset.upper(), args))\nprint('=*'*100)\n\n## define checkpoint\nif not os.path.exists(args.save_path):\n    os.makedirs(args.save_path)\n\nif args.mode == 'eval':\n    ckpt = args.load_from\nelse:\n    ckpt = os.path.join(args.save_path, \n        'detm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_L_{}_minDF_{}_trainEmbeddings_{}'.format(\n        args.dataset, args.num_topics, args.t_hidden_size, args.optimizer, args.clip, args.theta_act, \n            args.lr, args.batch_size, args.rho_size, args.eta_nlayers, args.min_df, args.train_embeddings))\n\n## define model and optimizer\nif args.load_from != '':\n    print('Loading checkpoint from {}'.format(args.load_from))\n    with open(args.load_from, 'rb') as f:\n        model = torch.load(f)\nelse:\n    model = DETM(args, embeddings)\nprint('\\nDETM architecture: {}'.format(model))\nmodel.to(device)\n\nif args.optimizer == 'adam':\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\nelif args.optimizer == 'adagrad':\n    optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\nelif args.optimizer == 'adadelta':\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\nelif args.optimizer == 'rmsprop':\n    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\nelif args.optimizer == 'asgd':\n    optimizer = optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\nelse:\n    print('Defaulting to vanilla SGD')\n    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n\ndef train(epoch):\n    \"\"\"Train DETM on data for one epoch.\n    \"\"\"\n    model.train()\n    acc_loss = 0\n    acc_nll = 0\n    acc_kl_theta_loss = 0\n    acc_kl_eta_loss = 0\n    acc_kl_alpha_loss = 0\n    cnt = 0\n    indices = torch.randperm(args.num_docs_train)\n    indices = torch.split(indices, args.batch_size) \n    for idx, ind in enumerate(indices):\n        optimizer.zero_grad()\n        model.zero_grad()\n        data_batch, times_batch = get_batch(\n            train_tokens, train_counts, ind, args.vocab_size, temporal=True, times=train_times)\n        sums = data_batch.sum(1).unsqueeze(1)\n        if args.bow_norm:\n            normalized_data_batch = data_batch / sums\n        else:\n            normalized_data_batch = data_batch\n\n        loss, nll, kl_alpha, kl_eta, kl_theta = model(data_batch, normalized_data_batch, times_batch, train_rnn_inp, args.num_docs_train)\n        loss.backward()\n        if args.clip > 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n        optimizer.step()\n\n        acc_loss += torch.sum(loss).item()\n        acc_nll += torch.sum(nll).item()\n        acc_kl_theta_loss += torch.sum(kl_theta).item()\n        acc_kl_eta_loss += torch.sum(kl_eta).item()\n        acc_kl_alpha_loss += torch.sum(kl_alpha).item()\n        cnt += 1\n\n        if idx % args.log_interval == 0 and idx > 0:\n            cur_loss = round(acc_loss / cnt, 2) \n            cur_nll = round(acc_nll / cnt, 2) \n            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n            cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n            cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n            lr = optimizer.param_groups[0]['lr']\n            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n                epoch, idx, len(indices), lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n    \n    cur_loss = round(acc_loss / cnt, 2) \n    cur_nll = round(acc_nll / cnt, 2) \n    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n    cur_kl_eta = round(acc_kl_eta_loss / cnt, 2) \n    cur_kl_alpha = round(acc_kl_alpha_loss / cnt, 2) \n    lr = optimizer.param_groups[0]['lr']\n    print('*'*100)\n    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. KL_eta: {} .. KL_alpha: {} .. Rec_loss: {} .. NELBO: {}'.format(\n            epoch, lr, cur_kl_theta, cur_kl_eta, cur_kl_alpha, cur_nll, cur_loss))\n    print('*'*100)\n\ndef visualize():\n    \"\"\"Visualizes topics and embeddings and word usage evolution.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        alpha = model.mu_q_alpha\n        beta = model.get_beta(alpha)\n        print('beta: ', beta.size())\n        print('\\n')\n        print('#'*100)\n        print('Visualize topics...')\n        times = [0, 10, 20]\n        topics_words = []\n        for k in range(args.num_topics):\n            for t in times:\n                #print(beta.shape)\n                gamma = beta[k, t, :]\n                top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n                topic_words = [vocab[a] for a in top_words]\n                topics_words.append(' '.join(topic_words))\n                print('Topic {} .. Time: {} ===> {}'.format(k, t, topic_words)) \n\n        print('\\n')\n        print('Visualize word embeddings ...')\n        queries = ['economic', 'assembly', 'security', 'management', 'debt', 'rights',  'africa']\n        try:\n            embeddings = model.rho.weight  # Vocab_size x E\n        except:\n            embeddings = model.rho         # Vocab_size x E\n        neighbors = []\n        for word in queries:\n            print('word: {} .. neighbors: {}'.format(\n                word, nearest_neighbors(word, embeddings, vocab, args.num_words)))\n        print('#'*100)\n\n        # print('\\n')\n        # print('Visualize word evolution ...')\n        # topic_0 = None ### k \n        # queries_0 = ['woman', 'gender', 'man', 'mankind', 'humankind'] ### v \n\n        # topic_1 = None\n        # queries_1 = ['africa', 'colonial', 'racist', 'democratic']\n\n        # topic_2 = None\n        # queries_2 = ['poverty', 'sustainable', 'trade']\n\n        # topic_3 = None\n        # queries_3 = ['soviet', 'convention', 'iran']\n\n        # topic_4 = None # climate\n        # queries_4 = ['environment', 'impact', 'threats', 'small', 'global', 'climate']\n\ndef _eta_helper(rnn_inp):\n    inp = model.q_eta_map(rnn_inp).unsqueeze(1)\n    hidden = model.init_hidden()\n    output, _ = model.q_eta(inp, hidden)\n    output = output.squeeze()\n    etas = torch.zeros(model.num_times, model.num_topics).to(device)\n    inp_0 = torch.cat([output[0], torch.zeros(model.num_topics,).to(device)], dim=0)\n    etas[0] = model.mu_q_eta(inp_0)\n    for t in range(1, model.num_times):\n        inp_t = torch.cat([output[t], etas[t-1]], dim=0)\n        etas[t] = model.mu_q_eta(inp_t)\n    return etas\n\ndef get_eta(source):\n    model.eval()\n    with torch.no_grad():\n        if source == 'val':\n            rnn_inp = valid_rnn_inp\n            return _eta_helper(rnn_inp)\n        else:\n            rnn_1_inp = test_1_rnn_inp\n            return _eta_helper(rnn_1_inp)\n\ndef get_theta(eta, bows):\n    model.eval()\n    with torch.no_grad():\n        inp = torch.cat([bows, eta], dim=1)\n        q_theta = model.q_theta(inp)\n        mu_theta = model.mu_q_theta(q_theta)\n        theta = F.softmax(mu_theta, dim=-1)\n        return theta    \n\ndef get_completion_ppl(source):\n    \"\"\"Returns document completion perplexity.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        alpha = model.mu_q_alpha\n        if source == 'val':\n            indices = torch.split(torch.tensor(range(args.num_docs_valid)), args.eval_batch_size)\n            tokens = valid_tokens\n            counts = valid_counts\n            times = valid_times\n            eta = get_eta('val')\n\n            acc_loss = 0\n            cnt = 0\n            for idx, ind in enumerate(indices):\n                data_batch, times_batch = get_batch(\n                    tokens, counts, ind, args.vocab_size, temporal=True, times=times)\n                sums = data_batch.sum(1).unsqueeze(1)\n                if args.bow_norm:\n                    normalized_data_batch = data_batch / sums\n                else:\n                    normalized_data_batch = data_batch\n\n                # 1. select time D[T]xTxK\n                eta_td = eta[times_batch.type('torch.LongTensor')]\n                # 2. get theta (eta_t) D[T]\n                theta = get_theta(eta_td, normalized_data_batch)\n                # 3. get alpha_t (KxD[T]xL)\n                alpha_td = alpha[:, times_batch.type('torch.LongTensor'), :]\n                # 4. get beta\n                ## alpha(KxD[T]xV) -> D[T]xKxV\n                beta = model.get_beta(alpha_td).permute(1, 0, 2)\n                # 5. get log-likelihood\n                # DxKx1 * D[T]xKxV\n                loglik = theta.unsqueeze(2) * beta\n                # D[T]xKxV\n                loglik = loglik.sum(1)\n                loglik = torch.log(loglik)\n                # 6. calculate perplexity\n                ## log()*databatch, sum(1)\n                nll = -loglik * data_batch\n                nll = nll.sum(-1)\n                loss = nll / sums.squeeze()\n                loss = loss.mean().item()\n                acc_loss += loss\n                cnt += 1\n            cur_loss = acc_loss / cnt\n            ppl_all = round(math.exp(cur_loss), 1)\n            print('*'*100)\n            print('{} PPL: {}'.format(source.upper(), ppl_all))\n            print('*'*100)\n            return ppl_all\n        else: \n            indices = torch.split(torch.tensor(range(args.num_docs_test)), args.eval_batch_size)\n            tokens_1 = test_1_tokens\n            counts_1 = test_1_counts\n\n            tokens_2 = test_2_tokens\n            counts_2 = test_2_counts\n\n            eta_1 = get_eta('test')\n\n            acc_loss = 0\n            cnt = 0\n            indices = torch.split(torch.tensor(range(args.num_docs_test)), args.eval_batch_size)\n            for idx, ind in enumerate(indices):\n                data_batch_1, times_batch_1 = get_batch(\n                    tokens_1, counts_1, ind, args.vocab_size, temporal=True, times=test_times)\n                sums_1 = data_batch_1.sum(1).unsqueeze(1)\n                if args.bow_norm:\n                    normalized_data_batch_1 = data_batch_1 / sums_1\n                else:\n                    normalized_data_batch_1 = data_batch_1\n                # DTxTxK\n                eta_td_1 = eta_1[times_batch_1.type('torch.LongTensor')]\n\n                theta = get_theta(eta_td_1, normalized_data_batch_1)\n\n                data_batch_2, times_batch_2 = get_batch(\n                    tokens_2, counts_2, ind, args.vocab_size, temporal=True, times=test_times)\n                sums_2 = data_batch_2.sum(1).unsqueeze(1)\n\n                alpha_td = alpha[:, times_batch_2.type('torch.LongTensor'), :]\n                beta = model.get_beta(alpha_td).permute(1, 0, 2)\n                loglik = theta.unsqueeze(2) * beta\n                loglik = loglik.sum(1)\n                loglik = torch.log(loglik)\n                nll = -loglik * data_batch_2\n                nll = nll.sum(-1)\n                loss = nll / sums_2.squeeze()\n                loss = loss.mean().item()\n                acc_loss += loss\n                cnt += 1\n            cur_loss = acc_loss / cnt\n            ppl_dc = round(math.exp(cur_loss), 1)\n            print('*'*100)\n            print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n            print('*'*100)\n            return ppl_dc\n\ndef _diversity_helper(beta, num_tops):\n    list_w = np.zeros((args.num_topics, num_tops))\n    for k in range(args.num_topics):\n        gamma = beta[k, :]\n        top_words = gamma.cpu().numpy().argsort()[-num_tops:][::-1]\n        list_w[k, :] = top_words\n    list_w = np.reshape(list_w, (-1))\n    list_w = list(list_w)\n    n_unique = len(np.unique(list_w))\n    diversity = n_unique / (args.num_topics * num_tops)\n    return diversity\n\ndef get_topic_quality():\n    \"\"\"Returns topic coherence and topic diversity.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        alpha = model.mu_q_alpha\n        beta = model.get_beta(alpha) \n        print('beta: ', beta.size())\n\n        print('\\n')\n        print('#'*100)\n        print('Get topic diversity...')\n        num_tops = 25\n        TD_all = np.zeros((args.num_times,))\n        for tt in range(args.num_times):\n            TD_all[tt] = _diversity_helper(beta[:, tt, :], num_tops)\n        TD = np.mean(TD_all)\n        print('Topic Diversity is: {}'.format(TD))\n\n        print('\\n')\n        print('Get topic coherence...')\n        print('train_tokens: ', train_tokens[0])\n        TC_all = []\n        cnt_all = []\n        for tt in range(args.num_times):\n            tc, cnt = get_topic_coherence(beta[:, tt, :].cpu().numpy(), train_tokens, vocab)\n            TC_all.append(tc)\n            cnt_all.append(cnt)\n        print('TC_all: ', TC_all)\n        TC_all = torch.tensor(TC_all)\n        print('TC_all: ', TC_all.size())\n        print('\\n')\n        print('Get topic quality...')\n        quality = tc * diversity\n        print('Topic Quality is: {}'.format(quality))\n        print('#'*100)\n\nif args.mode == 'train':\n    ## train model on data by looping through multiple epochs\n    best_epoch = 0\n    best_val_ppl = 1e9\n    all_val_ppls = []\n    for epoch in range(1, args.epochs):\n        train(epoch)\n#         if epoch % args.visualize_every == 0:\n#             visualize()\n        val_ppl = get_completion_ppl('val')\n        print('val_ppl: ', val_ppl)\n        if val_ppl < best_val_ppl:\n            with open(ckpt, 'wb') as f:\n                torch.save(model, f)\n            best_epoch = epoch\n            best_val_ppl = val_ppl\n        else:\n            ## check whether to anneal lr\n            lr = optimizer.param_groups[0]['lr']\n            if args.anneal_lr and (len(all_val_ppls) > args.nonmono and val_ppl > min(all_val_ppls[:-args.nonmono]) and lr > 1e-5):\n                optimizer.param_groups[0]['lr'] /= args.lr_factor\n        all_val_ppls.append(val_ppl)\n    with open(ckpt, 'rb') as f:\n        model = torch.load(f)\n    model = model.to(device)\n    model.eval()\n    with torch.no_grad():\n        print('saving topic matrix beta...')\n        alpha = model.mu_q_alpha\n        beta = model.get_beta(alpha).cpu().numpy()\n        scipy.io.savemat(ckpt+'_beta.mat', {'values': beta}, do_compression=True)\n        if args.train_embeddings:\n            print('saving word embedding matrix rho...')\n            rho = model.rho.weight.cpu().numpy()\n            scipy.io.savemat(ckpt+'_rho.mat', {'values': rho}, do_compression=True)\n        print('computing validation perplexity...')\n        val_ppl = get_completion_ppl('val')\n        print('computing test perplexity...')\n        test_ppl = get_completion_ppl('test')\nelse: \n    with open(ckpt, 'rb') as f:\n        model = torch.load(f)\n    model = model.to(device)\n        \n    print('saving alpha...')\n    with torch.no_grad():\n        alpha = model.mu_q_alpha.cpu().numpy()\n        scipy.io.savemat(ckpt+'_alpha.mat', {'values': alpha}, do_compression=True)\n\n    print('computing validation perplexity...')\n    val_ppl = get_completion_ppl('val')\n    print('computing test perplexity...')\n    test_ppl = get_completion_ppl('test')\n    print('computing topic coherence and topic diversity...')\n    get_topic_quality()\n    print('visualizing topics and embeddings...')\n    visualize()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-25T09:51:46.637699Z","iopub.execute_input":"2021-12-25T09:51:46.637959Z","iopub.status.idle":"2021-12-25T10:18:56.816249Z","shell.execute_reply.started":"2021-12-25T09:51:46.637925Z","shell.execute_reply":"2021-12-25T10:18:56.815117Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"Getting vocabulary ...\nGetting training data ...\nidx: 0/7\nGetting validation data ...\nidx: 0/1\nGetting testing data ...\nidx: 0/1\nidx: 0/1\nidx: 0/1\n\n\n=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\nTraining a Dynamic Embedded Topic Model on UN with the following settings: Namespace(anneal_lr=0, batch_size=1000, bow_norm=0, clip=0.0, data_path='./', dataset='un', delta=0.005, emb_path='skipgram/embeddings.txt', emb_size=300, embeddings_dim=torch.Size([6185, 300]), enc_drop=0.0, epochs=100, eta_dropout=0.0, eta_hidden_size=200, eta_nlayers=3, eval_batch_size=1000, load_from='', log_interval=10, lr=0.005, lr_factor=4.0, min_df=100, mode='train', nonmono=10, num_docs_test=750, num_docs_test_1=750, num_docs_test_2=750, num_docs_train=6380, num_docs_valid=377, num_times=46, num_topics=30, num_words=20, optimizer='adam', rho_size=300, save_path='./results', seed=2019, t_hidden_size=800, tc=0, theta_act='relu', train_embeddings=1, visualize_every=1, vocab_size=6185, wdecay=1.2e-06)\n=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\n\nDETM architecture: DETM(\n  (t_drop): Dropout(p=0.0, inplace=False)\n  (theta_act): ReLU()\n  (rho): Linear(in_features=300, out_features=6185, bias=False)\n  (q_eta_map): Linear(in_features=6185, out_features=200, bias=True)\n  (q_eta): LSTM(200, 200, num_layers=3)\n  (mu_q_eta): Linear(in_features=230, out_features=30, bias=True)\n  (logsigma_q_eta): Linear(in_features=230, out_features=30, bias=True)\n  (q_theta): Sequential(\n    (0): Linear(in_features=6215, out_features=800, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=800, out_features=800, bias=True)\n    (3): ReLU()\n  )\n  (logsigma_q_theta): Linear(in_features=800, out_features=30, bias=True)\n  (mu_q_theta): Linear(in_features=800, out_features=30, bias=True)\n)\n****************************************************************************************************\nEpoch----->1 .. LR: 0.005 .. KL_theta: 300316.17 .. KL_eta: 116546.55 .. KL_alpha: 209270628.57 .. Rec_loss: 29889034.29 .. NELBO: 239576530.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 5063.8\n****************************************************************************************************\nval_ppl:  5063.8\n****************************************************************************************************\nEpoch----->2 .. LR: 0.005 .. KL_theta: 36326.21 .. KL_eta: 1702.1 .. KL_alpha: 201389234.29 .. Rec_loss: 28923529.43 .. NELBO: 230350795.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 4333.9\n****************************************************************************************************\nval_ppl:  4333.9\n****************************************************************************************************\nEpoch----->3 .. LR: 0.005 .. KL_theta: 47236.07 .. KL_eta: 897.24 .. KL_alpha: 193361606.86 .. Rec_loss: 28639768.57 .. NELBO: 222049510.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 4039.2\n****************************************************************************************************\nval_ppl:  4039.2\n****************************************************************************************************\nEpoch----->4 .. LR: 0.005 .. KL_theta: 53880.41 .. KL_eta: 1279.39 .. KL_alpha: 186028116.57 .. Rec_loss: 28682376.29 .. NELBO: 214765654.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3963.2\n****************************************************************************************************\nval_ppl:  3963.2\n****************************************************************************************************\nEpoch----->5 .. LR: 0.005 .. KL_theta: 52558.05 .. KL_eta: 1480.2 .. KL_alpha: 179127789.71 .. Rec_loss: 28475613.43 .. NELBO: 207657442.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3954.0\n****************************************************************************************************\nval_ppl:  3954.0\n****************************************************************************************************\nEpoch----->6 .. LR: 0.005 .. KL_theta: 54182.07 .. KL_eta: 1503.1 .. KL_alpha: 172525645.71 .. Rec_loss: 28480984.86 .. NELBO: 201062320.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3949.9\n****************************************************************************************************\nval_ppl:  3949.9\n****************************************************************************************************\nEpoch----->7 .. LR: 0.005 .. KL_theta: 51818.52 .. KL_eta: 1411.12 .. KL_alpha: 166448665.14 .. Rec_loss: 28492950.86 .. NELBO: 194994850.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3932.1\n****************************************************************************************************\nval_ppl:  3932.1\n****************************************************************************************************\nEpoch----->8 .. LR: 0.005 .. KL_theta: 52615.26 .. KL_eta: 1292.53 .. KL_alpha: 160591334.86 .. Rec_loss: 28386963.71 .. NELBO: 189032203.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3906.5\n****************************************************************************************************\nval_ppl:  3906.5\n****************************************************************************************************\nEpoch----->9 .. LR: 0.005 .. KL_theta: 51624.15 .. KL_eta: 1175.95 .. KL_alpha: 154812836.57 .. Rec_loss: 28523640.86 .. NELBO: 183389277.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3880.2\n****************************************************************************************************\nval_ppl:  3880.2\n****************************************************************************************************\nEpoch----->10 .. LR: 0.005 .. KL_theta: 51380.37 .. KL_eta: 1036.86 .. KL_alpha: 149295760.0 .. Rec_loss: 28436707.71 .. NELBO: 177784882.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3860.0\n****************************************************************************************************\nval_ppl:  3860.0\n****************************************************************************************************\nEpoch----->11 .. LR: 0.005 .. KL_theta: 50852.5 .. KL_eta: 924.8 .. KL_alpha: 144405504.0 .. Rec_loss: 28416600.29 .. NELBO: 172873883.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3832.8\n****************************************************************************************************\nval_ppl:  3832.8\n****************************************************************************************************\nEpoch----->12 .. LR: 0.005 .. KL_theta: 49900.49 .. KL_eta: 832.01 .. KL_alpha: 139324610.29 .. Rec_loss: 28425379.71 .. NELBO: 167800722.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3805.4\n****************************************************************************************************\nval_ppl:  3805.4\n****************************************************************************************************\nEpoch----->13 .. LR: 0.005 .. KL_theta: 49195.78 .. KL_eta: 752.26 .. KL_alpha: 134697537.14 .. Rec_loss: 28347285.71 .. NELBO: 163094768.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3775.0\n****************************************************************************************************\nval_ppl:  3775.0\n****************************************************************************************************\nEpoch----->14 .. LR: 0.005 .. KL_theta: 50424.34 .. KL_eta: 679.75 .. KL_alpha: 130259969.14 .. Rec_loss: 28384269.71 .. NELBO: 158695344.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3741.4\n****************************************************************************************************\nval_ppl:  3741.4\n****************************************************************************************************\nEpoch----->15 .. LR: 0.005 .. KL_theta: 49233.65 .. KL_eta: 665.44 .. KL_alpha: 126181691.43 .. Rec_loss: 28239692.29 .. NELBO: 154471284.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3704.3\n****************************************************************************************************\nval_ppl:  3704.3\n****************************************************************************************************\nEpoch----->16 .. LR: 0.005 .. KL_theta: 48889.57 .. KL_eta: 632.82 .. KL_alpha: 121963558.86 .. Rec_loss: 28296708.29 .. NELBO: 150309782.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3669.2\n****************************************************************************************************\nval_ppl:  3669.2\n****************************************************************************************************\nEpoch----->17 .. LR: 0.005 .. KL_theta: 47548.48 .. KL_eta: 620.64 .. KL_alpha: 118090924.57 .. Rec_loss: 28257796.29 .. NELBO: 146396893.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3634.9\n****************************************************************************************************\nval_ppl:  3634.9\n****************************************************************************************************\nEpoch----->18 .. LR: 0.005 .. KL_theta: 48855.33 .. KL_eta: 596.56 .. KL_alpha: 114635145.14 .. Rec_loss: 28261261.43 .. NELBO: 142945856.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3593.3\n****************************************************************************************************\nval_ppl:  3593.3\n****************************************************************************************************\nEpoch----->19 .. LR: 0.005 .. KL_theta: 48299.28 .. KL_eta: 599.15 .. KL_alpha: 111107681.14 .. Rec_loss: 28204195.14 .. NELBO: 139360770.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3554.4\n****************************************************************************************************\nval_ppl:  3554.4\n****************************************************************************************************\nEpoch----->20 .. LR: 0.005 .. KL_theta: 47039.86 .. KL_eta: 592.75 .. KL_alpha: 107702132.57 .. Rec_loss: 28184359.43 .. NELBO: 135934121.14\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3513.4\n****************************************************************************************************\nval_ppl:  3513.4\n****************************************************************************************************\nEpoch----->21 .. LR: 0.005 .. KL_theta: 46414.59 .. KL_eta: 587.39 .. KL_alpha: 104540961.14 .. Rec_loss: 28318521.14 .. NELBO: 132906488.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3470.3\n****************************************************************************************************\nval_ppl:  3470.3\n****************************************************************************************************\nEpoch----->22 .. LR: 0.005 .. KL_theta: 46151.68 .. KL_eta: 584.2 .. KL_alpha: 101429716.57 .. Rec_loss: 28151254.0 .. NELBO: 129627706.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3427.4\n****************************************************************************************************\nval_ppl:  3427.4\n****************************************************************************************************\nEpoch----->23 .. LR: 0.005 .. KL_theta: 45754.95 .. KL_eta: 585.96 .. KL_alpha: 98703790.86 .. Rec_loss: 28232033.43 .. NELBO: 126982164.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3382.2\n****************************************************************************************************\nval_ppl:  3382.2\n****************************************************************************************************\nEpoch----->24 .. LR: 0.005 .. KL_theta: 45739.94 .. KL_eta: 583.35 .. KL_alpha: 95825424.0 .. Rec_loss: 28164405.14 .. NELBO: 124036153.14\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3340.9\n****************************************************************************************************\nval_ppl:  3340.9\n****************************************************************************************************\nEpoch----->25 .. LR: 0.005 .. KL_theta: 42795.88 .. KL_eta: 591.89 .. KL_alpha: 92996469.71 .. Rec_loss: 28187636.86 .. NELBO: 121227492.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3301.0\n****************************************************************************************************\nval_ppl:  3301.0\n****************************************************************************************************\nEpoch----->26 .. LR: 0.005 .. KL_theta: 42756.95 .. KL_eta: 595.52 .. KL_alpha: 90557662.86 .. Rec_loss: 28196655.14 .. NELBO: 118797674.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3257.2\n****************************************************************************************************\nval_ppl:  3257.2\n****************************************************************************************************\nEpoch----->27 .. LR: 0.005 .. KL_theta: 43683.33 .. KL_eta: 605.5 .. KL_alpha: 88183470.86 .. Rec_loss: 28218346.29 .. NELBO: 116446106.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3219.6\n****************************************************************************************************\nval_ppl:  3219.6\n****************************************************************************************************\nEpoch----->28 .. LR: 0.005 .. KL_theta: 43563.03 .. KL_eta: 600.18 .. KL_alpha: 85774563.43 .. Rec_loss: 28160284.86 .. NELBO: 113979011.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3187.5\n****************************************************************************************************\nval_ppl:  3187.5\n****************************************************************************************************\nEpoch----->29 .. LR: 0.005 .. KL_theta: 40715.61 .. KL_eta: 590.85 .. KL_alpha: 83460393.14 .. Rec_loss: 28080768.29 .. NELBO: 111582469.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3152.7\n****************************************************************************************************\nval_ppl:  3152.7\n****************************************************************************************************\nEpoch----->30 .. LR: 0.005 .. KL_theta: 41831.14 .. KL_eta: 593.0 .. KL_alpha: 81421179.43 .. Rec_loss: 28016106.86 .. NELBO: 109479712.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3120.4\n****************************************************************************************************\nval_ppl:  3120.4\n****************************************************************************************************\nEpoch----->31 .. LR: 0.005 .. KL_theta: 38148.32 .. KL_eta: 595.65 .. KL_alpha: 79316190.86 .. Rec_loss: 27953392.0 .. NELBO: 107308325.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3088.7\n****************************************************************************************************\nval_ppl:  3088.7\n****************************************************************************************************\nEpoch----->32 .. LR: 0.005 .. KL_theta: 39457.21 .. KL_eta: 595.6 .. KL_alpha: 77279270.86 .. Rec_loss: 28021568.57 .. NELBO: 105340893.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3064.3\n****************************************************************************************************\nval_ppl:  3064.3\n****************************************************************************************************\nEpoch----->33 .. LR: 0.005 .. KL_theta: 38868.68 .. KL_eta: 596.13 .. KL_alpha: 75378561.14 .. Rec_loss: 27973980.86 .. NELBO: 103392009.14\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3035.0\n****************************************************************************************************\nval_ppl:  3035.0\n****************************************************************************************************\nEpoch----->34 .. LR: 0.005 .. KL_theta: 36628.61 .. KL_eta: 602.2 .. KL_alpha: 73511064.0 .. Rec_loss: 27991149.14 .. NELBO: 101539443.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 3014.0\n****************************************************************************************************\nval_ppl:  3014.0\n****************************************************************************************************\nEpoch----->35 .. LR: 0.005 .. KL_theta: 36500.12 .. KL_eta: 600.74 .. KL_alpha: 71817909.71 .. Rec_loss: 28011730.86 .. NELBO: 99866741.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2995.5\n****************************************************************************************************\nval_ppl:  2995.5\n****************************************************************************************************\nEpoch----->36 .. LR: 0.005 .. KL_theta: 36949.99 .. KL_eta: 603.42 .. KL_alpha: 70013990.86 .. Rec_loss: 27942036.57 .. NELBO: 97993581.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2979.5\n****************************************************************************************************\nval_ppl:  2979.5\n****************************************************************************************************\nEpoch----->37 .. LR: 0.005 .. KL_theta: 34781.95 .. KL_eta: 602.41 .. KL_alpha: 68430116.57 .. Rec_loss: 27915702.0 .. NELBO: 96381202.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2965.5\n****************************************************************************************************\nval_ppl:  2965.5\n****************************************************************************************************\nEpoch----->38 .. LR: 0.005 .. KL_theta: 34905.69 .. KL_eta: 587.14 .. KL_alpha: 66855968.0 .. Rec_loss: 27768546.29 .. NELBO: 94660004.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2953.4\n****************************************************************************************************\nval_ppl:  2953.4\n****************************************************************************************************\nEpoch----->39 .. LR: 0.005 .. KL_theta: 34778.37 .. KL_eta: 589.14 .. KL_alpha: 65246654.29 .. Rec_loss: 27811277.14 .. NELBO: 93093298.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2944.2\n****************************************************************************************************\nval_ppl:  2944.2\n****************************************************************************************************\nEpoch----->40 .. LR: 0.005 .. KL_theta: 32719.12 .. KL_eta: 601.54 .. KL_alpha: 63842410.86 .. Rec_loss: 27811219.14 .. NELBO: 91686949.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2935.0\n****************************************************************************************************\nval_ppl:  2935.0\n****************************************************************************************************\nEpoch----->41 .. LR: 0.005 .. KL_theta: 34204.03 .. KL_eta: 600.8 .. KL_alpha: 62391452.0 .. Rec_loss: 27769726.29 .. NELBO: 90195982.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2929.1\n****************************************************************************************************\nval_ppl:  2929.1\n****************************************************************************************************\nEpoch----->42 .. LR: 0.005 .. KL_theta: 33280.95 .. KL_eta: 606.5 .. KL_alpha: 61100180.0 .. Rec_loss: 27772317.14 .. NELBO: 88906386.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2920.6\n****************************************************************************************************\nval_ppl:  2920.6\n****************************************************************************************************\nEpoch----->43 .. LR: 0.005 .. KL_theta: 33121.98 .. KL_eta: 604.96 .. KL_alpha: 59825795.43 .. Rec_loss: 27801000.29 .. NELBO: 87660522.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2916.4\n****************************************************************************************************\nval_ppl:  2916.4\n****************************************************************************************************\nEpoch----->44 .. LR: 0.005 .. KL_theta: 31238.52 .. KL_eta: 603.59 .. KL_alpha: 58493666.86 .. Rec_loss: 27753618.29 .. NELBO: 86279128.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2913.1\n****************************************************************************************************\nval_ppl:  2913.1\n****************************************************************************************************\nEpoch----->45 .. LR: 0.005 .. KL_theta: 32101.38 .. KL_eta: 602.81 .. KL_alpha: 57243009.71 .. Rec_loss: 27783660.29 .. NELBO: 85059374.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2909.3\n****************************************************************************************************\nval_ppl:  2909.3\n****************************************************************************************************\nEpoch----->46 .. LR: 0.005 .. KL_theta: 32634.52 .. KL_eta: 607.02 .. KL_alpha: 56061932.57 .. Rec_loss: 27875868.86 .. NELBO: 83971043.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2906.8\n****************************************************************************************************\nval_ppl:  2906.8\n****************************************************************************************************\nEpoch----->47 .. LR: 0.005 .. KL_theta: 31345.0 .. KL_eta: 599.05 .. KL_alpha: 54917380.57 .. Rec_loss: 27781352.29 .. NELBO: 82730676.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2904.2\n****************************************************************************************************\nval_ppl:  2904.2\n****************************************************************************************************\nEpoch----->48 .. LR: 0.005 .. KL_theta: 32128.71 .. KL_eta: 604.44 .. KL_alpha: 53826365.14 .. Rec_loss: 27656350.57 .. NELBO: 81515450.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2900.9\n****************************************************************************************************\nval_ppl:  2900.9\n****************************************************************************************************\nEpoch----->49 .. LR: 0.005 .. KL_theta: 30230.32 .. KL_eta: 607.46 .. KL_alpha: 52786482.86 .. Rec_loss: 27675639.14 .. NELBO: 80492958.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2898.8\n****************************************************************************************************\nval_ppl:  2898.8\n****************************************************************************************************\nEpoch----->50 .. LR: 0.005 .. KL_theta: 31012.12 .. KL_eta: 610.61 .. KL_alpha: 51693809.14 .. Rec_loss: 27816015.71 .. NELBO: 79541445.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2897.6\n****************************************************************************************************\nval_ppl:  2897.6\n****************************************************************************************************\nEpoch----->51 .. LR: 0.005 .. KL_theta: 31540.73 .. KL_eta: 622.12 .. KL_alpha: 50717771.43 .. Rec_loss: 27725121.71 .. NELBO: 78475056.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2901.3\n****************************************************************************************************\nval_ppl:  2901.3\n****************************************************************************************************\nEpoch----->52 .. LR: 0.005 .. KL_theta: 31474.56 .. KL_eta: 613.23 .. KL_alpha: 49773606.29 .. Rec_loss: 27720466.29 .. NELBO: 77526161.14\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2894.7\n****************************************************************************************************\nval_ppl:  2894.7\n****************************************************************************************************\nEpoch----->53 .. LR: 0.005 .. KL_theta: 31305.08 .. KL_eta: 611.25 .. KL_alpha: 48808021.71 .. Rec_loss: 27614229.14 .. NELBO: 76454168.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2899.8\n****************************************************************************************************\nval_ppl:  2899.8\n****************************************************************************************************\nEpoch----->54 .. LR: 0.005 .. KL_theta: 30832.47 .. KL_eta: 617.54 .. KL_alpha: 47884860.57 .. Rec_loss: 27727710.29 .. NELBO: 75644021.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2894.3\n****************************************************************************************************\nval_ppl:  2894.3\n****************************************************************************************************\nEpoch----->55 .. LR: 0.005 .. KL_theta: 32317.47 .. KL_eta: 624.19 .. KL_alpha: 47031397.71 .. Rec_loss: 27648672.86 .. NELBO: 74713012.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2890.5\n****************************************************************************************************\nval_ppl:  2890.5\n****************************************************************************************************\nEpoch----->56 .. LR: 0.005 .. KL_theta: 31227.82 .. KL_eta: 630.24 .. KL_alpha: 46158815.43 .. Rec_loss: 27709814.57 .. NELBO: 73900486.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2892.1\n****************************************************************************************************\nval_ppl:  2892.1\n****************************************************************************************************\nEpoch----->57 .. LR: 0.005 .. KL_theta: 30139.18 .. KL_eta: 625.31 .. KL_alpha: 45293624.57 .. Rec_loss: 27633595.71 .. NELBO: 72957986.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2887.1\n****************************************************************************************************\nval_ppl:  2887.1\n****************************************************************************************************\nEpoch----->58 .. LR: 0.005 .. KL_theta: 31973.91 .. KL_eta: 623.0 .. KL_alpha: 44509246.29 .. Rec_loss: 27794835.43 .. NELBO: 72336678.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2884.7\n****************************************************************************************************\nval_ppl:  2884.7\n****************************************************************************************************\nEpoch----->59 .. LR: 0.005 .. KL_theta: 31369.18 .. KL_eta: 640.4 .. KL_alpha: 43647668.57 .. Rec_loss: 27655476.0 .. NELBO: 71335153.14\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2886.7\n****************************************************************************************************\nval_ppl:  2886.7\n****************************************************************************************************\nEpoch----->60 .. LR: 0.005 .. KL_theta: 32031.73 .. KL_eta: 644.71 .. KL_alpha: 42926411.43 .. Rec_loss: 27741101.43 .. NELBO: 70700189.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2892.1\n****************************************************************************************************\nval_ppl:  2892.1\n****************************************************************************************************\nEpoch----->61 .. LR: 0.005 .. KL_theta: 33366.12 .. KL_eta: 625.56 .. KL_alpha: 42235964.57 .. Rec_loss: 27797152.29 .. NELBO: 70067109.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2886.4\n****************************************************************************************************\nval_ppl:  2886.4\n****************************************************************************************************\nEpoch----->62 .. LR: 0.005 .. KL_theta: 31077.4 .. KL_eta: 630.4 .. KL_alpha: 41478469.14 .. Rec_loss: 27654219.43 .. NELBO: 69164397.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2881.3\n****************************************************************************************************\nval_ppl:  2881.3\n****************************************************************************************************\nEpoch----->63 .. LR: 0.005 .. KL_theta: 32379.69 .. KL_eta: 630.96 .. KL_alpha: 40779143.43 .. Rec_loss: 27624363.71 .. NELBO: 68436516.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2880.9\n****************************************************************************************************\nval_ppl:  2880.9\n****************************************************************************************************\nEpoch----->64 .. LR: 0.005 .. KL_theta: 33047.81 .. KL_eta: 639.0 .. KL_alpha: 40115688.0 .. Rec_loss: 27666595.14 .. NELBO: 67815972.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2880.2\n****************************************************************************************************\nval_ppl:  2880.2\n****************************************************************************************************\nEpoch----->65 .. LR: 0.005 .. KL_theta: 33654.95 .. KL_eta: 653.8 .. KL_alpha: 39453400.0 .. Rec_loss: 27630542.57 .. NELBO: 67118251.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2874.5\n****************************************************************************************************\nval_ppl:  2874.5\n****************************************************************************************************\nEpoch----->66 .. LR: 0.005 .. KL_theta: 33415.62 .. KL_eta: 641.4 .. KL_alpha: 38837754.86 .. Rec_loss: 27684604.0 .. NELBO: 66556415.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2875.1\n****************************************************************************************************\nval_ppl:  2875.1\n****************************************************************************************************\nEpoch----->67 .. LR: 0.005 .. KL_theta: 33459.83 .. KL_eta: 648.54 .. KL_alpha: 38193298.29 .. Rec_loss: 27677940.57 .. NELBO: 65905347.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2869.2\n****************************************************************************************************\nval_ppl:  2869.2\n****************************************************************************************************\nEpoch----->68 .. LR: 0.005 .. KL_theta: 34176.7 .. KL_eta: 642.2 .. KL_alpha: 37562823.43 .. Rec_loss: 27562614.0 .. NELBO: 65160256.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2871.2\n****************************************************************************************************\nval_ppl:  2871.2\n****************************************************************************************************\nEpoch----->69 .. LR: 0.005 .. KL_theta: 34701.76 .. KL_eta: 641.67 .. KL_alpha: 36957026.86 .. Rec_loss: 27756320.86 .. NELBO: 64748692.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2870.6\n****************************************************************************************************\nval_ppl:  2870.6\n****************************************************************************************************\nEpoch----->70 .. LR: 0.005 .. KL_theta: 33945.68 .. KL_eta: 647.64 .. KL_alpha: 36321649.71 .. Rec_loss: 27745026.86 .. NELBO: 64101269.14\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2864.9\n****************************************************************************************************\nval_ppl:  2864.9\n****************************************************************************************************\nEpoch----->71 .. LR: 0.005 .. KL_theta: 33646.95 .. KL_eta: 647.04 .. KL_alpha: 35852628.0 .. Rec_loss: 27692912.0 .. NELBO: 63579834.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2868.4\n****************************************************************************************************\nval_ppl:  2868.4\n****************************************************************************************************\nEpoch----->72 .. LR: 0.005 .. KL_theta: 34271.12 .. KL_eta: 662.11 .. KL_alpha: 35269183.43 .. Rec_loss: 27649582.29 .. NELBO: 62953699.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2866.2\n****************************************************************************************************\nval_ppl:  2866.2\n****************************************************************************************************\nEpoch----->73 .. LR: 0.005 .. KL_theta: 36444.5 .. KL_eta: 650.82 .. KL_alpha: 34708244.57 .. Rec_loss: 27584802.0 .. NELBO: 62330141.14\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2862.4\n****************************************************************************************************\nval_ppl:  2862.4\n****************************************************************************************************\nEpoch----->74 .. LR: 0.005 .. KL_theta: 34992.76 .. KL_eta: 650.28 .. KL_alpha: 34189035.43 .. Rec_loss: 27634298.86 .. NELBO: 61858976.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2855.7\n****************************************************************************************************\nval_ppl:  2855.7\n****************************************************************************************************\nEpoch----->75 .. LR: 0.005 .. KL_theta: 36840.54 .. KL_eta: 661.41 .. KL_alpha: 33657304.57 .. Rec_loss: 27632464.0 .. NELBO: 61327270.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2861.5\n****************************************************************************************************\nval_ppl:  2861.5\n****************************************************************************************************\nEpoch----->76 .. LR: 0.005 .. KL_theta: 35816.24 .. KL_eta: 647.48 .. KL_alpha: 33182891.43 .. Rec_loss: 27664579.71 .. NELBO: 60883934.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2862.0\n****************************************************************************************************\nval_ppl:  2862.0\n****************************************************************************************************\nEpoch----->77 .. LR: 0.005 .. KL_theta: 38980.37 .. KL_eta: 660.26 .. KL_alpha: 32684271.14 .. Rec_loss: 27689200.86 .. NELBO: 60413112.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2857.5\n****************************************************************************************************\nval_ppl:  2857.5\n****************************************************************************************************\nEpoch----->78 .. LR: 0.005 .. KL_theta: 35559.14 .. KL_eta: 666.12 .. KL_alpha: 32186523.43 .. Rec_loss: 27634316.57 .. NELBO: 59857065.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2847.7\n****************************************************************************************************\nval_ppl:  2847.7\n****************************************************************************************************\nEpoch----->79 .. LR: 0.005 .. KL_theta: 37534.77 .. KL_eta: 662.99 .. KL_alpha: 31767813.71 .. Rec_loss: 27557254.29 .. NELBO: 59363264.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2845.9\n****************************************************************************************************\nval_ppl:  2845.9\n****************************************************************************************************\nEpoch----->80 .. LR: 0.005 .. KL_theta: 35863.57 .. KL_eta: 666.28 .. KL_alpha: 31288835.14 .. Rec_loss: 27583533.71 .. NELBO: 58908898.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2844.0\n****************************************************************************************************\nval_ppl:  2844.0\n****************************************************************************************************\nEpoch----->81 .. LR: 0.005 .. KL_theta: 39324.95 .. KL_eta: 672.09 .. KL_alpha: 30855479.14 .. Rec_loss: 27600549.14 .. NELBO: 58496026.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2849.3\n****************************************************************************************************\nval_ppl:  2849.3\n****************************************************************************************************\nEpoch----->82 .. LR: 0.005 .. KL_theta: 36301.95 .. KL_eta: 661.73 .. KL_alpha: 30423433.14 .. Rec_loss: 27600077.71 .. NELBO: 58060474.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2842.1\n****************************************************************************************************\nval_ppl:  2842.1\n****************************************************************************************************\nEpoch----->83 .. LR: 0.005 .. KL_theta: 40959.73 .. KL_eta: 675.33 .. KL_alpha: 29967013.43 .. Rec_loss: 27541937.14 .. NELBO: 57550583.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2837.5\n****************************************************************************************************\nval_ppl:  2837.5\n****************************************************************************************************\nEpoch----->84 .. LR: 0.005 .. KL_theta: 37010.01 .. KL_eta: 667.52 .. KL_alpha: 29523697.43 .. Rec_loss: 27470757.71 .. NELBO: 57032133.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2835.6\n****************************************************************************************************\nval_ppl:  2835.6\n****************************************************************************************************\nEpoch----->85 .. LR: 0.005 .. KL_theta: 41159.65 .. KL_eta: 659.17 .. KL_alpha: 29184822.0 .. Rec_loss: 27511765.14 .. NELBO: 56738406.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2829.1\n****************************************************************************************************\nval_ppl:  2829.1\n****************************************************************************************************\nEpoch----->86 .. LR: 0.005 .. KL_theta: 38897.71 .. KL_eta: 672.08 .. KL_alpha: 28740204.57 .. Rec_loss: 27547337.43 .. NELBO: 56327111.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2831.3\n****************************************************************************************************\nval_ppl:  2831.3\n****************************************************************************************************\nEpoch----->87 .. LR: 0.005 .. KL_theta: 43836.72 .. KL_eta: 691.77 .. KL_alpha: 28351454.57 .. Rec_loss: 27643967.43 .. NELBO: 56039950.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2825.0\n****************************************************************************************************\nval_ppl:  2825.0\n****************************************************************************************************\nEpoch----->88 .. LR: 0.005 .. KL_theta: 40203.62 .. KL_eta: 690.63 .. KL_alpha: 27989025.14 .. Rec_loss: 27570275.71 .. NELBO: 55600195.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2826.7\n****************************************************************************************************\nval_ppl:  2826.7\n****************************************************************************************************\nEpoch----->89 .. LR: 0.005 .. KL_theta: 40439.89 .. KL_eta: 679.67 .. KL_alpha: 27595946.57 .. Rec_loss: 27639699.43 .. NELBO: 55276766.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2817.3\n****************************************************************************************************\nval_ppl:  2817.3\n****************************************************************************************************\nEpoch----->90 .. LR: 0.005 .. KL_theta: 43119.2 .. KL_eta: 687.05 .. KL_alpha: 27229768.0 .. Rec_loss: 27544782.29 .. NELBO: 54818356.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2818.0\n****************************************************************************************************\nval_ppl:  2818.0\n****************************************************************************************************\nEpoch----->91 .. LR: 0.005 .. KL_theta: 40942.01 .. KL_eta: 673.86 .. KL_alpha: 26920483.14 .. Rec_loss: 27623826.86 .. NELBO: 54585925.14\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2810.8\n****************************************************************************************************\nval_ppl:  2810.8\n****************************************************************************************************\nEpoch----->92 .. LR: 0.005 .. KL_theta: 42425.43 .. KL_eta: 684.31 .. KL_alpha: 26530520.0 .. Rec_loss: 27605938.86 .. NELBO: 54179568.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2811.9\n****************************************************************************************************\nval_ppl:  2811.9\n****************************************************************************************************\nEpoch----->93 .. LR: 0.005 .. KL_theta: 42340.62 .. KL_eta: 691.43 .. KL_alpha: 26178984.57 .. Rec_loss: 27638046.29 .. NELBO: 53860061.71\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2803.4\n****************************************************************************************************\nval_ppl:  2803.4\n****************************************************************************************************\nEpoch----->94 .. LR: 0.005 .. KL_theta: 44293.57 .. KL_eta: 706.0 .. KL_alpha: 25842495.71 .. Rec_loss: 27524563.43 .. NELBO: 53412058.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2811.5\n****************************************************************************************************\nval_ppl:  2811.5\n****************************************************************************************************\nEpoch----->95 .. LR: 0.005 .. KL_theta: 42646.81 .. KL_eta: 687.29 .. KL_alpha: 25506417.14 .. Rec_loss: 27589173.43 .. NELBO: 53138924.57\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2800.1\n****************************************************************************************************\nval_ppl:  2800.1\n****************************************************************************************************\nEpoch----->96 .. LR: 0.005 .. KL_theta: 43798.45 .. KL_eta: 704.44 .. KL_alpha: 25164734.86 .. Rec_loss: 27515414.0 .. NELBO: 52724651.43\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2806.4\n****************************************************************************************************\nval_ppl:  2806.4\n****************************************************************************************************\nEpoch----->97 .. LR: 0.005 .. KL_theta: 43463.34 .. KL_eta: 695.97 .. KL_alpha: 24870416.86 .. Rec_loss: 27531979.43 .. NELBO: 52446554.86\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2801.7\n****************************************************************************************************\nval_ppl:  2801.7\n****************************************************************************************************\nEpoch----->98 .. LR: 0.005 .. KL_theta: 47123.29 .. KL_eta: 698.21 .. KL_alpha: 24586153.43 .. Rec_loss: 27563387.43 .. NELBO: 52197362.29\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2798.6\n****************************************************************************************************\nval_ppl:  2798.6\n****************************************************************************************************\nEpoch----->99 .. LR: 0.005 .. KL_theta: 43952.59 .. KL_eta: 697.58 .. KL_alpha: 24229405.43 .. Rec_loss: 27509017.43 .. NELBO: 51783072.0\n****************************************************************************************************\n****************************************************************************************************\nVAL PPL: 2793.0\n****************************************************************************************************\nval_ppl:  2793.0\nsaving topic matrix beta...\nsaving word embedding matrix rho...\ncomputing validation perplexity...\n****************************************************************************************************\nVAL PPL: 2793.0\n****************************************************************************************************\ncomputing test perplexity...\n****************************************************************************************************\nTEST Doc Completion PPL: 2842.1\n****************************************************************************************************\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport torch \nimport numpy as np\nimport bokeh.plotting as bp\n\ndef get_df(data, wi, wj=None):\n    \"\"\"\n    Obtain the document frequency\n    :param data: document vocabulary matrix\n    :param wi: word index w_i\n    :param wj: word index w_j\n    :return: document frequency for word w_i , w_i ∩ w_j\n    \"\"\"\n    if wj is None:\n        return torch.where(data[:, wi] > 0, 1, 0).sum(-1)\n    else:\n        df_wi = torch.where(data[:, wi] > 0, 1, 0)\n        df_wj = torch.where(data[:, wj] > 0, 1, 0)\n        return df_wj.sum(-1), (df_wi & df_wj).sum(-1)\n\n\ndef get_topic_coherence(beta, data):\n    D = torch.tensor(len(data)) ## number of docs...data is list of documents\n    TC = []\n    num_topics = len(beta)\n    counter = 0\n    for k in range(num_topics):\n        top_10 = list(torch.flip(beta[k].argsort()[-11:],[0]))\n        TC_k = 0\n        counter = 0\n        for i, word in enumerate(top_10):\n            D_wi = get_df(data, word)\n            j = i + 1\n            tmp = 0\n            while j < len(top_10) and j > i:\n                D_wj, D_wi_wj = get_df(data, word, top_10[j])\n                if D_wi_wj == 0:\n                    f_wi_wj = -1\n                else:\n                    f_wi_wj = -1 + (torch.log(D_wi)+torch.log(D_wj)-2.0*torch.log(D))/(torch.log(D_wi_wj)-torch.log(D))\n                tmp += f_wi_wj\n                j += 1\n                counter += 1\n            TC_k += tmp \n        TC.append(TC_k.detach().cpu().numpy())\n    TC = np.mean(TC) / counter\n    #print('Topic coherence is: {}'.format(TC))\n    return TC\n\n\ndef visualize(docs, _lda_keys, topics, theta):\n    tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n    # project to 2D\n    tsne_lda = tsne_model.fit_transform(theta)\n    colormap = []\n    for name, hex in matplotlib.colors.cnames.items():\n        colormap.append(hex)\n\n    colormap = colormap[:len(theta[0, :])]\n    colormap = np.array(colormap)\n\n    title = '20 newsgroups TE embedding V viz'\n    num_example = len(docs)\n\n    plot_lda = bp.figure(plot_width=1400, plot_height=1100,\n                     title=title,\n                     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n                     x_axis_type=None, y_axis_type=None, min_border=1)\n\n    plt.scatter(x=tsne_lda[:, 0], y=tsne_lda[:, 1],\n                 color=colormap[_lda_keys][:num_example])\n    plt.show()\n\n    \ndef get_rnn_input(dataloader, num_times, vocab_size):\n    # TxV\n    rnn_input = torch.zeros(num_times, vocab_size).to(device)\n    # times count\n    cnt = torch.zeros(num_times, ).to(device)\n    # create data loader\n    for data in dataloader:\n        data_batch = cvz[data['index'].long() - 1,:].to(device)\n        times_batch = ts[data['index'].long() - 1].to(device)\n        for t in range(num_times):\n            # check times\n            tmp = (times_batch == t).nonzero()\n            # sum the vocabulary in time t\n            docs = data_batch[tmp].squeeze().sum(0)\n            # feed in the vocabulary count in time t\n            rnn_input[t] += docs\n            # sum up the count\n            cnt[t] += tmp.shape[0]\n            # check the epoch\n    rnn_input = rnn_input / cnt.unsqueeze(1)\n    return rnn_input","metadata":{"execution":{"iopub.status.busy":"2021-12-25T10:19:38.454376Z","iopub.execute_input":"2021-12-25T10:19:38.454764Z","iopub.status.idle":"2021-12-25T10:19:38.484004Z","shell.execute_reply.started":"2021-12-25T10:19:38.454711Z","shell.execute_reply":"2021-12-25T10:19:38.482655Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    alpha = model.mu_q_alpha\n    beta = model.get_beta(alpha)\n    \nbeta = beta.permute(1,0,2)","metadata":{"execution":{"iopub.status.busy":"2021-12-25T10:19:37.470491Z","iopub.execute_input":"2021-12-25T10:19:37.470878Z","iopub.status.idle":"2021-12-25T10:19:37.526830Z","shell.execute_reply.started":"2021-12-25T10:19:37.470826Z","shell.execute_reply":"2021-12-25T10:19:37.525698Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"ds = torch.from_numpy(np.array(\n    get_batch(train_tokens, train_counts, range(train_tokens.shape[0]), args.vocab_size, temporal=True, times=train_times)[0]\n))","metadata":{"execution":{"iopub.status.busy":"2021-12-25T10:19:37.963104Z","iopub.execute_input":"2021-12-25T10:19:37.963497Z","iopub.status.idle":"2021-12-25T10:19:38.423230Z","shell.execute_reply.started":"2021-12-25T10:19:37.963399Z","shell.execute_reply":"2021-12-25T10:19:38.422162Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"ds.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-25T10:24:06.249133Z","iopub.execute_input":"2021-12-25T10:24:06.249449Z","iopub.status.idle":"2021-12-25T10:24:06.256388Z","shell.execute_reply.started":"2021-12-25T10:24:06.249415Z","shell.execute_reply":"2021-12-25T10:24:06.255298Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"torch.Size([6380, 6185])"},"metadata":{}}]},{"cell_type":"code","source":"vocab = cvectorizer.vocabulary_\ncvz = cvz.todense()","metadata":{"execution":{"iopub.status.busy":"2021-12-25T10:18:57.459589Z","iopub.execute_input":"2021-12-25T10:18:57.459896Z","iopub.status.idle":"2021-12-25T10:18:57.695251Z","shell.execute_reply.started":"2021-12-25T10:18:57.459859Z","shell.execute_reply":"2021-12-25T10:18:57.694263Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# KxTxL\n# alpha = model.mu_q_alpha.clone().contiguous()\n# alpha = alpha.permute(1,0,2)\n# #alpha = model.get_alpha()[0]\n# beta = model.get_beta(alpha,torch.arange(0,ts.unique().shape[0]))\n\ncnt = 0\ntc = 0\nfor time in range(0,beta.shape[0]):\n    beta_t = beta[time,:,:]\n    cnt+=1\n    tc+=get_topic_coherence(beta_t, ds)\n\ntc/=cnt\nprint(f'tc: {tc}')\n\n#print_top_words(beta[:,:,:-3],vocab)\ndef _diversity_helper(beta, num_tops):\n    list_w = torch.zeros((int(beta.shape[0]), num_tops))\n    for k in range(int(beta.shape[0])):\n        gamma = beta[k, :]\n        top_words = gamma.argsort()[-num_tops:]\n        list_w[k, :] = top_words\n    list_w = list_w.reshape(-1)\n    n_unique = len(list_w.unique())\n    diversity = n_unique / (beta.shape[0] * num_tops)\n    return diversity\n\ntd = 0\nfor t in range(beta.shape[0]):\n    d=_diversity_helper(beta[t],25)\n    td+=d\n    print(d)\nprint(f'TD: {td/beta.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2021-12-25T10:19:38.485880Z","iopub.execute_input":"2021-12-25T10:19:38.486327Z","iopub.status.idle":"2021-12-25T10:20:11.178479Z","shell.execute_reply.started":"2021-12-25T10:19:38.486211Z","shell.execute_reply":"2021-12-25T10:20:11.177085Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"tc: 0.07648717659735395\n0.49733333333333335\n0.488\n0.464\n0.472\n0.45866666666666667\n0.472\n0.4693333333333333\n0.4533333333333333\n0.45466666666666666\n0.44533333333333336\n0.43066666666666664\n0.41733333333333333\n0.4146666666666667\n0.4226666666666667\n0.41733333333333333\n0.42133333333333334\n0.4053333333333333\n0.39066666666666666\n0.416\n0.4146666666666667\n0.4186666666666667\n0.412\n0.392\n0.38133333333333336\n0.392\n0.396\n0.43066666666666664\n0.43733333333333335\n0.44533333333333336\n0.4533333333333333\n0.44533333333333336\n0.42533333333333334\n0.4093333333333333\n0.4026666666666667\n0.4013333333333333\n0.39066666666666666\n0.3933333333333333\n0.4026666666666667\n0.4\n0.396\n0.3893333333333333\n0.3893333333333333\n0.41733333333333333\n0.436\n0.456\n0.496\nTD: 0.4268405797101448\n","output_type":"stream"}]}]}