\section{Introduction}\label{A}
Today, myriads of terabyte data is one of the crucial challenge for scientific researcher.  Information Retrieval become increasingly important for building useful information from massive data set. Specifically, topic modeling is one of the most popular technique for extracting key point ideas and exploring documents. Specifically, correlated topic models (CTM) make use of the correlation between topics and deliver a better result. In the research, we would like to explore the application one of the topic modeling techniques and try to improve their performance. In section \ref{AA}, we provide a brief introduction to the existing algorithm and covers the background of it. Then, section \ref{AB} will discuss the current application and state-of-art improvement on topic modeling advances. Following that, section \ref{AC} will elaborate the conduct the research and the general direction. And section \ref{AD} will explain the way the proposal model to be evaluated and compare to the existing model. Finally, section \ref{AE} will make a summary for the proposal so far.
\subsection{Motivation}
Topic modeling is one of the most exciting domain in Information Retrieval (IR). It can be extended to accomplish versatile range of IR and data mining tasks. For instance, one of the topic model: Latent Dirichlet Allocation (LDA), was proposed and examined its capability on extracting latent topics and output keywords suggestion for each topic. As result, LDA has been implemented into varies area of applications. However, There are several problems that LDA could not handle well.
\paragraph{Computational complexity}
Generally, LDA acquires to computer the posterior distribution for inference, which is relatively expensive to obtain an exact solution. In the same way, its variant, correlated topic model (CTM) requires to calculate the covariance matrix specifically, which makes it not feasible come into practical application.
\paragraph{Statistical Laws}In particular, LDA does not take empirical statistical laws observed in text into account. For example, LDA's prior does not dependent on Zipf's Law or Heap's Law, which may not collaborate well with natural document text data. Similarly, Moody\cite{moody_mixing_2016} proposed lda2vec, which exploit the meta-information of each document and evaluate their correlation between documents.
\subsection{Applications}
In this research, we would like to conduct a investigation on the existing correlated topic modeling techniques and . It is expected our result will benefit varies application areas as followings 
\paragraph{Feature Extraction}\label{AAA} For number of n topics, LDA can accomplish the task cluster them and extract a set of corpus with k terms which can represent each topic most and uniquely. Eren \cite{eren_covid-19_2020} uses LDA to analysis all literature related to COVID-19 and subdivided them into minor topics. As result, each subtopic were extracted with a set of keywords.
\paragraph{Text Classification}Topic model can also treated to deal with classification task to identify unseen data. Kim \cite{kim_multi-co-training_2019} adapted the semi-supervised method with multi-co-training method to improve the overall classification performance. Moreover, the paper extended Word2Vec to Doc2Vec which maintain semantic relationship between two paragraphs. Doc2Vec transforms a paragraph into a d-dimensional vectors, which put documents with similar paragraph into near vector space.
\paragraph{Recommender Systems} LDA often can be applied to recommendations. Xu\cite{xu_uis-lda_2017} employed UIS-LDA (A User Recommendation based on Social Connections and Interests of Users in Uni-Directional Social Networks), which utilizes Generative Polya Urn (GPU) model and perform prediction for nearest user for the recommendations. Wang \cite{wang_st-sage_2017} implemented a LDA version which utilize the twitter datasets and recommend a serial of tourist location to user.

Topic model are one of the crucial tasks in discovering hidden topic from document collections. It has been applied to many tasks. 
Topic modeling is one of the most common NLP tasks in current days. Classic models, for example, Probabilistic Latent Semantic Analysis (PLSA), Nonnegative Matrix Factorization (NMF) and Latent Dirichlet Allocation (LDA). These topic models take bag-of-word assumption and model each document as an admixture of latent topics, which are multinomial distributions over words.
Moreover, in the growth of word embedding [2013] enables an effective way to capture semantic meaning in language in a continuous vector space. Vocabularies that have similar meaning are close together by Euclidean distance. 
In the ascendant of powerful accelerator such as GPU and more memory, we are able to build more complex architecture and boost up computation time. Specifically, Transformer has been one of the most used NLP architecture. Number of variants have been built due to its success, BERT, ROBERTA, ELMO. 
However, LDA faces a number of drawbacks. Since it takes bag-of-word assumption, it cannot capture positional information from document. This limited the ability the model relates close word into same topic.
