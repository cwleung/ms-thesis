Topic model are one of the crucial tasks in discovering hidden topic from document collections. It has been applied to many tasks. 
Topic modeling is one of the most common NLP tasks in current days. Classic models, for example, Probabilistic Latent Semantic Analysis (PLSA), Nonnegative Matrix Factorization (NMF) and Latent Dirichlet Allocation (LDA). These topic models take bag-of-word assumption and model each document as an admixture of latent topics, which are multinomial distributions over words.
Moreover, in the growth of word embedding [2013] enables an effective way to capture semantic meaning in language in a continuous vector space. Vocabularies that have similar meaning are close together by Euclidean distance. 
In the ascendant of powerful accelerator such as GPU and more memory, we are able to build more complex architecture and boost up computation time. Specifically, Transformer has been one of the most used NLP architecture. Number of variants have been built due to its success, BERT, ROBERTA, ELMO. 
However, LDA faces a number of drawbacks. Since it takes bag-of-word assumption, it cannot capture positional information from document. This limited the ability the model relates close word into same topic.
