In this chapter, we give a detailed explanation and procedure of LKJTM to be implemented.
\section{Model description}
The TETM utilizes the Transformer as embedding to the topic-word representations. To compare with the original topic model, the word-topic distribution $ \beta $ is the 
First, the topic embedding embeds the vocabulary into L-dimensional space, which is by Transformer embedding. Second, the context embedding maps the embedding into K-dimensional space . 
In the generative process, the LKJTM uses the topic embedding to form a per-topic vector to represent the meaning over the vocabulary. 
% Generative process
The generative process of the $ d^{th} $ document is the following:\\
\begin{algorithm}[H]
\For{document d in D}{
Sample topic distribution $ \theta_d\sim \mathcal{LN}(0,I) $\\
\For{word position n in $ N_d $}{
Sample word $ w_{d,n}\sim \sigma(\theta_d(\rho^\top\alpha)_{\cdot,w_{d,n}}) $
}
}
\caption{Generative Process for TETM}
\label{algorithm:lkjtm}
\end{algorithm}
From algorithm \ref{algorithm:lkjtm}, starting from step 1, the topic proportion $ \theta_d $ is drawn from the logistic-normal distribution $ \mathcal{LN}(\cdot) $ with zero mean and identical covariance.
From Step 2-a, for each word position $ n $ in document $ d $, a topic assignment to word $ w_{dn} $ is drawn from categorical distribution $ Cat(\theta_d) $ parameterized by topic proportion $ \theta_d $
Step 2-b, the model draw a word from embedding of the vocabulary $ \rho $ and the assigned topic embedding $ \alpha_{z_{dn}} $ to draw the observed word from the assigned topic, as given by $ z_{dn} $. The embedding is applied softmax function to make them topic distribution.
The TETM likelihood uses a matrix of word embedding $ \rho $, a representation of the vocabulary in a lower dimensional space. In practice, it can either rely on previously fitted embeddings as part of the fitting procedure, it simultaneously finds topics and an embedding space.

% TODO LKJ Correlation Distribution Formulation

% TODO Transformer Embeddings
% Transformer embeddings
\subsection{Transformer Embeddings}
Following the ETM architecture, we modify topic-word distribution as an embedding and put transformer embedding to work into it. 
\begin{equation}\label{eq:transformer_embedding}
\beta\sim\text{softmax}(\rho^\top\alpha)
\end{equation}
Equation \ref{eq:transformer_embedding}, 
%% Joint Distribution
\subsection{Joint Distribution}
We give a description of the joint distribution, $ W,Z,\theta $ and $ \Sigma $ are variables and $ \beta, \mu $ and $ \gamma $ are latent variables. $ W $ is the word likelihood from the document collections, $ Z $ represents the topic-word assignment, $ \theta $ models the topic distribution for each document, and $ \Sigma $ is the covariance matrix which depends on the document-topic distribution $ \theta $.
\begin{align*}
p(w,\theta,\alpha)=\prod_{d=1}^{D}\int p(\theta_d)\prod_{i=1}^{N_d}p(w|\theta_d,\alpha)d\theta_d
\end{align*}
by taking  log on the joint probability, we obtain a objective function for optimization
% Marginal Likelihood
\subsection{Marginal Likelihood}
The parameter $ \alpha $ is the topic embedding on the word embedding space of dimension K. We take a log marginal likelihood of the document,
\begin{align*}
\mathcal{L}(\alpha)=\sum_{d=1}^{D}\log p(w_d|\alpha)
\end{align*}
\begin{align*}
p(w_d|\alpha)=\int p(\theta_d|\mu,\Sigma)\prod_{n=1}^{N_d}p(w_{dn}|\theta_d,\alpha)d\theta_d
\end{align*}
the conditional distribution $ p(w_{dn}|\theta_d,\alpha) $ marginalize out the the topic assignment $ z_{dn} $,
\begin{align*}
p(w_{dn}|\theta_d,\alpha)=\sum_{k=1}^{K}\theta_{dk}\beta_{k,w_{dn}}
\end{align*}
where $ \beta $ represent the topic-word distribution, composed of transformer embedding $ \rho $ and topic embedding $ \alpha $, in such case
\begin{align*}
\beta_{kv}=\text{softmax}(\rho^\top\alpha)_{v}.
\end{align*}
then we take the amortized inference to take out neural network for the representation,
\begin{align*}
p(w_{d}|N(\mu_\theta(w),\sigma_\theta(w)),\beta)=\mathbb{E}_{\theta\sim N(\mu_\theta(w),\sigma_\theta(w))}\left[w^\top\sigma(\beta\theta)\right]
\end{align*}
Reparametrization Trick $ \theta=\mu+\sigma^{1/2}\epsilon $\\
\begin{align*}
p(w_{d}|\mu_\theta(w)+\epsilon\sigma^{1/2}_\theta(w),\beta)=\mathbb{E}_{\epsilon\sim N(0,1)}\left[w_d^\top\sigma(\beta(\mu_\theta(w)+\epsilon\sigma^{1/2}_\theta(w)))\right]
\end{align*}