In this section, we give short description on topic model techniques and several key components and terminologies related to LDA model. In section \ref{ch2:tm}, we formulate the problem description of topic modeling. In \ref{ch2:we}, we give description to word embedding. In section \ref{ch2:ctm}, we explain the algorithm of Correlated topic model(CTM). Then,  section \ref{ch2:lkj} will give the definition for LKJ correlation distribution. Section \ref{ch2:etm} will cover the Embedded Topic Model(ETM). Finally, section \ref{ch2:transformer} will give introduction to transformer embedding that we are used in the model.
\section{Topic Modeling} \label{ch2:tm} Several topic modeling techniques include non-negative matrix factorization (NMF) \cite{lee_learning_1999}, Latent Semantic Analysis \cite{landauer_solution_1997}, probabilistic Latent Semantic Analysis (pLSA) \cite{hofmann_probabilistic_2013} and Latent Dirichlet Allocation (LDA)\cite{blei_latent_2003}.

\section{Word Embedding} \label{ch2:we}
% explain what is word embeddings
Word embedding\cite{bengio_neural_nodate} is a kind of representation for words from document collections using a vector formulation. The nature of word embedding is that, the words that having similar meaning have a close distance(in most case euclidean distance), and vice versa. For instance, continuous bag-of-words(CBOW) \cite{mikolov_distributed_nodate} is a kind of word embeddings converting bag-of-word in to a vector of n-dimension continuous space, which contains the following formulation
\begin{align*}
w\sim\text{softmax}(\rho^\top\alpha)
\end{align*}
where $ \rho $ is the embedding matrix $ L\times V $  which maps $ V $ vocabularies into $ L $ dimension of continuous vector space. And $ \alpha $ is the context embedding, 
\section{Correlated Topic Model} \label{ch2:ctm}
Correlated Topic Model (CTM)\cite{blei_correlated_2007} is an extension of LDA\cite{blei_latent_2003} that utilize the correlation of latent topics, and relates the similar documents together. Instead of Dirichlet distribution, CTM applies multivariate logistic-normal distribution to model the word distribution.

The model contain $ K $ topics distribution as $ \beta_{1:K} $, $ z_{n,d} $ is the topic assigned to the n-th topic and d-th document.$ \theta_d $ is the corresponding proportion a topic is distributed to d-th document.$ \mu $ and $ \Sigma $ are the corresponding mean and $ K \times K $ covariance matrix of the distribution between documents.

\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 16mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
  % sigma
  \node[main] (alpha) [label=below:$\alpha$] { };    
  % eta
  \node[main] (theta) [right=of alpha,label=below:$\theta$] { };
  % z
  \node[main] (z) [right=of theta,label=below:z] {};
  % beta
  \node[main] (beta) [above=of z,label=below:$\beta$] { };
%  \node[main] (beta) [left=of psi,label=below:$\beta$] { };
  \node[main, fill = black!10] (w) [right=of z,label=below:w] { };
  \path (alpha) edge [connect] (theta)
        (theta) edge [connect] (z)
		(z) edge [connect] (w)
		(beta) edge [connect] (w);
%		(beta) edge [connect] (psi);
  \node[rectangle, inner sep=4.4mm,draw=black!100, fit= (beta)] {};
  \node[rectangle, inner sep=4.4mm, fit= (beta), label=below right:K, xshift=-5mm, yshift=18.5mm] {};
  \node[rectangle, inner sep=0mm, fit= (z) (w),label=below right:N, , xshift=13mm] {};
  \node[rectangle, inner sep=4.4mm,draw=black!100, fit= (z) (w)] {};
    \node[rectangle, inner sep=4.6mm, fit= (z) (w),label=below right:D, xshift=12.5mm] {};
  \node[rectangle, inner sep=9mm, draw=black!100, fit = (theta) (z) (w)] {};
\end{tikzpicture}
\caption{Graphical representation for CTM}
\label{graph:ctm}
\end{figure}
\begin{algorithm}[H]
Initialize $ \mu, \Sigma $\\
\For{document d in D}{
Sample a topic distribution $ \eta_d\sim\mathcal{N}(\mu,\Sigma) $\\
\For{word position n in $ N_d $}{
Sample a topic assignment $ z_{dn}\sim \text{Mult}(f(\eta_d)) $\\
Sample a word $ w_{dn}\sim \text{Mult}(\beta_{z_d,n}) $
}
}
\label{algorithm:ctm}
\caption{Generative Process for CTM}
\end{algorithm}
From Algorithm \ref{algorithm:ctm}, the parametrization $ \mu, \Sigma $ are initialized. For each document d in document collection D, a topic distribution $ \eta_d $ is drawn from normal distribution parametrized $ \mu,\Sigma $. Then for each word position n in $ N $ words in doucment d, a topic assignment $ z_{dn} $ is drawn from multinomial distribution parametrized $ f(\eta_d) $, where the transformation $ f(\eta) $ represents the softmax function maps the sample draw from normal distribution to topic proportion $ \theta $, a topic distribution which points on the K-1 simplex. Finally, a word is sampled from multinomial distribution $ Mult(\beta_{z_d,n}) $.

From figure \ref{graph:ctm} we can see, word and topic-word assignment are in the word and document plate $ N\times D $. The document topic proportion $ \eta $ is on the document plate D. Specifically, the topic word proportion $ \beta $ is on the topic plate K, which is specified as word distribution selected by topic assignment z. 

\paragraph{Mathematical Formulation} The joint distribution for CTM is described as follows,
\begin{align*}
p(\eta,z,w|\beta,\mu,\Sigma)=\prod_{d=1}^{D}p(\eta_d|\mu,\Sigma)\prod_{n=1}^{N_d}p(z_{dn}|\eta_d)p(w_{dn}|z_{dn},\beta_{1:K})
\end{align*}
and the ELBO is defined as,
\begin{align*}
\mathcal{L}\geq&\sum_{d=1}^{D}\mathbb{E}_{q_d}\left[\log p(\eta_d,z_d,w_d|\mu,\Sigma,\beta_{1:K})\right]-\sum_{i=1}^{K}\log\text{KL}(q(\eta_i|\lambda_i,\nu_i^2)|p(\eta_d|\mu,\Sigma))\\
&-\sum_{n=1}^{N}\log\text{KL}(q(z_n|\phi_n)||p(z_n|\eta_d))
\end{align*}
\section{LKJ Correlation Distribution} \label{ch2:lkj}
LKJ distribution \cite{lewandowski_generating_2009} is a distribution for modeling correlation matrix. The distribution is described as equation \ref{eq:lkj}
\begin{align} \label{eq:lkj}
f(C|\eta)=&2^{\sum_{k=1}^{K-1}(2(\eta-1)+K-k)(K-k)}\times\\
&\prod_{k=1}^{K-1}(B(\eta+(K-k-1)/2,\eta+(K-k-1)/2)^{K-k})(\det(C)^{\eta-1})
\end{align}
When $ \eta=1 $ it is simply a uniform distribution allocated over the correlation matrix. If $ \eta>0 $, it is a modal correlation matrix. The density concentrated around center when the $ \eta $ value become larger.
To apply it into normal distribution as covariance, we could apply transformation equation \ref{eq:lkj_trans} and turn it into covariance matrix\cite{barnard_modeling_2000}.
\begin{align}\label{eq:lkj_trans}
\Sigma=diag(\sigma)\cdot C \cdot diag(\sigma)
\end{align}
% Transformation
Directly drawing correlation matrix from LKJ distribution is not practical in reality case. It is common to draw correlation matrix from factorized Cholesky LKJ distribution instead, where the probability density function is described as equation \ref{eq:lkj_chol}
\begin{align} \label{eq:lkj_chol}
\text{LKJChol}(L|\eta)&\propto|J|\det(LL^\top)^{(\eta-1)}\\
&=\prod_{k=2}^{K}L_{kk}^{K-k+2\eta-2}
\end{align}
The lower triangular matrix L is a Cholesky factorization for the correlation matrix iff $ L_{k,k}>0 $
\begin{align}
\Sigma=\text{diag}(\sigma)\cdot LL^\top \cdot \text{diag}(\sigma)
\end{align}
similarly, the transformation from LKJ Cholesky matrix to covariance matrix as equation \ref{eq:lkj_trans}.
\section{Embedded Topic Model} \label{ch2:etm}
Embedded Topic Model \cite{dieng_topic_2019} is one of the state-of-art approaches for topic model task. It takes word distribution $ \beta $ as a topic embedding for words. % explain the algorithm
Similar to Word2Vec\cite{mikolov_distributed_nodate}, the word distribution is a softmax function of the inner product of context matrix $ \rho $ and context embedding $ \alpha $. Specifically, the algorithm   equation \ref{eq:etm_embedding}
\begin{equation}\label{eq:etm_embedding}
\beta\sim\sigma(\rho^\top\alpha)
\end{equation}
The word is drawn from the generative process shown in algorithm \ref{algorithm:etm}, for each document, sample a topic distribution $ \theta $ from logistic-normal distribution parameterized with zero mean and identity covariance. Then for each word position n, the model sample topic assignment $ z_{dn} $ from categorical distribution $ \theta_d $. Finally, a word is drawn from $ \text{softmax}(\rho^\top\alpha) $ on $ z_{dn} $ the row.\\
\begin{algorithm}[H]\label{algorithm:etm}
\ForEach{document $d\in 1\dots D $}{
Draw document topic distribution $ \theta_d\sim \mathcal{LN}(0,I) $\\
\ForEach{word position $n\in 1\dots N_d $}{
%Generate topic $ z_{d,n}\sim Cat(\eta_d) $\\
Draw topic assignment $ z_{d,n}\sim Cat(\theta_d) $\\
Draw word $ w_{d,n}\sim \sigma(\rho^\top\alpha)_{z_{dn}} $
}}
\caption{Generative Process for ETM}
\end{algorithm}
Although ETM can maintain for Word2Vec training as the embedding for topic modeling, it cannot capture 
\section{Transformer} \label{ch2:transformer}
Transformer\cite{vaswani_attention_nodate} is a popular neural network architecture in natural language processing. To briefly explain Transformer, it is an stacked encoder-decoder architecture. The component that makes transformer stands out of other architectures it the attention mechanism. The attention mechanism 