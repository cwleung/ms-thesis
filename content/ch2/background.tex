In this section, we give short description on topic model techniques and several key components and terminologies related to LDA model. In section \ref{ch2:tm}, we formulate the problem description of topic modeling. In \ref{ch2:we}, we give description to word embedding. In section \ref{ch2:ctm}, we explain the algorithm of Correlated topic model(CTM). Then,  section \ref{ch2:lkj} will give the definition for LKJ correlation distribution. Section \ref{ch2:etm} will cover the Embedded Topic Model(ETM). Finally, section \ref{ch2:transformer} will give introduction to transformer embedding that we are used in the model. Several topic modeling techniques include non-negative matrix factorization (NMF) \cite{lee_learning_1999}, Latent Semantic Analysis \cite{landauer_solution_1997}, probabilistic Latent Semantic Analysis (pLSA) \cite{hofmann_probabilistic_2013} and Latent Dirichlet Allocation (LDA)\cite{blei_latent_2003}.
\section{Problem definition}\label{ch2:tm} Massive data sets in internet has made the task of understanding data by accessing them one-by-one became not humanly possible. The raise of topic modeling gives possibility to summarize a given set of document collections. To describe topic modeling intuitively, for a document collection $ d=\{1\cdots D\} $ and define a number of topic $ K $, the model outputs topic-word distribution $\beta\in\mathbb{R}^{K\times V}$, where $ K $ is number of topics and $ V $ is number of vocabularies in document set. For $ \{\beta_{k,1},\beta_{k,2},\cdots,\beta_{k,V}\}^{K}_{k=1} $, each $ \{\beta_{k,v}\}^{K}_{k=1} $ resulting the expression power vocabulary v could represent in topic k. The higher value a vocabulary obtained in tuple $ \{\beta_{k,v}\}^{V}_{v=1} $, the high representation power that the word is related to a latent topic. Practically, we capture top-m words from topic model for each topic k. For a set of top-m words obtained from topic k in descending order, defined as $ \{\beta_{k,v_m}\}^{M}_{m=1} $ where $ \beta_{k,v_1}\succeq\cdots\succeq\beta_{k,v_m}\succeq\cdots\succeq\beta_{k,v_M} $.
\section{Bag-of-word assumption} 
% Define the document
Suppose we have a collection of documents $ \in\mathbb{R}^{} $. The document and vocabularies
% Define BOW representation
Specifically, Topic model is a generative model that the probability based on Bag-of-words(BOWs) assumptions. Bag-of-words(BOWs) is a assumption that all words in the document are considered are independently distributed. To represent BOW, let a document collection $ W=(w_1,w_2,\cdots,w_D),d\in\{1\dots D\} $ documents, where $ w_d $ is a single document $ d $ contain words $ w_d=(w_{d1},w_{d2},\cdots,w_{dN_d}), n\in\{1\dots N_d\} $ word position, where $ w_{dn} $ is a single word in document $ d $ at position $ n $.
In equation \ref{eq:bow}, we take unigram model as an example\cite{__2015}, 
\begin{align}\label{eq:bow}
p(W|\phi)=\prod_{d=1}^{D}p(w_d|\phi)=\prod_{d=1}^{D}\prod_{n=1}^{N_d}p(w_{dn}|\phi)=\prod_{d=1}^{D}\prod_{n=1}^{N_d}\phi_{w_{dn}}=\prod_{v=1}^{V}\phi^{N_v}_{v}
\end{align}
% conversion to vocabularies
For sake of convenience, due to documents contain different size of words. It is not feasible to construct matrix representation for modeling. For the reason, we can exchange the representation for BOW from iterating the word occurrence for each word position to iterating the word occurrence in a vocabulary set. In this way, we can define a matrix for BOW: $ W\in\mathbb{R}^{D\times V} $, with D rows of document and V rows of vocabulary count. Formally, for every document $ W=(w_{d1},w_{d2},\cdots,w_{dV}), v\in\{1,\cdots,V\} $ vocabularies, $ w_{dv} $ is the occurrence of a vocabulary v in document d.
\section{LKJ Correlation Distribution} \label{ch2:lkj}
LKJ distribution \cite{lewandowski_generating_2009} is a distribution for modeling correlation matrix. The distribution is described as equation \ref{eq:lkj}\footnote{\url{https://distribution-explorer.github.io/multivariate_continuous/lkj.html}}
\begin{align} \label{eq:lkj}
f(C|\eta)=&2^{\sum_{k=1}^{K-1}(2(\eta-1)+K-k)(K-k)}\times\\
&\prod_{k=1}^{K-1}(B(\eta+(K-k-1)/2,\eta+(K-k-1)/2)^{K-k})(\det(C)^{\eta-1})
\end{align}
$ B(\cdot,\cdot) $ is beta distribution, and K is the number of variable in correlation matrix. $ \eta $ is concentration parameter for LKJ distribution. When $ \eta=1 $ it is basically an uniform distribution allocated over the correlation matrix. If $ \eta>0 $ and become larger, the density of the matrix concentrates around center.
To apply it into normal distribution as covariance, we could apply transformation equation \ref{eq:lkj_trans} and turn it into covariance matrix\cite{barnard_modeling_2000}. $ \text{diag}(\sigma) $ is the diagonal elements of the variance vector $ \sigma $.
\begin{align}\label{eq:lkj_trans}
\Sigma=\text{diag}(\sigma)\cdot C \cdot \text{diag}(\sigma)
\end{align}
% Transformation
Directly drawing correlation matrix from LKJ distribution is not efficient in reality case. It is common to draw correlation matrix from factorized Cholesky LKJ distribution instead, where the probability density function is described as equation \ref{eq:lkj_chol}
\begin{align} \label{eq:lkj_chol}
\text{LKJChol}(L|\eta)&\propto|J|\det(LL^\top)^{(\eta-1)}\\
&=\prod_{k=2}^{K}L_{kk}^{K-k+2\eta-2}
\end{align}
The lower triangular matrix L is a Cholesky factorization for the correlation matrix iff $ L_{k,k}>0 $
\begin{align}
\Sigma=\text{diag}(\sigma)\cdot LL^\top \cdot \text{diag}(\sigma)
\end{align}
similarly, the transformation from LKJ Cholesky matrix to covariance matrix as equation \ref{eq:lkj_trans}.
\section{Topic Models}
\subsection{Correlated Topic Model} \label{ch2:ctm}
Correlated Topic Model (CTM)\cite{blei_correlated_2007} is an extension of LDA\cite{blei_latent_2003} that utilize the correlation of latent topics, and relates the similar documents together. Instead of Dirichlet distribution, CTM applies multivariate logistic-normal distribution to model the word distribution.

The model contain $ K $ topics distribution as $ \beta_{1:K} $, $ z_{n,d} $ is the topic assigned to the n-th topic and d-th document.$ \theta_d $ is the corresponding proportion a topic is distributed to d-th document.$ \mu $ and $ \Sigma $ are the corresponding mean and $ K \times K $ covariance matrix of the distribution between documents.\\
\begin{algorithm}[H]
Initialize $ \mu, \Sigma $\\
\For{document d in D}{
Sample a topic distribution $ \eta_d\sim\mathcal{N}(\mu,\Sigma) $\\
\For{word position n in $ N_d $}{
Sample a topic assignment $ z_{dn}\sim \text{Mult}(f(\eta_d)) $\\
Sample a word $ w_{dn}\sim \text{Mult}(\beta_{z_d,n}) $
}
}
\label{algorithm:ctm}
\caption{Generative Process for CTM}
\end{algorithm}
From Algorithm \ref{algorithm:ctm}, the parametrization $ \mu, \Sigma $ are initialized. For each document d in document collection D, a topic distribution $ \eta_d $ is drawn from normal distribution parametrized $ \mu,\Sigma $. Then for each word position n in $ N $ words in doucment d, a topic assignment $ z_{dn} $ is drawn from multinomial distribution parametrized $ f(\eta_d) $, where the transformation $ f(\eta) $ represents the softmax function maps the sample draw from normal distribution to topic proportion $ \theta $, a topic distribution which points on the simplex that all elements in the vector sums to 1. Finally, a word is sampled from multinomial distribution $ Mult(\beta_{z_d,n}) $.
\begin{figure}[h]
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 16mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
  % sigma
  \node[main] (alpha) [label=below:$\alpha$] { };    
  % eta
  \node[main] (theta) [right=of alpha,label=below:$\theta$] { };
  % z
  \node[main] (z) [right=of theta,label=below:z] {};
  % beta
  \node[main] (beta) [above=of z,label=below:$\beta$] { };
%  \node[main] (beta) [left=of psi,label=below:$\beta$] { };
  \node[main, fill = black!10] (w) [right=of z,label=below:w] { };
  \path (alpha) edge [connect] (theta)
        (theta) edge [connect] (z)
		(z) edge [connect] (w)
		(beta) edge [connect] (w);
%		(beta) edge [connect] (psi);
  \node[rectangle, inner sep=4.4mm,draw=black!100, fit= (beta)] {};
  \node[rectangle, inner sep=4.4mm, fit= (beta), label=below right:K, xshift=-5mm, yshift=18.5mm] {};
  \node[rectangle, inner sep=0mm, fit= (z) (w),label=below right:N, , xshift=13mm] {};
  \node[rectangle, inner sep=4.4mm,draw=black!100, fit= (z) (w)] {};
    \node[rectangle, inner sep=4.6mm, fit= (z) (w),label=below right:D, xshift=12.5mm] {};
  \node[rectangle, inner sep=9mm, draw=black!100, fit = (theta) (z) (w)] {};
\end{tikzpicture}
\caption{Graphical representation for CTM}
\label{graph:ctm}
\end{figure}
From figure \ref{graph:ctm} we can see, word and topic-word assignment are in the word and document plate $ N\times D $. The document topic proportion $ \eta $ is on the document plate D. Specifically, the topic word proportion $ \beta $ is on the topic plate K, which is specified as word distribution selected by topic assignment z. 

\paragraph{Mathematical Formulation} The joint distribution for CTM is described as follows,
\begin{align*}
p(\eta,z,w|\beta,\mu,\Sigma)=\prod_{d=1}^{D}p(\eta_d|\mu,\Sigma)\prod_{n=1}^{N_d}p(z_{dn}|\eta_d)p(w_{dn}|z_{dn},\beta_{1:K})
\end{align*}
and the ELBO is defined as,
\begin{align*}
\mathcal{L}\geq&\sum_{d=1}^{D}\mathbb{E}_{q_d}\left[\log p(\eta_d,z_d,w_d|\mu,\Sigma,\beta_{1:K})\right]-\sum_{i=1}^{K}\log\text{KL}(q(\eta_i|\lambda_i,\nu_i^2)|p(\eta_d|\mu,\Sigma))\\
&-\sum_{n=1}^{N}\log\text{KL}(q(z_n|\phi_n)||p(z_n|\eta_d))
\end{align*}
\subsection{Embedded Topic Model} \label{ch2:etm}
Embedded Topic Model \cite{dieng_topic_2019} is one of the state-of-art approaches for topic model task. It takes word distribution $ \beta $ as a topic embedding for words. % explain the algorithm
Similar to Word2Vec\cite{mikolov_distributed_nodate}, the word distribution is a softmax function of the inner product of context matrix $ \rho $ and context embedding $ \alpha $. Specifically, the algorithm   equation \ref{eq:etm_embedding}
\begin{equation}\label{eq:etm_embedding}
\beta\sim\sigma(\rho^\top\alpha)
\end{equation}
The word is drawn from the generative process shown in algorithm \ref{algorithm:etm}, for each document, sample a topic distribution $ \theta $ from logistic-normal distribution parameterized with zero mean and identity covariance. Then for each word position n, the model sample topic assignment $ z_{dn} $ from categorical distribution $ \theta_d $. Finally, a word is drawn from $ \text{softmax}(\rho^\top\alpha) $ on $ z_{dn} $ the row.\\
\begin{algorithm}[H]\label{algorithm:etm}
\ForEach{document $d\in 1\dots D $}{
Draw document topic distribution $ \theta_d\sim \mathcal{LN}(0,I) $\\
\ForEach{word position $n\in 1\dots N_d $}{
%Generate topic $ z_{d,n}\sim Cat(\eta_d) $\\
Draw topic assignment $ z_{d,n}\sim Cat(\theta_d) $\\
Draw word $ w_{d,n}\sim \sigma(\rho^\top\alpha)_{z_{dn}} $
}}
\caption{Generative Process for ETM}
\end{algorithm}
\section{Representation learning}
\subsection{Word Embedding} \label{ch2:we}
% explain what is word embeddings
Word embedding\cite{bengio_neural_nodate} is a kind of representation for words from document collections using a vector formulation. The nature of word embedding is that, the words that having similar meaning have a close distance(in most case euclidean distance), and vice versa. For instance, continuous bag-of-words(CBOW) \cite{mikolov_distributed_nodate} is a kind of word embeddings converting bag-of-word in to a vector of n-dimension continuous space, which contains the following formulation,
\begin{align*}
w\sim\text{softmax}(\rho^\top\alpha)
\end{align*}
where $ \rho\in\mathbb{R}^{L\times V} $ is the embedding matrix which a function $ f:\mathbb{R}^V\mapsto\mathbb{R}^{L} $ maps $ V $ vocabularies into $ L $ dimension of continuous vector space. And $ \alpha $ is the context embedding, which conveniently convert the latent dimension L to a custom dimension of continuous embedding space $ \mathbb{R}^{N} $ as $ \tilde{f}:\mathbb{R}^L\mapsto\mathbb{R}^{N} $.
\subsection{Transformer} \label{ch2:transformer}
Transformer\cite{vaswani_attention_nodate} is a popular neural network architecture in natural language processing. To briefly explain transformer, it is an stacked encoder-decoder architecture. The component that makes transformer stands out of other architectures it the mutli-head self-attention mechanism. 

In this section we only cover the main components of transformer. The details for transformer can be reviewed in author's blog post\footnote{\url{https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html}}.

To define Transformer, equation \ref{eq:scale-dot} denotes the scaled dot product that transformer use to calculate attention score. $ Q\in\mathbb{R}^{T\times d_k} $,  $ K\in\mathbb{R}^{T\times d_k} $ and $ V\in\mathbb{R}^{T\times d_v} $ are the query, key, value term vector used to calculate the context vector. S represents the sequence length and $ d_k $ and $ d_v $ the dimension for the key and value respectively. $ \frac{1}{\sqrt{d_k}} $ is used to scale down attention matrix in which to maintain a proper variance for the attention scores.
\begin{align}\label{eq:scale-dot}
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{align}
Weight matrices $ W_i^Q\in\mathbb{R}^{d_{model}\times d_k} $, $ W_i^K\in\mathbb{R}^{d_{model}\times d_k} $ and $ W_	i^V\in\mathbb{R}^{d_{model}\times d_k} $  for query, key and value are parameters to be learned. The scaled dot-product attention computes a sequence of vector outputs. $ W^O\in\mathbb{R}^{hd_v\times d_{model}} $, where $ d_{model} $ is the dimension of key-value pair multiplies number of head.
\begin{align}
\text{Multihead(Q,K,V)}&=\text{concat}(h_1,\cdots,h_h)W^O\\
h_i&=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)
\end{align}
% Layer Norm
The sub-layer multi-head self-attention is connected to the fully-connected feed-forward network. The outputs of each sub-layer are added to the original output with residual connection applying Layer Normalization as eq. \ref{eq:layernorm}.
\begin{align}\label{eq:layernorm}
x=\text{LayerNorm}(x+\text{Sublayers}(x))
\end{align}
%Transformer\cite{vaswani_attention_nodate} is a popular neural network architecture in natural language processing. To briefly explain Transformer, it is an stacked encoder-decoder architecture. The component that makes transformer stands out of other architectures it the mutli-head self-attention mechanism. 
%
%In this section we only cover the main components of transformer. The details for transformer can be reviewed in author's blog post\footnote{\url{https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html}}.
%
%To define Transformer,
%\subsubsection{Scaled Dot Product Attention}
%\begin{align*}
%\text{Attention}(Q,K,V) = \sigma\left(\frac{QK^T}{\sqrt{d_k}}\right)V
%\end{align*}
%\begin{align*}
%k_i, q_i\sim\mathcal{N}(0,\sigma)\rightarrow\text{Var}\left(\sum_{i=1}^{d_k}q_i\cdot k_i\right)=\sigma\cdot d_k
%\end{align*}
%\subsubsection{Multi-head attention}
%To obtain a big value from a sentence, extending the attention mechanism to multiple head instead of one could 
%\begin{align*}
%h_i&=Attn(QW_i^Q,KW_i^K,VW_i^V)\\
%\text{MHA(Q,K,V)}&=(h_1,\cdots,h_h)W^O
%\end{align*}
%\subsection{Multi-Head Attention}
%\begin{align*}
%\text{Multihead}(Q,K,V)=\text{Concat}(\text{head}_i,\dots,\text{head}_h)W^O \\
%\text{head}_i = \text{Attention}(QW_i^{Q}, KW_i^K,VW_i^V)
%\end{align*}
\subsubsection{Positional Encoding}
One drawback for Multi-Head Attention block is that it does not consider information about word positioning. The positional encoding function maps the sentence sequences into i-dimension of hidden space fo each permutation position $ pos $. And so the model can identify the from the additional positional features space of the input.
\footnote{Description from \url{https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html.}}
\[ PE_{(pos,i)} = \begin{cases} 
\sin\left(\frac{pos}{10000^{i/d}model}\right) & \text{if }i \mod 2 = 0 \\
\cos\left(\frac{pos}{10000^{(i-1)/d}model}\right) & \text{otherwise}
       \end{cases}
    \]
the positional encoding function $ PE_{pos,i} $, where the $ pos $ represents the permutation 
and the odd/even dimension are treated on sine/cosine function respectively. 
\section{Time Series model}
\subsection{Gaussian Process (GP)}
Gaussian Process\cite{rasmussen_gaussian_2005} is a versatile algorithm which is commonly used for both supervised and unsupervised learning problems including regression, classification and clustering. It has been demonstrated a strong pursuit on times-series and reinforcement learning problems. A Gaussian Process $ \mathcal{GP}(\cdot,\cdot) $ is denoted by following, where m is the mean and k stands for a kernel for covariance function,
\begin{align*}
f\sim\mathcal{GP}(m,k)
\end{align*}
For a set of input $ x_1,\dots,x_N $, the joint probability density function $ p(f(x_1),\dots,f(x_N)) $ is a normal distribution condition on mean vector $ m(X) $ and covariance matrix $ k(x,x') $ which is a positive semidefinite matrix. In most case the mean vector are specified with zero mean (m(X)=0).
\begin{align*}
p(f(X))=\mathcal{N}(m(X),k(X,X))
\end{align*}
%\begin{align*}
%q(f(X))=\int q(f(X)|u)q(u)du
%\end{align*}
In particular, in Gaussian Process regression task, a output variable $ y_i $ denoted $ y_i\sim f(x_i)+\epsilon $ where $ \epsilon $ is a Gaussian noise. Therefore, given a set of training input and target, 
\begin{align*}
p(y|f)=\mathcal{N}(y;f,\sigma_\epsilon^2I)
\end{align*} 
Radial Basis Function kernel (namely RBF kernel) is a common function that used as a kernel in GP model. For a kernel $ k(x,x') $,
\begin{align*}
k(x,x')=\sigma^2\exp\left(-\frac{||x-x'||^2}{2l^2}\right)
\end{align*}
where $ l $ is the kernel bandwith.
%\paragraph{Matern Kernel}
%\begin{align*}
%k_{\text{Matern}}(x_1,x_2)=\frac{2^{1-\nu}}{\Gamma(\nu)}(\sqrt{2\nu}d)^{\nu}K_\nu(\sqrt{2\nu}d)
%\end{align*}
The ELBO for GP is derived from \cite{hensman_gaussian_nodate}, as
\begin{align*}
\log p(y)\geq\mathbb{E}_{q(f)}[\log p(y|f)]-KL[q(u)||p(u)]
\end{align*}
where the variational distribution is $ q(f)\approx\int p(f|u)q(u)du $
\subsection{Gaussian Process Latent Variable Model (GPLVM)}
Gaussian Process Latent Variable Model \cite{lawrence_gaussian_nodate,titsias_bayesian_nodate} is a dimension reduction method turn high dimensional data into low-dimension space. Formally, given a latent variable $ X\in\mathbb{R}^{N\times Q} $, and high dimensional real valued observations $ Y\in\mathbb{R}^{N\times D} $, the model induce high dimensional from a low-dimensional mapping $ \mathcal{X}\rightarrow\mathcal{Y} $ such that $ Q<<D $.　The prior latent variable X is defined at a standard normal distribution in dimension Q.
\begin{align*}
p(X)=\prod_{n=1}^{N}\mathcal{N}(x_n;0,I_Q)
\end{align*}
The latent function $ f $ take the kernel $ K_d $ that determine the covariance matrix at $ D $-dimension
\begin{align*}
p(f|X,\theta)=\prod_{d=1}^{D}\mathcal{GP}(f_d;0,K_d)
\end{align*}
and then it can recover back the observed data through the latent function $ f $ conditioned on the $ \mathcal{GP} $ prior with $ p(Y|f,X) $
\begin{align*}
p(Y|f,X)=\prod_{i=1}^{N}\prod_{i=1}^{D}\mathcal{N}(y_{n,d};f_d(x_n),\sigma_y^2)
\end{align*}
Here we discuss the variational inference method for the inference process. The variational distribution for GPLVM is given as eq. \ref{ch2:gplvm_var} according to GP sparse approximation in \cite{lawrence_learning_2007}. In the formulation, $ x  $ is the latent variable, $ f_d $ is the latent function for the covariance 
where variational distribution for $ q(X)=\sum_{n=1}^{N}\mathcal{N}(x_n;m_n,S_n) $ for each $ n^{th} $ row on $ x $, and $ q(u_d)=\mathcal{N}(u_d|0,K_{MM}) $
\begin{align}\label{ch2:gplvm_var}
q(\{f_d,u_d\}^{D}_{d=1},X) = \prod_{i=1}^{N}q(x_n)\prod_{d=1}^{D}p(f_d|u_d,X)q(u_d)
\end{align}
The latent variable ELBO is given by,
\begin{align}
\mathcal{L}=&\sum_{n,d}\mathbb{E}_{q_\phi(x_n)}\mathbb{E}_{p(f_d|u_d,x_n)q_{\lambda}(u_d)}\left[\log\mathcal{N}(y_{n,d};f_d(x_n),\sigma_y^2)\right]\\
&-\sum_{n}\text{KL}(q_{\phi}(x_n)||p(x_n))-\sum_{d}\text{KL}(q_\lambda(u_d)||p(u_d|Z))
\end{align}
the $ \phi $ is the local variational parameters, $ \lambda $ is the global variational parameters, $ \theta $ is the kernel hyperparameters and $ \sigma^y $ is the likelihood noise.
% TODO sparse variational inference
to avoid heaving computation of X inside the conditional probability $ p(f_d|X) $, inducing input has been introduced \cite{lawrence_learning_2007} to replace the X with inducing variables $ u_d\in\mathbb{R}^M $, which conditioning on inducing input locations $ z\in\mathbb{R}^{M\times Q} $, 
\subsection{Dynamic Topic Model}
Dynamic Topic Model \cite{blei_dynamic_2006} is a model enables to capture topic information from time-series data set. The model utilize documents from different time and generates the corresponding topic-word representation.

% Generative process
The generative process of the $ d^{th} $ document is the following:\\
\begin{algorithm}[H]
Draw topc embedding $ \alpha_k^t\sim\mathcal{N}(\alpha_k^{t-1},\gamma^2I) $\\
Sample topc proportion $ \theta_{t_d}\sim\mathcal{N}(\eta_{t_d},\xi^2I) $\\
\For{word position n in $ N_d $}{
Sample topic $ z_{d,n}\sim\text{Mult}(\theta_{t_d}) $\\
Sample word $ w_{d,n}\sim\text{Cat}(\sigma(\rho^\top\alpha_{z_{d_n}}^{(t_d)})) $
}
\caption{Generative Process for DTM}
\label{algorithm:dtm}
\end{algorithm}
$\alpha_k^{t-1}$ is the mean of current time $ t $ which take the value previous time step $ t-1 $.
$ \gamma^2 $ and $ \xi^2 $ are the variances for the prior.
\section{Posterior Inference}\label{ch4:1}
Since the exact inference of the posterior is intractable in real application, we employed approximation scheme for the posterior inference. The popular approaches are Markov Chain Monte Carlo Method (MCMC) and Variational Inference(VI)\cite{blei_variational_2006,hoffman_stochastic_2013}. Gibbs sampling is one of the MCMC method and it is fast to compute the approximation and easy to the implementation. Then, Variational EM algorithm to be carried out for maximizing the likelihood over all word in corpus in the document. An alternative way to perform estimation is Monte Carlo method.
\subsection{Variational Inference}
Given that posterior approximiation is not alway practical in real world application. Approximation method are necessary to be apply. Their are two main approach for the posterior approximation: Markov Chain Monte Carlo(MCMC) Variational Inference. Variational Inference is a method approximating the posterior in optimization fashion. To give a better intuition, let probability $ p(x) $ depending on a latent variable $ z $ such that $ p(x|z)=\int p(x|z)p(z)dz $. We can turn the following posterior inference problem into a optimization problem. Here we derive the bound and perform optimization, by doing some calculation, we can obtain the following lower bound for the likelihood $ \mathcal{L}_i(p,q_i) $
\begin{align}
\log p(x_i)\geq\mathbb{E}_{z\sim q_i(z))}[\log p(x_i|z)+\log p(z)]+\mathbb{E}_{z\sim q_i(z))}[\log q_i(z)]=\mathcal{L}_i(p,q_i)
\end{align}
then by deriving the KL-divergence, we can obtain the log probability $ \log p(x_i) $ is the likelihood-term minus the KL-divergence of $ q_i(z) $ and $ p(z|x_i) $.
\begin{align}
KL(q_i(z)||p(z|x_i))&=-\mathcal{L}_i(p,q_i)+\log p(x_i)\\
\mathcal{L}_i(p,q_i)&=\log p(x_i)-KL(q_i(z)||p(z|x_i))
\end{align}
with this derivation, we turn a posterior approximation problem into a optimization problem by maximizing the evidence lower bound(ELBO).
\subsection{Stochastic Variational Inference}
Stochastic Variational Inference (SVI)\cite{hoffman_stochastic_2013} is a scalable variant of variational inference, which enables mini-batching to split dataset and train for each epochs, then become a standard of optimization for probabilistic models.
Two main improvement are made by the SVI: stochastic optimization and noisy gradient.
% TODO Collapsing parameters
\subsection{Collapsing Parameters}
In original LDA model, the parameter z is responsible for sampling topic assignment for each word position in every single document. Collapsing parameters\cite{srivastava_autoencoding_2017} introduced to reduce the latent variable $ z $ in the generative process in hence to speed up computation.
\begin{align}\label{eq:cp}
w_d\sim\prod_{n=1}^{N_d}\text{Cat}(\theta_d^\top\beta_{w_{dn}})
\end{align}
The trick in equation \ref{eq:cp} rewrite the original LDA word drawing process, and hence define a new evidence lower bound for the topic model.
% TODO Blei's notes
\subsection{Autoencoding Variational Bayes (AEVB)}
Generally when we optimize a variational parameter, it is neceasary to derive a ELBO　and then derive the optimization step for gradient descent. While amortized inference latent variable z is parameterized by two inference network  $ \mu_{\phi(x)},\sigma_{\phi}(x) $ .
% TODO UCLA notes
\begin{align}
z = \mathcal{N}(\mu_{\phi}(x_i), \sigma_{\phi}(x_i))
\end{align}
After deriving the ELBO, we obtain the following likelihood term.
\begin{align}
\mathcal{L}=\mathbb{E}_{z\sim\mathcal{N}(\mu_{\phi}(x_i),\sigma_\phi(x_i))}\left[\log p_\theta(x_i|z)\right]-KL\left(q_\phi(z|x_i)||p(z)\right)
\end{align}
\subsection{Reparameterization trick}
The drawback of amortized inference is that, sampling from normal distribution parameterizing $ \mu_{\phi(x),\sigma_{\phi}(x)} $ could lead to high variance outcome and hamper the inference performance. For the reason, taking reparamterization trick\cite{kingma_auto-encoding_2014} to transform as equation \ref{eq:rt},
\begin{align}\label{eq:rt}
z= \mu_{\phi}(x_i)+\epsilon\sigma_\phi(x_i)\text{, }\epsilon\sim\mathcal{N}(0,1)
\end{align}
where $ \epsilon $ is a sample from normal distribution $ \mathcal{N}(0,1) $. and so the modified ELBO becomes equation \ref{eq:elbo_rt},
\begin{align}\label{eq:elbo_rt}
\mathcal{L}=\mathbb{E}_{\epsilon\sim N(0,1)}\left[\log p_\theta(x_i|\mu_{\phi}(x_i)+\epsilon\sigma_\phi(x_i))\right]-KL\left(q_\phi(z|x_i)||p(z)\right)
\end{align}
\section{Evaluation metrics}
\subsection{Perplexity}The proposed model will be evaluated with perplexity metric. The metric will examine how well the model can tackle with unseen data. It is equivalent algebraically to the inverse of the geometric mean per-word likelihood. Lower perplexity scores mean better.\begin{equation}
\text{Perplexity}(D_{test})=\exp{{-\frac{\sum_{d=1}^{M}\sum_{m=1}^{N_d}\log p(w_{dm})}{\sum_{d=1}^{M}N_d}}}
\end{equation}
\subsection{Topic Coherence}Topic Coherence\cite{mimno_optimizing_2011} measures the quality of the topic 
\begin{equation}
TC=\frac{1}{K}\sum_{k=1}^{K}\frac{1}{45}\sum_{i=1}^{10}\sum_{j=i+1}^{10}f(w_i^{(k)},w_j^{(k)})\end{equation}
where $\{w_1^{(k)},\cdots,w_{10}^{(k)}\}$ denotes top-10 most likely words in topic k. And function $f(\cdot,\cdot)$ is te normalized point-wise mutual information.\begin{equation}
f(w_i,w_j)=\frac{\log\frac{P(w_i,w_j)}{P(w_i)P(w_j)}}{-\log P(w_i,w_j)}\end{equation}
\subsection{Topic Diversity} In order to compare how the words each topic are differentiate the others. We applied the Topic Diversity metric \cite{dieng_topic_2019}. Topic Diversity (TD) to be the percentage of unique words in the top 25 words of all topics. Diversity close to 0 indicates redundant topics; diversity close to 1 indicates more varied topics. We define the overall metric for the quality of a model’s topics as the product of its topic diversity and topic coherence.
\begin{align}
TD=\frac{|A\cap B|}{|A \cup B|}
\end{align}
where $ A $ and $ B $ are top-k words from two topics. 