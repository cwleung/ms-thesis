\section{Latent Dirichlet Allocation (LDA)} Latent Dirichlet Allocation\cite{blei_latent_2003} is one of the popular latent variable model. It is found extensively useful in finding hidden topic model in a massive document set. The model is inspired by probabilistic semantic analysis (pLSA)\cite{hofmann_probabilistic_2013} and its naming was taken in a similar sense.
In the following, the LDA model will be explain with graphical model approach. In the figure,This define a joint posterior distribution $p(\theta,z,\beta|w)$. \begin{itemize}
\item $z_d$, $n$ is the per-word topic assignment.
\item $\theta_d$ is the per-document topic proportion.
\item $\theta_k$ is the per-corpus topic distribution.
\end{itemize}
\begin{figure}
\centering
\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 16mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
  % sigma
  \node[main] (alpha) [label=below:$\alpha$] { };    
  % eta
  \node[main] (theta) [right=of alpha,label=below:$\theta$] { };
  % z
  \node[main] (z) [right=of theta,label=below:z] {};
  % beta
  \node[main] (beta) [above=of z,label=below:$\beta$] { };
%  \node[main] (beta) [left=of psi,label=below:$\beta$] { };
  \node[main, fill = black!10] (w) [right=of z,label=below:w] { };
  \path (alpha) edge [connect] (theta)
        (theta) edge [connect] (z)
		(z) edge [connect] (w)
		(beta) edge [connect] (w);
%		(beta) edge [connect] (psi);
  \node[rectangle, inner sep=4.4mm,draw=black!100, fit= (beta)] {};
  \node[rectangle, inner sep=4.4mm, fit= (beta), label=below right:K, xshift=-5mm, yshift=18.5mm] {};
  \node[rectangle, inner sep=0mm, fit= (z) (w),label=below right:N, , xshift=13mm] {};
  \node[rectangle, inner sep=4.4mm,draw=black!100, fit= (z) (w)] {};
    \node[rectangle, inner sep=4.6mm, fit= (z) (w),label=below right:D, xshift=12.5mm] {};
  \node[rectangle, inner sep=9mm, draw=black!100, fit = (theta) (z) (w)] {};
\end{tikzpicture}
\caption{Graphical representation for LDA}
\label{graph:lda}
\end{figure}
The mathematical formulation for LDA as follows.
\begin{itemize}
\item Joint Distribution for LDA
\begin{equation*}
\prod_{i=1}^{K}p(\beta_i)\left[\prod_{d=1}^{D}p(\theta_d)\prod_{n=1}^{N}p(z_{d,n}|\theta_d)p(w_{d,n}|\beta_{1:K},z_{d,n})\right]
\end{equation*} 
\item conditioning on $w_{1:D}$
\begin{equation*}
p(\beta_{1:K},\theta_{1:D},z_{1:D}|w_{1:D})=\frac{p(\beta_{1:K},\theta_{1:D},z_{1:D},w_{1:D})}{p(w_{1:D})}
\end{equation*}
\end{itemize}
\begin{algorithm}[H]
Initialize hyperparameters $ \alpha $, $ \beta $\\
\For{topic k in K}{
Sample a word distribution $ \phi_k\sim \text{Dir}(\beta) $
}
\For{document d in D}{
Sample a topic distribution $ \theta_d\sim \text{Dir}(\alpha) $\\
\For{word position n in $ N_d $}{
Sample a topic assignment $ z_{dn}\sim \text{Cat}(\theta_d) $\\
Sample a word $ w_{dn}\sim \text{Cat}(\phi_{z_{dn}}) $
}
}
\caption{Generative Process for LDA}
\end{algorithm}
The procedure for LDA as follows. For each topic k$\in$\{1,...,K\}, draw a multinomial distribution $\beta_k$ from a Dirichlet distribution with parameter $\lambda$. For each document d$\in$\{1,...,M\}, draw a multinomial distribution $\theta_d$ from a Dirichlet distribution with parameter $\alpha$. For each word position n$\in$\{1,...,N\}, select a topic $z_n$ from the Multinomial distribution parameterized by $\theta_d$. Choose the observed word $w_n$ from the distribution $\theta_{z_n}$.
