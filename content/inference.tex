To perform inference to the model. We first derive the equation for the method.
% Marginal Likelihood
\subsection{Marginal Likelihood}
\begin{align*}
\mathcal{L}(\alpha,\rho)=\sum_{d=1}^{D}\log p(w_d|\alpha,\rho)=\sum_{d=1}^{D}\log p(w_d|\alpha)
\end{align*}
\begin{align*}
p(w_d|\alpha)=\int p(\delta_d)\prod_{n=1}^{N_d}p(w_{dn}|\delta_d,\alpha)d\delta_d
\end{align*}
\begin{align*}
p(w_{dn}|\delta_d,\alpha)=\sum_{k=1}^{K}\theta_{dk}\beta_{k,w_{dn}}
\end{align*}
% Inference Equation
\subsection{ELBO \& Variational Inference}
% Variational Inference
% KL-divergence for the logistic normal distribution
The KL divergence for the logistic normal distribution become the following
\subsection{ELBO for LKJ}
Joint Distribution
\begin{align*}
p(W,Z,\eta,\Sigma|\beta,\mu,\gamma)&=P(W|Z)P(Z|\eta)P(\eta|\mu,\Sigma)\\
&=p(\Sigma|\gamma)\prod_{d=1}^{D}P(\eta_d|\mu,\Sigma)\prod_{n=1}^{V}P(z_{d,n}|\eta_d)P(w_{d,n}|z_{d,n},\beta)
\end{align*}
Evidence Lower Bound(ELBO)
\begin{align*}
\mathcal{L}\geq&\mathbb{E}_q[\log p(W,Z,\eta,\Sigma)]-\mathbb{E}_q[\log q(Z,\eta,\Sigma)]\\
=&\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(w_{d,n}|z_{d,n},\beta)]+\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(z_{d,n}|\eta_d)]\\
&+\sum_{d=1}^{D}\mathbb{E}[\log p(\eta_d|\mu,\Sigma)]+\mathbb{E}_q[\log p(\Sigma|\gamma)]-\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log q(z_{d,n}|\alpha_{d,n})]\\
&-\sum_{d=1}^{D}\mathbb{E}_q[\log q(\eta_d|\lambda_d,\nu_d)]-\mathbb{E}_q[\log q(\Sigma|\phi)]
\end{align*}
Collapsing Parameters
\begin{align*}
\mathcal{L}&\geq\sum_{d=1}^{D}\int\int q(\eta_d)\log\frac{p(W_d|\eta_d,\beta)p(\eta_d|\mu,\Sigma)p(\Sigma|\gamma)}{q(\eta_d)q(\Sigma)}d\eta_d d\Sigma\\
&=\sum_{d=1}^{D}\left(\mathbb{E}_{q(\eta_d)}\left[\log p(W_d|\eta_d,\beta)\right]-KL(q(\eta_d)||p(\eta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}
Amortized Inference
\begin{align*}
=&\sum_{d=1}^{D}\left(\mathbb{E}_{q(\eta_d)}\left[\log p(w_d|\mathcal{LN}(\mu_{\eta_d}(w_d),\sigma_{\eta_d}(w_d)),\beta)\right]-KL(q(\eta_d)||p(\eta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}
Reparameterization Trick
\begin{align*}
=&\sum_{d=1}^{D}\left(\mathbb{E}_{q(\epsilon)}\left[\log p(w_d|\sigma(\mu_{\eta_d}(w_d)+\epsilon\sigma_{\eta_d}(w_d)),\beta)\right]-KL(q(\eta_d)||p(\eta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}
Mini-batch
\begin{align*}
\tilde{\mathcal{L}}=\frac{\mathcal{D}}{|\mathcal{B}|}\sum_{d\in\mathcal{D_B}}\left(\mathbb{E}_{q(\epsilon)}\left[\log p(w_d|\sigma(\mu_{\eta_d}(w_d)+\epsilon\sigma_{\eta_d}(w_d)),\beta)\right]-KL(q(\eta_d)||p(\eta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}
\begin{align*}
P(w|\mu_0(w_d)+\Sigma_0^{1/2}(w)\epsilon)
\end{align*}
The KL-divergence for the logistic-normal distribution
\begin{align*}
\tilde{\mathcal{L}}=&\sum_{d=1}^{D}\left[-\frac{1}{2}\left(tr(\sigma_1^{-1}\sigma_0)+(\mu_1-\mu_0)\Sigma_1^{-1}(\mu_1-\mu_0)-K+\log\frac{|\sigma_1|}{|\sigma_0|}\right)\right]\\
&+\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}\left[w_d^\top\log\sigma(\beta(\mu_0+\sigma_0^{1/2}\epsilon))\right]-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}