\section{Inference}
\subsection{Variational Inference}
Given that posterior approximiation is not alway practical in real world application. Approximation method are necessary to be apply. Their are two main approach for the posterior approximation: Markov Chain Monte Carlo(MCMC) Variational Inference. Variational Inference is a method approximating the posterior in optimization fashion.
\subsection{Stochastic Variational Inference}
Stochastic Variational Inference (SVI) is a kind of approach. 
Two main improvement are made by the SVI: stochastic optimization and noisy gradient. 
\subsection{Autoencoding Variational Bayes (AEVB)}
the latent variable is parameterized by neural network
\begin{align*}
z = \mathcal{N}(\mu_{\phi}(x_i), \sigma_{\phi}(x_i))
\end{align*}
\begin{align*}
\mathcal{L}=\mathbb{E}_{z\sim\mathcal{N}(\mu_{\phi}(x_i),\sigma_\phi(x_i))}\left[\log p_\theta(x_i|z)\right]-D_{KL}\left(q_\phi(z|x_i)||p(z)\right)
\end{align*}
\subsection{Reparameterization trick}
\begin{align*}
z= \mu_{\phi}(x_i)+\epsilon\sigma_\phi(x_i)\text{, }\epsilon\sim\mathcal{N}(0,1)
\end{align*}
\begin{align*}
\mathcal{L}=\mathbb{E}_{\epsilon\sim N(0,1)}\left[\log p_\theta(x_i|\mu_{\phi}(x_i)+\epsilon\sigma_\phi(x_i))\right]-D_{KL}\left(q_\phi(z|x_i)||p(z)\right)
\end{align*}
\section{Posterior Inference \& Parameter Estimation} Since the exact inference of the posterior is intractable in real application, we employed approximation scheme for the posterior inference. The popular approaches are Markov Chain Monte Carlo Method (MCMC) and Variational Inference (VI)\cite{blei_variational_2006,hoffman_stochastic_2013}. Gibbs sampling is one of the MCMC method and it is fast to compute the approximation and easy to the implementation. Then, Variational EM algorithm to be carried out for maximizing the likelihood over all word in corpus in the document. An alternative way to perform estimation is Monte Carlo method.

To perform inference to the model. We first derive the equation for the method.
% Marginal Likelihood
\section{Marginal Likelihood}
\begin{align*}
\mathcal{L}(\alpha,\rho)=\sum_{d=1}^{D}\log p(w_d|\alpha,\rho)=\sum_{d=1}^{D}\log p(w_d|\alpha)
\end{align*}
\begin{align*}
p(w_d|\alpha)=\int p(\delta_d)\prod_{n=1}^{N_d}p(w_{dn}|\delta_d,\alpha)d\delta_d
\end{align*}
\begin{align*}
p(w_{dn}|\delta_d,\alpha)=\sum_{k=1}^{K}\theta_{dk}\beta_{k,w_{dn}}
\end{align*}
% Inference Equation
\section{ELBO \& Variational Inference}
% Variational Inference
% KL-divergence for the logistic normal distribution
The KL divergence for the logistic normal distribution become the following
\section{ELBO for LKJ}
Joint Distribution
\begin{align*}
p(W,Z,\eta,\Sigma|\beta,\mu,\gamma)&=P(W|Z)P(Z|\eta)P(\eta|\mu,\Sigma)\\
&=p(\Sigma|\gamma)\prod_{d=1}^{D}P(\eta_d|\mu,\Sigma)\prod_{n=1}^{V}P(z_{d,n}|\eta_d)P(w_{d,n}|z_{d,n},\beta)
\end{align*}
Evidence Lower Bound(ELBO)
\begin{align*}
\mathcal{L}\geq&\mathbb{E}_q[\log p(W,Z,\eta,\Sigma)]-\mathbb{E}_q[\log q(Z,\eta,\Sigma)]\\
=&\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(w_{d,n}|z_{d,n},\beta)]+\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(z_{d,n}|\eta_d)]\\
&+\sum_{d=1}^{D}\mathbb{E}[\log p(\eta_d|\mu,\Sigma)]+\mathbb{E}_q[\log p(\Sigma|\gamma)]-\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log q(z_{d,n}|\alpha_{d,n})]\\
&-\sum_{d=1}^{D}\mathbb{E}_q[\log q(\eta_d|\lambda_d,\nu_d)]-\mathbb{E}_q[\log q(\Sigma|\phi)]
\end{align*}
Collapsing Parameters
\begin{align*}
\mathcal{L}&\geq\sum_{d=1}^{D}\int\int q(\eta_d)\log\frac{p(W_d|\eta_d,\beta)p(\eta_d|\mu,\Sigma)p(\Sigma|\gamma)}{q(\eta_d)q(\Sigma)}d\eta_d d\Sigma\\
&=\sum_{d=1}^{D}\left(\mathbb{E}_{q(\eta_d)}\left[\log p(W_d|\eta_d,\beta)\right]-KL(q(\eta_d)||p(\eta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}
Amortized Inference
\begin{align*}
=&\sum_{d=1}^{D}\left(\mathbb{E}_{q(\eta_d)}\left[\log p(w_d|\mathcal{LN}(\mu_{\eta_d}(w_d),\sigma_{\eta_d}(w_d)),\beta)\right]-KL(q(\eta_d)||p(\eta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}
Reparameterization Trick
\begin{align*}
=&\sum_{d=1}^{D}\left(\mathbb{E}_{q(\epsilon)}\left[\log p(w_d|\sigma(\mu_{\eta_d}(w_d)+\epsilon\sigma_{\eta_d}(w_d)),\beta)\right]-KL(q(\eta_d)||p(\eta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}
Mini-batch
\begin{align*}
\tilde{\mathcal{L}}=\frac{\mathcal{D}}{|\mathcal{B}|}\sum_{d\in\mathcal{D_B}}\left(\mathbb{E}_{q(\epsilon)}\left[\log p(w_d|\sigma(\mu_{\eta_d}(w_d)+\epsilon\sigma_{\eta_d}(w_d)),\beta)\right]-KL(q(\eta_d)||p(\eta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}
\begin{align*}
P(w|\mu_0(w_d)+\Sigma_0^{1/2}(w)\epsilon)
\end{align*}
The KL-divergence for the logistic-normal distribution
\begin{align*}
\tilde{\mathcal{L}}=&\sum_{d=1}^{D}\left[-\frac{1}{2}\left(tr(\sigma_1^{-1}\sigma_0)+(\mu_1-\mu_0)\Sigma_1^{-1}(\mu_1-\mu_0)-K+\log\frac{|\sigma_1|}{|\sigma_0|}\right)\right]\\
&+\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}\left[w_d^\top\log\sigma(\beta(\mu_0+\sigma_0^{1/2}\epsilon))\right]-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align*}