\section{Inference and Estimation}
% Inference Equation
% Variational Inference
% KL-divergence for the logistic normal distribution
\subsection{Evidence Lower Bound(ELBO)}\label{ch4:2} To perform Variational inference, it is essential to derive the Evidence Lower Bound (ELBO) first as the objective function for the optimization. By 
\begin{align}\label{eq:elbo_1}
\mathcal{L}\geq&\mathbb{E}_q[\log p(W,Z,\theta,\Sigma)]-\mathbb{E}_q[\log q(Z,\theta,\Sigma)]\\
=&\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(w_{d,n}|z_{d,n},\beta)]+\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(z_{d,n}|\theta_d)]\\
&+\sum_{d=1}^{D}\mathbb{E}[\log p(\theta_d|\mu,\Sigma)]+\mathbb{E}_q[\log p(\Sigma|\gamma)]-\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log q(z_{d,n}|\alpha_{d,n})]\\
&-\sum_{d=1}^{D}\mathbb{E}_q[\log q(\theta_d|\lambda_d,\nu_d)]-\mathbb{E}_q[\log q(\Sigma|\phi)]
\end{align}
% TODO Monte Carlo Estimate
The expectation log likelihood term in \ref{eq:elbo_1} can be efficiently appriximated by the Monte Carlo sampling method,
\begin{align}\label{eq:elbo_2}
\mathcal{L}\approx\frac{1}{S}\sum_{s=1}^{S}p(W|\theta^{(s)})
\end{align}
\paragraph{Collapsing Parameters}
\begin{align}\label{eq:elbo_3}
\mathcal{L}&\geq\sum_{d=1}^{D}\int\int q(\theta_d)\log\frac{p(W_d|\theta_d,\beta)p(\theta_d|\mu,\Sigma)p(\Sigma|\gamma)}{q(\theta_d)q(\Sigma)}d\theta_d d\Sigma\\
&=\sum_{d=1}^{D}\left(\mathbb{E}_{q(\theta_d)}\left[\log p(W_d|\theta_d,\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
Here we define amortized inference, a optimization technique which perform inference by defined neural networks. $ \mu_\theta(w) $ and $ \sigma_\theta(w) $ are two inference networks take input from the word $ w $. Then a output is generated by the Normal distribution parameterized by $ \mu_\theta(w) $ and $ \sigma_\theta(w) $.
\begin{align}\label{eq:elbo_4}
=&\sum_{d=1}^{D}\left(\mathbb{E}_{q(\theta_d)}\left[\log p(w_d|\mathcal{LN}(\mu_{\theta_d}(w_d),\sigma_{\theta_d}(w_d)),\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)\\
&-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
To apply reparameterization trick, we take transformation from normal distribution to $ \theta=\mu+\epsilon\sigma^{1/2} $ where $ \epsilon\sim N(0,1) $, as  equation \ref{eq:elbo_5}
\begin{align}\label{eq:elbo_5}
=&\sum_{d=1}^{D}\left(\mathbb{E}_{q(\epsilon)}\left[\log p(w_d|\sigma(\mu_{\theta_d}(w_d)+\epsilon\sigma_{\theta_d}(w_d)),\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)\\
&-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
we also apply the minibatch to make able the model perform by subsampling the document collection. By equation \ref{eq:elbo_6}
\begin{align}\label{eq:elbo_6}
\tilde{\mathcal{L}}=&\frac{\mathcal{D}}{|\mathcal{B}|}\sum_{d\in\mathcal{D_B}}\left(\mathbb{E}_{q(\epsilon)}\left[\log p(w_d|\sigma(\mu_{\theta_d}(w_d)+\epsilon\sigma_{\theta_d}(w_d)),\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)\\
&-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
The KL-divergence for the logistic-normal distribution is given as equation \ref{eq:elbo_7} closed-form expression,
\begin{align}\label{eq:elbo_7}
\text{KL}(q(\theta_d)||p(\theta_d|\mu,\Sigma))=-\frac{1}{2}\left(tr(\sigma_1^{-1}\sigma_0)+(\mu_1-\mu_0)\Sigma_1^{-1}(\mu_1-\mu_0)-K+\log\frac{|\sigma_1|}{|\sigma_0|}\right)
\end{align}
so the ELBO then becomes \ref{eq:elbo_8}
\begin{align}\label{eq:elbo_8}
\tilde{\mathcal{L}}=&\sum_{d=1}^{D}\left[-\frac{1}{2}\left(tr(\sigma_1^{-1}\sigma_0)+(\mu_1-\mu_0)\Sigma_1^{-1}(\mu_1-\mu_0)-K+\log\frac{|\sigma_1|}{|\sigma_0|}\right)\right]\\
&+\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}\left[w_d^\top\log\sigma(\beta(\mu_0+\sigma_0^{1/2}\epsilon))\right]-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
% TODO algorithm for the optimization here
\subsection{Optimization step}\label{ch4:3}
In algorithm \ref{algorithm:lkjtm_obj}, first initialize the model and variational parameters. Then, for each epochs, we obtain the transformer embedding $ \rho $ from transformer. After that, the topic embedding $ \beta $ is computed by taking softmax of dot-product of $ \rho $ and $ \alpha $. Then a minibatch $ \mathcal{B} $ is selected from the document for optimization. The number of minibatch is the document collection divides minibatch size where $ \#\text{minibatch}=\frac{\mathcal{D}}{|\mathcal{B}|} $. For each minibatch, the model takes a document and sample lower Cholesky matrix from LKJ Cholesky distribution(description see section \ref{ch2:lkj}). A topic assignment for document d $ \theta_d $ is sampled from logistic-normal distribution $ \mathcal{LN}(\mu,\sigma LL^\top\sigma) $, where $ \mu $ is sampled from half-Cauchy distribution and covariance is a transformation from equation \ref{eq:lkj_trans}. For each word position n, a word is sampled from the softmax of dot-product of transformer embedding $ \rho $ and NN weight $ \alpha $. After the sampling process for the document collection, we estimate the ELBO loss $ L_{ELBO} $ for the topic model, and the cross entropy loss $ L_{CrossEntropy} $. Remind that the topic model and transformer take input differently. The topic model part takes bag-of-words input, a document-vocabulary matrix $ D\times V $ counting the occurrence of vocabulary v in document d. While transformer take sequence of document as input. 
To calculate the loss of the model , we sum up the ELBO loss $ L_{ELBO} $ and cross entropy loss for transformer $ L_{CrossEntropy} $. Then a stochastic gradient is computed by backpropagation. a gradient step to . The process iterates until the maximum iteration is reached. 
\\
\begin{algorithm}[H]
Initialize model and variational parameters\\
\For{epoch $i=1,2,\dots N$}{
Compute the trnasformer embedding $ \rho $\\
Compute $ \beta=\text{softmax}(\rho^\top\alpha) $\\
Choose a minibatch $ \mathcal{B} $ of documents\\
\ForEach{document d in $ \mathcal{B} $}{
Compute $ \mu_d=\text{NN}(x_d;\nu_\mu) $\\
Compute $ \sigma_d=\text{NN}(x_d;\mu_\sigma) $\\
Sample $ L\sim \text{LKJChol}(\gamma) $ \\
Sample $\theta_d\sim\mathcal{LN}(\mu,\sigma LL^\top\sigma)$\\
\ForEach{word position n in docuemnt $ N_d $}{
Sample word $ w_{dn}\sim \text{softmax}(\beta_{w_{dn}}\theta_{d}) $
}
}
Estimate ELBO loss $ \text{L}_\text{ELBO}$ from Eq. \ref{eq:elbo_8}\\
Compute Transformer loss $ \text{L}_\text{CrossEntropy}$ from Eq. \ref{eq:crossentropy}\\
Compute the total loss $ \text{L}=\text{L}_\text{ELBO}+\text{L}_\text{CrossEntropy} $\\
Compute the stochastic gradient via backpropagation\\
Take a stochastic gradient step\\
Update model parameters ($\rho,\alpha,$)\\
Update variational parameters ($ \ $)
}
\label{algorithm:lkjtm_obj}
\caption{Topic modeling with the LKJTM}
\end{algorithm}
%\begin{algorithm}[H]
%Initial $ \theta^{(0)} $ randomly\\
%\While{Not Converge}{
%Sample a document d uniformly from dataset $ \mathcal{D} $\\
%For all k, initial $ \gamma^{d}_{k}=1 $\\
%\While{Not Converge}{
%\For{$ i=1,\cdots,N_d $}{
%\begin{align*}
%\phi_{ik}^{d}\propto\exp{\mathbb{E}}[\log\pi^d_k]+\mathbb{E}[\log\beta_{k,w_i^d}]
%\end{align*}
%}
%Set $ \gamma^{d}=\alpha+\sum_{i=1}^{N_d}\phi_i^d $
%}
%Take a stochastic gradient step $ \theta^{t}=\theta^{t-1}+\epsilon_t+\triangledown_\theta\mathcal{L}_d $
%}
%\caption{SVI for LDA}
%\end{algorithm}