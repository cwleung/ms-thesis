\section{Inference and Estimation}
To perform posterior inference, we apply variational inference to transform the log-likelihood function into a lower bounded optimization problem.
% Inference Equation
\subsection{Variational distribution}
The variational distribution for $ q(\theta_d) $ is a multivariate normal distribution in $ \mathbb{R}^{D} $ that parameterized by mean vector $ \mu(w_d) $, a inference network take input from bag-of-words $ w_d $ and then outputs means from its weight parameters; $ \Sigma $ is a conditional variational parameter from $ q(\Sigma) $. 
\begin{align*}
q(\theta_d|\Sigma)=\mathcal{N}(\mu(w_d),\Sigma)
\end{align*}
% Variational Inference
\subsection{Evidence Lower Bound(ELBO)}\label{ch4:2} To perform Variational inference, it is essential to derive the Evidence Lower Bound (ELBO) first as the objective function for the optimization. By equation \ref{eq:elbo_1}
\begin{align}\label{eq:elbo_1}
\mathcal{L}\geq&\mathbb{E}_q[\log p(W,Z,\theta,\Sigma)]-\mathbb{E}_q[\log q(Z,\theta,\Sigma)]\notag\\
=&\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(w_{d,n}|z_{d,n},\beta)]+\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(z_{d,n}|\theta_d)]\notag\\
&+\sum_{d=1}^{D}\mathbb{E}[\log p(\theta_d|\mu,\Sigma)]+\mathbb{E}_q[\log p(\Sigma|\gamma)]-\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log q(z_{d,n}|\alpha_{d,n})]\notag\\
&-\sum_{d=1}^{D}\mathbb{E}_q[\log q(\theta_d|\lambda_d,\nu_d)]-\mathbb{E}_q[\log q(\Sigma|\phi)]
\end{align}
\paragraph{Collapsing Parameters} we can speed up the computation by marginalize out the z. The probability for word w could be simplified as $ w_{dn}\sim\text{Cat}(\sigma(\theta^\top_{d}\beta)) $.
\begin{align}\label{eq:elbo_3}
\mathcal{L}&\geq\sum_{d=1}^{D}\int\int q(\theta_d)\log\frac{p(W_d|\theta_d,\beta)p(\theta_d|\mu,\Sigma)p(\Sigma|\gamma)}{q(\theta_d)q(\Sigma)}d\theta_d d\Sigma\notag\\
&=\sum_{d=1}^{D}\left(\mathbb{E}_{q(\theta_d)}\left[\log p(W_d|\theta_d,\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
Here we define amortized inference \cite{kingma_auto-encoding_2014}, a optimization technique which perform inference by defined neural networks. $ \mu_\theta(w) $ is an inference network that takes inputs from the bag-of-words vector $ w_d $. Then a output is generated by the normal distribution parameterized by mean vector generated by  $ \mu_\theta(w) $ and covariance $ \Sigma $.
\begin{align}\label{eq:elbo_4}
=&\sum_{d=1}^{D}\left(\mathbb{E}_{\theta_d\sim\mathcal{N}(\mu_{\theta_d}(w_d), q(\Sigma))}\left[\log p(w_d|\theta_d,\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)\notag\\
&-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
To apply reparameterization trick, we take transformation from normal distribution to $ \theta=\mu+\Sigma^{1/2}\epsilon $ where $ \epsilon\sim N(0,I) $ drawn as $ 1\times K $ vector, as equation \ref{eq:elbo_5}
\begin{align}\label{eq:elbo_5}
=&\sum_{d=1}^{D}\left(\mathbb{E}_{\epsilon\sim\mathcal{N}(0,1)}\left[\log p(w_d|\sigma(\mu_{\theta_d}(w_d)+\Sigma^{1/2}\epsilon),\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)\notag\\
&-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
The KL-divergence for the logistic-normal distribution is given as equation \ref{eq:elbo_7} closed-form expression, the KL-divergence between $ q(\theta_d) $ and $ p(\theta_d) $ becomes
\begin{align}\label{eq:elbo_7}
\text{KL}(q(\theta_d)||p(\theta_d|\mu,\Sigma))=\frac{1}{2}\left(\text{tr}(\Sigma_1^{-1}\Sigma_0)+(\mu_1-\mu_0)^\top\Sigma_1^{-1}(\mu_1-\mu_0)+\log\frac{|\Sigma_1|}{|\Sigma_0|}-K\right)
\end{align}
so the ELBO then becomes \ref{eq:elbo_8}
\begin{align}\label{eq:elbo_8}
\tilde{\mathcal{L}}=&\sum_{d=1}^{D}\left[\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}\left[w_d^\top\log\sigma(\beta(\mu_0+\Sigma_0^{1/2}\epsilon))\right]\right.\notag\\
&\left.-\frac{1}{2}\left(\text{tr}(\Sigma_1^{-1}\Sigma_0)+(\mu_1-\mu_0)^\top\Sigma_1^{-1}(\mu_1-\mu_0)+\log\frac{|\Sigma_1|}{|\Sigma_0|}-K\right)\right]\notag\\
&-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
% TODO Rewrite Monte Carlo Estimate
The expectation log likelihood term in \ref{eq:elbo_1} can be efficiently approximated by the Monte Carlo sampling method,
\begin{align}\label{eq:elbo_2}
\mathcal{L}\approx&\sum_{d=1}^{D}\left[\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}\left[w_d^\top\log\sigma(\beta(\mu_0^{(s)}(w_d)+\Sigma_0^{1/2}\epsilon^{(s)}))\right]\right.\notag\\
&\left.-\frac{1}{2}\left(\text{tr}(\Sigma_1^{-1}\Sigma_0)+(\mu_1-\mu_0)^\top\Sigma_1^{-1}(\mu_1-\mu_0)+\log\frac{|\Sigma_1|}{|\Sigma_0|}-K\right)\right]\notag\\
&-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
where the expectation of the reconstruction loss is taken from a set of sample $ S $ to compute the unbiased estimate of ELBO. We also apply the minibatch to make able the model perform by sub-sampling the document collection. By equation \ref{eq:ch4_elbo_6}
% elbo - minibatch
\begin{align}\label{eq:ch4_elbo_6}
\tilde{\mathcal{L}}\approx&\frac{\mathcal{D}}{|\mathcal{B}|}\sum_{d\in\mathcal{D_B}}\left[\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}\left[w_d^\top\log\sigma(\beta(\mu_0^{(s)}(w_d)+\Sigma_0^{1/2}\epsilon^{(s)}))\right]\right.\notag\\
&\left.-\frac{1}{2}\left(\text{tr}(\Sigma_1^{-1}\Sigma_0)+(\mu_1-\mu_0)^\top\Sigma_1^{-1}(\mu_1-\mu_0)+\log\frac{|\Sigma_1|}{|\Sigma_0|}-K\right)\right]\notag\\
&-KL(q(\Sigma)||p(\Sigma|\gamma))
\end{align}
% TODO Transformer loss
The transformer loss is calculated over the sum of tokens of a selected sentence sequence \textsc{SEQ} from each document, the loss of each token is evaluated in Cross entropy loss.
\begin{equation}\label{ch4:eq_cross_entropy}
L_{CrossEntropy}=\sum_{d=1}^{D}\sum_{n=1}^{|\textsc{SEQ}|}\text{CrossEntropy}(w_{dn})
\end{equation}
where
\begin{align}
\text{CrossEntropy}(w) = -\sum_{i=1}^{V}p(w^{(i)})\log \hat{p}(w^{(i)})
\end{align}
it is a metric for comparing the probability between the word probability $ p(w) $ and the probability from prediction $ \hat{p}(w) $, which is converted to a probability from one hot vector by mapping it to simplex by softmax function that sum to 1.
% TODO algorithm for the optimization here
\subsection{Optimization step}\label{ch4:3}
In algorithm \ref{algorithm:lkjtm_obj}, first initialize the model and variational parameters. Then, for each epochs, we obtain the transformer embedding $ \rho $ from transformer. After that, the topic embedding $ \beta $ is computed by taking softmax of dot-product of $ \rho $ and $ \alpha $. Then a minibatch $ \mathcal{B} $ is selected from the document for optimization. The number of minibatch is the document collection divides minibatch size where $ \#\text{minibatch}=\frac{\mathcal{D}}{|\mathcal{B}|} $. For each minibatch, the model takes a document and sample lower Cholesky matrix from LKJ Cholesky distribution(description see section \ref{ch2:lkj}). A topic assignment for document d $ \theta_d $ is sampled from logistic-normal distribution $ \mathcal{LN}(\mu,\text{LL}^\top) $, where $ \mu $ is sampled from half-Cauchy distribution and covariance is a transformation from equation \ref{eq:lkj_trans}. For each word position n, a word is sampled from the softmax of dot-product of transformer embedding $ \rho $ and NN weight $ \alpha $. After the sampling process for the document collection, we estimate the ELBO loss $ L_{ELBO} $ for the topic model, and the cross entropy loss $ L_{CrossEntropy} $. Remind that the topic model and transformer take input differently. The topic model part takes bag-of-words input, a document-vocabulary matrix $ D\times V $ counting the occurrence of vocabulary v in document d. While transformer take sequence of document as input. 
To calculate the loss of the model , we sum up the ELBO loss $ L_{ELBO} $ and cross entropy loss for transformer $ L_{CrossEntropy} $. Then a stochastic gradient is computed by backpropagation. a gradient step to . The process iterates until the maximum iteration is reached. 
\\
\begin{algorithm}[H]
Initialize model and variational parameters\\
\For{epoch $i=1,2,\dots N$}{
Obtain trnasformer embedding $ \rho $\\
Compute $ \beta=\text{softmax}(\rho^\top\alpha) $\\
Choose a minibatch $ \mathcal{B} $ of documents\\
\ForEach{document d in $ \mathcal{B} $}{
Compute $ \mu_d=\mu_{\phi}(w_d) $\\
Sample $ L\sim \text{LKJChol}(\gamma) $ \\
Sample $\theta_d\sim\mathcal{LN}(\mu_d,\Sigma)$ where $ \Sigma=LL^\top $\\
\ForEach{word position n in docuemnt $ N_d $}{
Sample word $ w_{dn}\sim\text{Cat}(\theta_{d}\beta_{w_{dn}}) $
}
}
Estimate ELBO loss $ \text{L}_\text{ELBO}$ from Eq. \ref{eq:ch4_elbo_6}\\
Compute Transformer loss $ \text{L}_\text{CrossEntropy}$ from Eq. \ref{ch4:eq_cross_entropy}\\
Compute the total loss $ \text{L}=\text{L}_\text{ELBO}+\text{L}_\text{CrossEntropy} $\\
Compute the stochastic gradient via backpropagation\\
Take a stochastic gradient step\\
Update model parameters\\
Update variational parameters
}
\label{algorithm:lkjtm_obj}
\caption{Training on TECTM}
\end{algorithm}
%\begin{algorithm}[H]
%Initial $ \theta^{(0)} $ randomly\\
%\While{Not Converge}{
%Sample a document d uniformly from dataset $ \mathcal{D} $\\
%For all k, initial $ \gamma^{d}_{k}=1 $\\
%\While{Not Converge}{
%\For{$ i=1,\cdots,N_d $}{
%\begin{align*}
%\phi_{ik}^{d}\propto\exp{\mathbb{E}}[\log\pi^d_k]+\mathbb{E}[\log\beta_{k,w_i^d}]
%\end{align*}
%}
%Set $ \gamma^{d}=\alpha+\sum_{i=1}^{N_d}\phi_i^d $
%}
%Take a stochastic gradient step $ \theta^{t}=\theta^{t-1}+\epsilon_t+\triangledown_\theta\mathcal{L}_d $
%}
%\caption{SVI for LDA}
%\end{algorithm}