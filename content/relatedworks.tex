\section{Related Works}\label{AB}
% #1 CTM
Correlated Topic Model (CTM)\cite{blei_correlated_2007} is the original work that proposed to alleviate the problem LDA, which did not utilize the topic information between correlated topics. The proposed model replaced dirichlet distribution with a logistic-normal prior with covariance matrix to represent the relationship between topics.
% #3 CGTM
Xun \cite{he_efficient_2017} employed words embedding into Correlated Topic Model, the new correlated topic model as Correlated Gaussian Topic Model (CGTM). In their paper they make use of word embedding space and model the correlation between topics by calculation of similarity between words in the embedding space.
% #2 CTMTE
Similarly, He\cite{xun_correlated_nodate} proposed Correlated Topic Modeling with Topic Embedding (CTMTE), which transformed the topic distribution previously obtained into lower dimension topic embedding space. The correlation between topics were directly computed through the similarity calculation in the vector space. The paper stated it reduces the running time as a scalable framework into large applications.
% #4 GATON
Yang\cite{yang_graph_2020} introduced new topic model with Graph neural network techniques. The paper introduced Graph Attention TOpic Network (GATON) which hybridized the graph attention network (GAT) and amortized inference into application of topic modeling which supposed to reduce the require computation complexity.

There have been a several of works focus on word embedding and topic model. Major of them combined statistical model and embedding approach to model topic distribution. In other words, representing a word by mapping every single word into continuous space
-	Gaussian Topic Model models the topic using Gaussian distributions with latent means and covariances. 
-	Correlated Topic model with embeddings the author combined the embedding method and correlated topic model. 
Amortized inference are common in implementing to topic models, specifically, a neural network architecture with encoder-decoder are used into topic model structure for model inference.
-	Autoencoding Variational Inference for Topic Model the author applied amortized variational inference to approximate the variational distribution of the model.
-	Embedded Topic model, which on top of the ProdLDA topic model, implemented Word2Vec semantics to further improve the performance on topic coherence and predictive distribution.
Some other attempts use graph techniques to model topic distributions.
-	Graph Attention Topic Model, the author used graph attention network to model the topic distribution. 
In this paper, we develop the Transformer Embedded Topic Model (TMTE), a model that combine word embedding and topic model together to make a better fit of the dataset. Moreover, we integrate the Transformer into embedding, such that we can also take assumption of word position and convert it into meaningful contextual embeddings. 
In its generative process, the model uses the topic embedding to forma a per-topic distribution over the vocabulary. Specifically, the TMTE uses a log-linear model that takes the inner product of the word embedding matrix and the topic embedding.
With this form, the TMTE assigns high probability to a word v in topic k by measuring the agreement between the word’s embedding and the topic’s embedding.

To evaluate our model, we applied the proposed model on 20Newsgroups and Reuter-21578 dataset. The experiment results demonstrate that our model is capable to obtain high quality topics than the state-of-the-art model. 