\label{A}
Today, myriads of terabyte data is one of the crucial challenge for scientific researcher.  Information Retrieval become increasingly important for building useful information from massive data set. Specifically, topic modeling is one of the most popular technique for extracting key point ideas and exploring documents. Specifically, correlated topic models (CTM) make use of the correlation between topics and deliver a better result. In the research, we would like to explore the application one of the topic modeling techniques and try to improve their performance. In section \ref{AA}, we provide a brief introduction to the existing algorithm and covers the background of it. Then, section \ref{AB} will discuss the current application and state-of-art improvement on topic modeling advances. Following that, section \ref{AC} will elaborate the conduct the research and the general direction. And section \ref{AD} will explain the way the proposal model to be evaluated and compare to the existing model. Finally, section \ref{AE} will make a summary for the proposal so far.
\section{Motivation}
Topic modeling is one of the most exciting domain in Information Retrieval (IR). It can be extended to accomplish versatile range of IR and data mining tasks. For instance, one of the topic model: Latent Dirichlet Allocation (LDA), was proposed and examined its capability on extracting latent topics and output keywords suggestion for each topic. As result, LDA has been implemented into varies area of applications. However, There are several problems that LDA could not handle well.
\paragraph{Computational complexity}
Generally, LDA acquires to computer the posterior distribution for inference, which is relatively expensive to obtain an exact solution. In the same way, its variant, correlated topic model (CTM) requires to calculate the covariance matrix specifically, which makes it not feasible come into practical application.
\paragraph{Statistical Laws}In particular, LDA does not take empirical statistical laws observed in text into account. For example, LDA's prior does not dependent on Zipf's Law or Heap's Law, which may not collaborate well with natural document text data. Similarly, Moody\cite{moody_mixing_2016} proposed lda2vec, which exploit the meta-information of each document and evaluate their correlation between documents.
\paragraph{Correlation information} Correlation information can be useful to identify topics. For instance, hockey and soccer are correlated but uncorrelated other topic like space and religion. Such intuition could help topic model to exploit those information. 
\paragraph{Bag-of-word assumption} Typical topic model like Nonnegative Matrix Factorization (NMF) \cite{paisley_bayesian_nodate} and Latent Dirichlet Allocation (LDA) \cite{blei_latent_2003} do not consider positional information from the document set. This lead to the drawbacks of those models may not make good prediction on the topic words due to the limitations. For most of the NLP tasks, it is very common to let the model learning context by
\paragraph{Transfer Learning} Due to the prevalence of deep neural network in recent years, Transform Learning has became a hot topic in research. The aim for transfer learning is to make find a way to reduce computational cost and improve re-usability of machine learning models. In the ascendant of powerful accelerator such as GPU and more memory, we are able to build more complex architecture and boost up computation time. Specifically, Transformer has been one of the most used NLP architecture. Number of variants have been built due to its success, such as, BERT, ROBERTA, ELMO.
\section{Applications}
Topic model are one of the crucial tasks in discovering hidden topic from document collections. The success of LDA make able it does not limit to topic modeling task. many tasks have been applied with the model, for example,
\paragraph{Feature Extraction}\label{AAA} For number of n topics, LDA can accomplish the task cluster them and extract a set of corpus with k terms which can represent each topic most and uniquely. Eren \cite{eren_covid-19_2020} uses LDA to analysis all literature related to COVID-19 and subdivided them into minor topics. As result, each subtopic were extracted with a set of keywords.
\paragraph{Text Classification}Topic model can also treated to deal with classification task to identify unseen data. Kim \cite{kim_multi-co-training_2019} adapted the semi-supervised method with multi-co-training method to improve the overall classification performance. Moreover, the paper extended Word2Vec to Doc2Vec which maintain semantic relationship between two paragraphs. Doc2Vec transforms a paragraph into a d-dimensional vectors, which put documents with similar paragraph into near vector space.
\paragraph{Recommender Systems} LDA often can be applied to recommendations. Xu\cite{xu_uis-lda_2017} employed UIS-LDA (A User Recommendation based on Social Connections and Interests of Users in Uni-Directional Social Networks), which utilizes Generative Polya Urn (GPU) model and perform prediction for nearest user for the recommendations. Wang \cite{wang_st-sage_2017} implemented a LDA version which utilize the twitter datasets and recommend a serial of tourist location to user.

Moreover, in the growth of word embedding [2013] enables an effective way to capture semantic meaning in language in a continuous vector space. Vocabularies that have similar meaning are close together by Euclidean distance. 
