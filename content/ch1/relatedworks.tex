\section{Literature Review}\label{ch1:3}
%% Original models
These topic models take bag-of-word assumption and model each document as an admixture of latent topics, which are multinomial distributions over words.
% # NMF
Nonnegative Matrix Factorization (NMF) \cite{lee_learning_1999} uses singular value decomposition to construct latent topic and topic-word distribution from the document, which consist of document-topic distribution matrix and topic word distribution matrix.
% # PLSA
Probabilistic Latent Semantic Analysis (PLSA) \cite{hofmann_probabilistic_2013} is a probabilistic model assign every document in a single topic, and then assign word for every word position given the topic assignment.
% # LDA
Similar to PLSA, Latent Dirichlet Allocation (LDA) \cite{blei_latent_2003} advanced PLSA from the topic assumption documents, which every document consist of admixture of topic distribution.

Some improvement exploit the correlation information between topics, which model the topic assignment with multivariate distribution to parameterize the relation between topics with mean and covariance.

Moreover, due to the success of LDA. there have been a numbers of topic models proposed on top of the LDA model. Dynamic Latent Dirichlet Allocation\cite{blei_probabilistic_2012} was developed for continuous time data. Relational Latent Dirichlet Allocation \cite{chang_relational_nodate} . Supervised Latent Dirichlet Allocation \cite{mcauliffe_supervised_2008} includes labeled data which supposed to be helpful in several particular application areas such as movie review and sparse data prediction. Later LDA was extended to nonparametric version, hierarchical Latent Dirichlet Allocation (hLDA)\cite{teh_hierarchical_2006}, which follows a stochastic process called n-Chinese Restaurant Process (nCRP)\cite{teh_tutorial_nodate}. hLDA maintain a hierarchical structure of topic instead of flat structure in LDA. % pachinko LDA % 

%% Amortized inference
Amortized inference\cite{kingma_auto-encoding_2014} are common in implementing to topic models, specifically, a neural network architecture with encoder-decoder are used into topic model structure for model inference.
% # AVITM
Srivastava\cite{srivastava_autoencoding_2017} applied amortized variational inference to approximate the variational distribution of the model. Specifically, product of expert were used to collapse out the document-topic assignment parameter and simplify the inference process.

% Graph model
Some other attempts use graph techniques to model topic distributions.
% # GATON
Yang\cite{yang_graph_2020} introduced new topic model with Graph neural network techniques. The paper introduced Graph Attention TOpic Network (GATON) which hybridized the graph attention network (GAT) and amortized inference into application of topic modeling which supposed to reduce the require computation complexity.
% # GTM
%Gerlach\cite{gerlach_network_2018} proposed
%% Bi-gram
In past research, some considered n-gram to model the word pattern under a sentence structure which results a better prediction.
% Wallach 06
Wallach \cite{wallach_topic_2006} proposed a topic model using bi-gram information from the data set to yield a better performance in topic interpretibility.
% Wang 07
Wang \cite{wang_topical_2007} extends the topic model to n-gram assumption with similar approach.
% Times Series model


% Gaussian Process
% deep Gaussian processes (Dai et al., 2015)
% stochastic variational sparse GP formulation (Hensman et al., 2013)
% Recently, Jahnichen et al.(2018) developed a stochastic variational inference for DTM