\section{Objective and Outline}\label{ch1:4}
In this paper, we develop the Transformer Embedded Topic Model (TMTE), a model that combine word embedding and topic model together to make a better fit of the dataset. Moreover, we integrate the Transformer into embedding, such that we can also take assumption of word position and convert it into meaningful contextual embeddings. 
In its generative process, the model uses the topic embedding to forma a per-topic distribution over the vocabulary. Specifically, the TMTE uses a log-linear model that takes the inner product of the word embedding matrix and the topic embedding.
With this form, the TMTE assigns high probability to a word v in topic k by measuring the agreement between the word’s embedding and the topic’s embedding.

To evaluate our model, we applied the proposed model on 20Newsgroups and Reuter-21578 dataset. The experiment results demonstrate that our model is capable to obtain high quality topics than the state-of-the-art model. 

% Explain chapters
To give a outline of this thesis, chapter \ref{ch2} will give a brief background to the problem description and the related knowledge. Chapter \ref{ch3} will specify the methodology and explain the detail of our model. Then chapter \ref{ch4} will derive the inference formulation for our model. The result compared with other baseline models are examined in chapter \ref{ch5}. Lastly, chapter \ref{ch6} will sum up the merit and limitation overall the research, prospect the future works.