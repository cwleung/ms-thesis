\section{Inference and Estimation}
Since the posterior is intractable to compute, we apply variational inference to approximate the parameters for the log-marginal likelihood.
\subsection{Variational Distribution}
To begin with, we first setup variational distribution to approximate the parameter of the model. 
\begin{equation}
q(\theta,\eta,\Sigma,\alpha)=\prod_{d=1}^{D}q(\theta_d|w_d,\eta_{t_d},\Sigma_{t_d})\prod_{t=1}^{T}q(\eta_t|w_t)q(\Sigma_{t_d}|\gamma_{t_d})\prod_{k=1}^{K}q(\alpha_k^{(t)})
\end{equation}
The variational distribution for topic proportion $ q(\theta_d|\eta_{t_d},w_d) $ is logistic-normal distribution. We applied amortized inference to approximate the model, which the mean and covariance matrix is generated by two inference network $ \mu_\phi $ and $ \sigma_\phi $ taking bag-of-word input $ w_d $ at each document d and residual input $ \eta_{t_d} $ end up $ \mathbb{R}^{D\times (V+K)} $ dimension in the input space.
% residual input part
The output for time-topic proportion is applied to the residual connection on the amortized inference of document-topic proportion $ \theta $. The inference network for variational parameter $ \phi $ take the residual input from stacked input: both $ \mathbb{R}^{V} $ bag-of-words vectors $ w_d $, and the $ \mathbb{R}^{K} $ time-topic proportion $ \eta_{t_d} $.

% layernorm
To perform amortized inference for $ \theta $, we apply Layer Normalization\cite{ba_layer_2016} to normalize the input vectors. The LayerNorm performs normalization over features, which enables a better training. Also, it is more stable than batch normalization for training while the batch size is pretty small. And thus it is able to maintain a lower variance than batch normalization does throughout the training loop.
\begin{align}\label{eq:ch5_variational_theta}
q(\theta_d|\eta_{t_d},w_d)&=\mathcal{N}(\mu_\phi(x),\Sigma_{t_d})\\
x&=\text{LayerNorm}([w_d,\eta_{t_d}])
\end{align}
% TODO eta variational distribution, how to write the decoder encoder format
The variational distribution for $ q(\eta_t|w_t) $ is basically theã€€normal distribution parametrized by two inference network, which takes the input from bag-of-word $ w_d $ and the variable from \cite{lawrence_learning_2007}, L is the dimension for latent input space, V is the token size.
\begin{align*}
q(\eta_t|w_t)=\int\prod_{i=1}^{V}q(w_i)\prod_{d=1}^{L}p(f_d|u_d,x)q(u_d)du_d
\end{align*}
% alpha
The variational distribution for $ q(\alpha_k^{(t)}) $ is the embedding is parameterized mean $ \mu_{\varphi} $ and $ \sigma_{\varphi} $ conditioned on local parameter $ \varphi $. Notice that $ \varphi $ is not a variational parameter as usual amortized inference.
\begin{align}
q(\alpha)=\mathcal{N}(\mu_\varphi,\sigma_\varphi)
\end{align}
% reparameterization trick
both variational distribution $ q(\theta_d|\eta_{t_d}, w_d) $ and $ q(a) $ applied reparameterization trick \cite{kingma_auto-encoding_2014} with transformation $ \mathcal{N}(\mu,\sigma)\approx\mu+\epsilon\sigma^{1/2}, \epsilon\sim \mathcal{N}(0,1) $, to avoid high variance issue on the variational variables.
% Inference Equation
\subsection{Evidence lower bound (ELBO)}
We take the log marginal likelihood from eq. \ref{eq:dtetm_joint}. Noted the given equation could derive the ELBO by simply applying the Jensen inequality. For sake of simplicity, we decompose the marginal likelihood two parts, the document-topic proportion part and the time-topic proportion part. Then summing up the terms as eq. \ref{eq:ch6_elbo1}. In such way, we are able to derive a ELBO without calculating the KL-divergence for $ q(\eta_{t_d}) $ and $ p(\eta_{t_d}|w_t) $. And so we could obtain the ELBO for $ \log p(\eta_{t_d}|w_t) $ conveniently from \cite{titsias_bayesian_nodate}.
\begin{align}\label{eq:ch6_elbo1}
\log p(w_d|\theta_d,\alpha)&=\sum_{d=1}^{D}\iint\log p(w_d|\theta_d,\eta_{t_d},\alpha^{(t_d)})p(\theta_d|\eta_{t_d})p(\eta_{t_d}|w_t)d\theta_{d}d\eta_{t_d}\\
&=\sum_{d=1}^{D}\iint\log p(w_d|\theta_d,\eta_{t_d},\alpha^{(t_d)})p(\theta_d|\eta_{t_d})d\theta_{d}+\sum_{d=1}^{D}\int\log p(\eta_{t_d}|w_t)d\eta_{t_d}
\end{align}
We first derive the ELBO for the document-topic model part, denoted as $ \mathcal{L}_{1} $, we can obtain $ p(w_d,\theta_d,\alpha|\eta_{t_d})=p(w_d|\theta_d,\alpha^{(t_d)})p(\theta_{d}|\eta_{t_d})p(\alpha)$ by factorization. 
\begin{align}\label{eq:ch5_elbo_p1}
\mathcal{L}_{1}\geq&\sum_{d=1}^{D}\int q(\theta_d)\log\frac{p(w_d|\theta_{d},\alpha)p(\theta_d|\eta_{t_d},w_d)}{q(\theta_d)q(\eta_{t_d})}d\theta_d\\
=&\sum_{d=1}^{D}\mathbb{E}_{ q(\theta_d|\eta_{t_d},w_d)}[\log p(w_{d}|\theta_d,\alpha^{(t_d)})]-\text{KL}(q(\theta_d|\eta_{t_d},w_t)||p(\theta_d|\eta_{t_d}))\\
\end{align}
% alpha
The KL-divergence for $ \alpha^{(t)} $ in time t is a closed-form in normal distribution. And so the equation can be derived as \ref{eq:ch5_kl_alpha}. The variational distribution for $ q(\alpha_k^{(k)}) $ is parametrized by two inference network $ \mu_{\varphi} $ and $ \sigma_{\varphi} $, where $ \varphi $ is a local variational parameter.
At initial time point, the mean for $ p(\alpha^{(1)}) $ parameterized at 0.
\begin{equation}\label{eq:ch5_kl_alpha}
\text{KL}(q(\alpha_k^{(t)})||p(\alpha_k^{(t)}|\alpha^{(t-1)}_k))=\frac{1}{2}\left(\log\frac{\gamma^2}{\sigma_\varphi^2}+\frac{\sigma_\varphi^2+(\mu_\varphi-\alpha_k^{(t-1)})^2}{\gamma^2}-1\right)
\end{equation}
% monte carlo estimation
Then we compute the unbiased estimate by Monte Carlo sampling.
\begin{equation}
\tilde{\mathcal{L}}_{1}\approx\frac{1}{S}\sum_{s=1}^{S}\sum_{d=1}^{D}\mathbb{E}_{ q(\theta_d|\eta_{t_d},w_d)}[\log p(w_{d,n}|\theta_d,\alpha^{(t_d)})]-\text{KL}(q(\theta_d|\eta_{t_d},w_t)||p(\theta_d|\eta_{t_d}))
\end{equation}
% minibatch
To speed up computation, we apply mini-batching as previous chapter
\begin{equation}
\tilde{\mathcal{L}}\approx\frac{|\mathcal{D}|}{|\mathcal{B}|}\sum_{d\in\mathcal{B}}\mathbb{E}_{ q(\theta_d|\eta_{t_d},w_d)}[\log p(w_{d,n}|\theta_d,\alpha^{(t_d)})]-\text{KL}(q(\theta_d|\eta_{t_d},w_t)||p(\theta_d|\eta_{t_d}))
\end{equation}
% expectation
The first term is the likelihood term for , where $ p(w_{d,n}|\theta_d,\alpha^{(t_d)}) $ is the , and $ \sigma(\cdot) $ is the softmax function.
\begin{align}
\mathbb{E}_{ q(\theta_d|\eta_{t_d},w_d)}[\log p(w_{d,n}|\theta_d,\alpha^{(t_d)})]=\mathbb{E}_{ q(\theta_d|\eta_{t_d},w_d)}[\sigma(w_{d,n}^\top\sigma(\rho^\top\alpha^{(t_d)}))]
\end{align}
% theta
The second term is the KL-divergence between $ p(\theta_d) $ and $ q(\theta_d) $, the distributions are logistic-normal distributions so can be represented in closed-form. So we simply derive the KL-divergence by substitution,
\begin{align}\label{eq:ch5_kl_theta}
\text{KL}(q(\theta_d|\mu_\phi,\sigma_\phi)||p(\theta_d|\eta_{t_d},w_d))=\frac{1}{2}\left(\log\frac{1}{\sigma_\varphi^2}+\sigma_\phi^2+(\mu_\phi-\eta_{t_d})^2-1\right)
\end{align}
% eta
For the second part, we derive the ELBO for GPLVM, which the detailed derivation for the ELBO has been discussed in the original paper \cite{titsias_bayesian_nodate}. $ w=\{w_t\}^{T}_{t=1}\in\mathbb{R}^{T\times V} $ is the observed data, which bag-of-words with respect to the word count by that timestamps over the documents. $ \eta_{t_d}=\{\}\in\mathbb{R}^{T\times K} $, the latent variable distributes the topic proportion over time. Known that the dimension reduction is performed, as $ K<<V $, where the defined number of topic is supposed to be much smaller than the size of vocabularies.
% f_d, u_d, Z
$ f_d $ is the latent function which takes $ M $ inducing point $ u_d $, where M is the number of inducing points defined for the training process. And where $ u_d $ is conditioned at input locations $ Z\in\mathbb{R}^{M\times K} $. 
% phi, lambda
$ \phi $ is the local variational parameters. $ \lambda $ is the global variational parameters. $ \sigma_{w_t} $ is the gaussian noise.
% First Term

% Second Term

% Third Term

% Variational distribution
% TxV->TxK
\begin{align}\label{eq:ch5_elbo_p2}
\log p(w_t|\eta_{t_d})\geq&\sum_{d=1}^{V}\sum_{t=1}^{T}\mathbb{E}_{q_\phi(\eta_t)}\mathbb{E}_{p(f_d|u_d,\eta_t)q_{\lambda}(u_d)}\left[\log\mathcal{N}(w_{t,d};f_d(\eta_t),\sigma_y^2)\right]\\
&-\sum_{t=1}^{T}\text{KL}(q_{\phi}(\eta_t)||p(\eta_t))-\sum_{d=1}^{V}\text{KL}(q_\lambda(u_d)||p(u_d|Z))\\
=&\mathcal{L}_2
\end{align}
% Variables
% First term
% Second term
% Third term
By assembling the ELBO terms from above, and substituting the KL terms from eq. \ref{eq:ch5_kl_alpha}, \ref{eq:ch5_kl_theta} into \ref{eq:ch6_elbo1}, the ELBO equation becomes follows
\begin{align}\label{eq:ch6_elbo2}
p(w|\theta,\alpha)\geq&\sum_{d=1}^{D}\mathbb{E}_{ q(\theta_d|\eta_{t_d},w_d)}[\sigma(w_{d}^\top\theta_d\sigma(\rho^\top\alpha^{(t_d)}))]\\
&-\frac{1}{2}\sum_{d=1}^{D}\left(\log\frac{1}{\sigma_\varphi^2}+\sigma_\phi^2+(\mu_\phi-\eta_{t_d})^2-1\right)\\
&-\frac{1}{2}\sum_{t=1}^{T}\sum_{k=1}^{K}\left(\log\frac{\gamma^2}{\sigma_\varphi^2}+\frac{\sigma_\varphi^2+(\mu_\varphi-\alpha_k^{(t-1)})^2}{\gamma^2}-1\right)\\
&-\sum_{t=1}^{T}\text{KL}(q_{\phi}(\eta_t)||p(\eta_t))-\sum_{d=1}^{V}\text{KL}(q_\lambda(u_d)||p(u_d|Z))\\
&-\sum_{t=1}^{T}\text{KL}(q(\Sigma_{t})|p(\Sigma_{t}|\gamma_{t}))\\
=&\mathcal{L}
\end{align}
% TODO should we also add the term for the transformer error?
%\subsection{Stochastic variational inference for GPLVM}
%\begin{align*}
%p(\beta|t)=\int p(\beta|U_\beta,t,z_\beta)p(U_\beta|z_\beta)dU_\beta\\
%\log p(\cdot|\beta)\geq\mathbb{E_{q(\beta)}}[p(\cdot|\beta)]-KL(q(U_\beta)||p(U_\beta))
%\end{align*}
%\begin{align*}
%p(\mu|t)=\int p(\mu|U_\mu,t,z_\mu)p(U_\mu|z_\mu)dU_\mu\\
%\log p(\cdot|\mu)\geq\mathbb{E_{q(\mu)}}[p(\cdot|\mu)]-KL(q(U_\mu)||p(U_\mu))
%\end{align*}
%Variational inference for Wishart Process
%\begin{align*}
%p(f_{ij}|t)=\int p(f_{ij}|u_{ij},t,z_{ij})p(u_{ij}|z_{ij})du_{ij}
%\end{align*}
%\begin{align*}
%\log p(\cdot|\Sigma)\geq\mathbb{E}_{q(F)q(l)}[p(\cdot|\Sigma)]-\sum_{i,j}KL(q(u_{ij})||p(u_{ij}))-KL(q(l)||p(l))
%\end{align*}
%where
%\begin{align*}
%q(F)=\prod_{i,j}q(f_{ij}), q(f_{ij})=\int p(f_{ij}|u_{ij})q(u_{ij})du_{ij}
%\end{align*}
%likelihood
%\begin{align*}
%\mathbb{E}_{q(\mu)q(F)q(L)q(\beta)}[\mathcal{L}_W]=\sum_{d=1}^{D}\left(\mathbb{E}_{q(\eta_d)q(\beta_{t_d})}[\log p(W_d|\eta_d,\beta_{t_d})]-\mathbb{E}_{q(\eta_d)q(\mu_{t_d})q(F_{t_d})q(L)}[KL(q(\eta_d)||p(\eta_d|\mu_{t_d},\Sigma_{t_d}))]\right)
%\end{align*}
\begin{algorithm}[H]
Initialize hyperparameters\\
\For{epoch $ 1, \dots, N$}{
Compute $ \beta_{1:K}=\text{softmax}(\rho^\top\alpha_{1:K}) $\\
Choose a minibatch $ \mathcal{B} $ of documents\\
\For{document d in minibatch}{
% sample the topic proportion
Compute the topic proportion $ \theta_d $ from eq. \ref{eq:ch5_variational_theta}\\
\For{word n in document d}{
Sample the word $ w_{d,n} $
}
}
% parameters
Estimate ELBO loss $ \text{L}_{\text{ELBO}} $ from Eq. \ref{eq:ch6_elbo2}\\
Compute Transformer loss $ \text{L}_{\text{CrossEntropy}} $ \\
Compute the unbiased gradient estimate\\
Compute the stochastic gradient via backpropagation\\
Take a stochastic gradient step with Adam\\
Update the model and variational parameters
}
\caption{DTETM Algorithm}
\label{algorithm:detem_algorithm}
\end{algorithm}
\paragraph{Algorithm}The procedure for the model training is described in algorithm \ref{algorithm:detem_algorithm}. To begin with, the parameters are initialized.
% Epochs
For each epochs $ 1,\dots, N $, the topic embedding are computed. 
Then the topic-word proportion $ \beta $ are computed.
To perform stochastic variational inference, we divide data set into smaller data batch $ \mathcal{B} $.
For each document d in $ \mathcal{B} $, we compute the topic proportion $ \theta_d $. For each word position n in document, a word is then to be drawn as $ w_{d,n} $
% the optimization process
The ELBO loss is being computed by the sum of document-topic proportion part from equation \ref{eq:ch5_elbo_p1} and time-topic proportion part from equation \ref{eq:ch5_elbo_p2}.
Following that, the transformer loss is computed by cross entropy error, 
To optimizer the model, we compute unbiased gradient estimate from the model 
The procedure continue repeating until the maximum iteration is reached.
% Variational Inference

% KL-divergence for the logistic normal distribution

%\subsection{Evidence Lower Bound(ELBO)}\label{ch4:2} To perform Variational inference, it is essential to derive the Evidence Lower Bound (ELBO) first as the objective function for the optimization. By 
%\begin{align}\label{eq:elbo_1}
%\mathcal{L}\geq&\mathbb{E}_q[\log p(W,Z,\theta,\Sigma)]-\mathbb{E}_q[\log q(Z,\theta,\Sigma)]\\
%=&\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(w_{d,n}|z_{d,n},\beta)]+\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(z_{d,n}|\theta_d)]\\
%&+\sum_{d=1}^{D}\mathbb{E}[\log p(\theta_d|\mu,\Sigma)]+\mathbb{E}_q[\log p(\Sigma|\gamma)]-\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log q(z_{d,n}|\alpha_{d,n})]\\
%&-\sum_{d=1}^{D}\mathbb{E}_q[\log q(\theta_d|\lambda_d,\nu_d)]-\mathbb{E}_q[\log q(\Sigma|\phi)]
%\end{align}
%% TODO Monte Carlo Estimate
%The expectation log likelihood term in \ref{eq:elbo_1} can be efficiently appriximated by the Monte Carlo sampling method,
%\begin{align}\label{eq:elbo_2}
%\mathcal{L}\approx\frac{1}{S}\sum_{s=1}^{S}p(W|\theta^{(s)})
%\end{align}
%\paragraph{Collapsing Parameters}
%\begin{align}\label{eq:elbo_3}
%\mathcal{L}&\geq\sum_{d=1}^{D}\int\int q(\theta_d)\log\frac{p(W_d|\theta_d,\beta)p(\theta_d|\mu,\Sigma)p(\Sigma|\gamma)}{q(\theta_d)q(\Sigma)}d\theta_d d\Sigma\\
%&=\sum_{d=1}^{D}\left(\mathbb{E}_{q(\theta_d)}\left[\log p(W_d|\theta_d,\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)-KL(q(\Sigma)||p(\Sigma|\gamma))
%\end{align}
%Here we define amortized inference, a optimization technique which perform inference by defined neural networks. $ \mu_\theta(w) $ and $ \sigma_\theta(w) $ are two inference networks take input from the word $ w $. Then a output is generated by the Normal distribution parameterized by $ \mu_\theta(w) $ and $ \sigma_\theta(w) $.
%\begin{align}\label{eq:elbo_4}
%=&\sum_{d=1}^{D}\left(\mathbb{E}_{q(\theta_d)}\left[\log p(w_d|\mathcal{LN}(\mu_{\theta_d}(w_d),\sigma_{\theta_d}(w_d)),\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)\\
%&-KL(q(\Sigma)||p(\Sigma|\gamma))
%\end{align}
%To apply reparameterization trick, we take transformation from normal distribution to $ \theta=\mu+\epsilon\sigma^{1/2} $ where $ \epsilon\sim N(0,1) $, as  equation \ref{eq:elbo_5}
%\begin{align}\label{eq:elbo_5}
%=&\sum_{d=1}^{D}\left(\mathbb{E}_{q(\epsilon)}\left[\log p(w_d|\sigma(\mu_{\theta_d}(w_d)+\epsilon\sigma_{\theta_d}(w_d)),\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)\\
%&-KL(q(\Sigma)||p(\Sigma|\gamma))
%\end{align}
%we also apply the minibatch to make able the model perform by subsampling the document collection. By equation \ref{eq:elbo_6}
%\begin{align}\label{eq:elbo_6}
%\tilde{\mathcal{L}}=&\frac{\mathcal{D}}{|\mathcal{B}|}\sum_{d\in\mathcal{D_B}}\left(\mathbb{E}_{q(\epsilon)}\left[\log p(w_d|\sigma(\mu_{\theta_d}(w_d)+\epsilon\sigma_{\theta_d}(w_d)),\beta)\right]-KL(q(\theta_d)||p(\theta_d|\mu,\Sigma))\right)\\
%&-KL(q(\Sigma)||p(\Sigma|\gamma))
%\end{align}
%The KL-divergence for the logistic-normal distribution is given as equation \ref{eq:elbo_7} closed-form expression,
%\begin{align}\label{eq:elbo_7}
%\text{KL}(q(\theta_d)||p(\theta_d|\mu,\Sigma))=-\frac{1}{2}\left(tr(\sigma_1^{-1}\sigma_0)+(\mu_1-\mu_0)\Sigma_1^{-1}(\mu_1-\mu_0)-K+\log\frac{|\sigma_1|}{|\sigma_0|}\right)
%\end{align}
%so the ELBO then becomes \ref{eq:elbo_8}
%\begin{align}\label{eq:elbo_8}
%\tilde{\mathcal{L}}=&\sum_{d=1}^{D}\left[-\frac{1}{2}\left(tr(\sigma_1^{-1}\sigma_0)+(\mu_1-\mu_0)\Sigma_1^{-1}(\mu_1-\mu_0)-K+\log\frac{|\sigma_1|}{|\sigma_0|}\right)\right]\\
%&+\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}\left[w_d^\top\log\sigma(\beta(\mu_0+\sigma_0^{1/2}\epsilon))\right]-KL(q(\Sigma)||p(\Sigma|\gamma))
%\end{align}
%% TODO algorithm for the optimization here
%\subsection{Optimization step}\label{ch4:3}
%In algorithm \ref{algorithm:lkjtm_obj}, first initialize the model and variational parameters. Then, for each epochs, we obtain the transformer embedding $ \rho $ from transformer. After that, the topic embedding $ \beta $ is computed by taking softmax of dot-product of $ \rho $ and $ \alpha $. Then a minibatch $ \mathcal{B} $ is selected from the document for optimization. The number of minibatch is the document collection divides minibatch size where $ \#\text{minibatch}=\frac{\mathcal{D}}{|\mathcal{B}|} $. For each minibatch, the model takes a document and sample lower Cholesky matrix from LKJ Cholesky distribution(description see section \ref{ch2:lkj}). A topic assignment for document d $ \theta_d $ is sampled from logistic-normal distribution $ \mathcal{LN}(\mu,\sigma LL^\top\sigma) $, where $ \mu $ is sampled from half-Cauchy distribution and covariance is a transformation from equation \ref{eq:lkj_trans}. For each word position n, a word is sampled from the softmax of dot-product of transformer embedding $ \rho $ and NN weight $ \alpha $. After the sampling process for the document collection, we estimate the ELBO loss $ L_{ELBO} $ for the topic model, and the cross entropy loss $ L_{CrossEntropy} $. Remind that the topic model and transformer take input differently. The topic model part takes bag-of-words input, a document-vocabulary matrix $ D\times V $ counting the occurrence of vocabulary v in document d. While transformer take sequence of document as input. 
%To calculate the loss of the model , we sum up the ELBO loss $ L_{ELBO} $ and cross entropy loss for transformer $ L_{CrossEntropy} $. Then a stochastic gradient is computed by backpropagation. a gradient step to . The process iterates until the maximum iteration is reached. 
%\\
%\begin{algorithm}[H]
%Initialize model and variational parameters\\
%\For{epoch $i=1,2,\dots N$}{
%Compute the trnasformer embedding $ \rho $\\
%Compute $ \beta=\text{softmax}(\rho^\top\alpha) $\\
%Choose a minibatch $ \mathcal{B} $ of documents\\
%\ForEach{document d in $ \mathcal{B} $}{
%Compute $ \mu_d=\text{NN}(x_d;\nu_\mu) $\\
%Compute $ \sigma_d=\text{NN}(x_d;\mu_\sigma) $\\
%Sample $ L\sim \text{LKJChol}(\gamma) $ \\
%Sample $\theta_d\sim\mathcal{LN}(\mu,\sigma LL^\top\sigma)$\\
%\ForEach{word position n in docuemnt $ N_d $}{
%Sample word $ w_{dn}\sim \text{softmax}(\beta_{w_{dn}}\theta_{d}) $
%}
%}
%Estimate ELBO loss $ \text{L}_\text{ELBO}$ from Eq. \ref{eq:elbo_8}\\
%Compute Transformer loss $ \text{L}_\text{CrossEntropy}$ from Eq. \ref{eq:crossentropy}\\
%Compute the total loss $ \text{L}=\text{L}_\text{ELBO}+\text{L}_\text{CrossEntropy} $\\
%Compute the stochastic gradient via backpropagation\\
%Take a stochastic gradient step\\
%Update model parameters ($\rho,\alpha,$)\\
%Update variational parameters ($ \ $)
%}
%\label{algorithm:lkjtm_obj}
%\caption{Topic modeling with the LKJTM}
%\end{algorithm}
%\begin{algorithm}[H]
%Initial $ \theta^{(0)} $ randomly\\
%\While{Not Converge}{
%Sample a document d uniformly from dataset $ \mathcal{D} $\\
%For all k, initial $ \gamma^{d}_{k}=1 $\\
%\While{Not Converge}{
%\For{$ i=1,\cdots,N_d $}{
%\begin{align*}
%\phi_{ik}^{d}\propto\exp{\mathbb{E}}[\log\pi^d_k]+\mathbb{E}[\log\beta_{k,w_i^d}]
%\end{align*}
%}
%Set $ \gamma^{d}=\alpha+\sum_{i=1}^{N_d}\phi_i^d $
%}
%Take a stochastic gradient step $ \theta^{t}=\theta^{t-1}+\epsilon_t+\triangledown_\theta\mathcal{L}_d $
%}
%\caption{SVI for LDA}
%\end{algorithm}