\section{Inference and Estimation}
Since the posterior is intractable to compute, we apply variational inference to approximate the parameters for the log-marginal likelihood.
\subsection{Variational Distribution}
To begin with, we first setup variational distribution to approximate the parameter of the model. 
\begin{equation}
q(\theta,\eta,\Sigma,\alpha)=\prod_{d=1}^{D}q(\theta_d|w_d,\eta_{t_d},\Sigma_{t_d})\prod_{t=1}^{T}q(\eta_t|w_t)q(\Sigma_{t}|\gamma_{t})\prod_{k=1}^{K}q(\alpha_k^{(t)})
\end{equation}
The variational distribution for topic proportion $ q(\theta_d|\eta_{t_d},w_d) $ is logistic-normal distribution. We applied amortized inference to approximate the model, which the mean and covariance matrix is generated by two inference network $ \mu_\phi $ and $ \sigma_\phi $ taking bag-of-word input $ w_d $ at each document d and residual input $ \eta_{t_d} $ end up $ \mathbb{R}^{D\times (V+K)} $ dimension in the input space.
% residual input part
The output for time-topic proportion is applied to the residual connection on the amortized inference of document-topic proportion $ \theta $. The inference network for variational parameter $ \phi $ take the residual input from stacked input: both $ \mathbb{R}^{V} $ bag-of-words vectors $ w_d $, and the $ \mathbb{R}^{K} $ time-topic proportion $ \eta_{t_d} $.
Then we transform the  using reparameterization trick.
% layernorm
To perform amortized inference for $ \theta $, we apply Layer Normalization\cite{ba_layer_2016} to normalize the input vectors. The LayerNorm performs normalization over features, which enables a better training. Also, it is more stable than batch normalization for training while the batch size is pretty small. And thus it is able to maintain a lower variance than batch normalization does throughout the training loop.
\begin{align}\label{eq:ch5_variational_theta}
q(\theta_d|\eta_{t_d},w_d,\Sigma_{t_d})&=\mathcal{N}(\mu_\phi(x),\Sigma_{t_d})\\
x&=\text{LayerNorm}([w_d,\eta_{t_d}])
\end{align}
% TODO eta variational distribution, how to write the decoder encoder format
The variational distribution for $ q(\eta_t|w_t) $ is basically theã€€normal distribution parametrized by two inference network, which takes the input from bag-of-word $ w_d $ and the variable from \cite{lawrence_learning_2007}, L is the dimension for latent input space, V is the token size.
\begin{align}\label{ch5:eq_var_eta}
q(\eta_t|w_t)=\int\prod_{i=1}^{V}q(w_i)\prod_{d=1}^{L}p(f_d|u_d,W)q(u_d)du_d
\end{align}
% alpha
The variational distribution for $ q(\alpha_k^{(t)}) $ is the embedding is parameterized mean $ \mu_{\varphi} $ and $ \sigma_{\varphi} $ conditioned on local parameter $ \varphi $. Notice that $ \varphi $ is not a variational parameter as usual amortized inference.
\begin{align}\label{ch5:eq_var_alpha}
q(\alpha^{(t)})=\mathcal{N}(\mu_\varphi^{(t)},\sigma_\varphi^{(t)})
\end{align}
% reparameterization trick
both variational distribution $ q(\theta_d|\eta_{t_d}, w_d) $ and $ q(a^{(t)}) $ applied reparameterization trick \cite{kingma_auto-encoding_2014} with transformation $ \mathcal{N}(\mu,\sigma)\approx\mu+\epsilon\sigma^{1/2}, \epsilon\sim \mathcal{N}(0,1) $, to avoid high variance on the variational variables.
% Inference Equation
\subsection{Evidence lower bound (ELBO)}
We take the log marginal likelihood from eq. \ref{eq:dtetm_joint}. Noted the given equation could derive the ELBO by simply applying the Jensen inequality. For sake of simplicity, we decompose the marginal likelihood two parts, the document-topic proportion part and the time-topic proportion part. Then summing up the terms as eq. \ref{ch5:eq_elbo1}. In such way, we are able to derive a ELBO without calculating the KL-divergence for $ q(\eta_{t_d}) $ and $ p(\eta_{t_d}|w_t) $. And so we could obtain the ELBO for $ \log p(\eta_{t_d}|w_t) $ conveniently from \cite{titsias_bayesian_nodate}.
%\begin{align}\label{eq:ch6_elbo1}
%\log p(w_d|\theta_d,\alpha)=&\sum_{d=1}^{D}p(\alpha^{(t_d)}|\alpha^{(t_d-1)})\iint\log p(w_d|\theta_d,\eta_{t_d},\alpha^{(t_d)})p(\theta_d|\eta_{t_d},w_n,\Sigma_{t_d})p(\eta_{t_d}|w_t)d\theta_{d}d\eta_{t_d}\\
%=&\sum_{d=1}^{D}\int\log p(w_d|\theta_d,\eta_{t_d},\alpha^{(t_d)})p(\theta_d|\eta_{t_d},w_n,\Sigma_{t_d})\\
%+\log p(\eta_{t_d}|w_t)d\theta_dd\eta_{t_d}
%\end{align}
%define the ELBO
%\begin{align*}
%\mathcal{L}_1\geq\sum_{d=1}^{D}\iiint q(\theta_d)q(\eta_{t_d})q(\Sigma_{t_d})\frac{p(w_{d,n}|\theta_d,\rho,\alpha^{(t_d)})p(\theta_d|\eta_{t_d},w_d,\Sigma_{t_d})p(\Sigma_{t_d}|\gamma_{t_d})}{q(\theta_d)q(\eta_{t_d})q(\Sigma_{t_d})}d\theta_{d}d\eta_{t_d}d\Sigma_{t_d}
%\end{align*}
\begin{align}\label{ch5:eq_elbo1}
\mathcal{L}\geq&\mathbb{E}_q[\log p(W,\theta,\eta,\Sigma,\alpha|\rho,\gamma)]-\mathbb{E}_q[\log q(\theta,\eta,\Sigma,\alpha)]\notag\\
=&\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(w_{d,n}|\theta_d,\rho,\alpha^{(t_d)})]+\sum_{d=1}^{D}\mathbb{E}[\log p(\theta_d|\eta_{t_d},\Sigma_{t_d})]\notag\\
&+\sum_{t=1}^{T}\mathbb{E}_{q}[\log p(\eta_{t})]+\sum_{t=1}^{T}\mathbb{E}_q[\log p(\Sigma_{t})]+\sum_{t=1}^{T}\sum_{k=1}^{K}\mathbb{E}_{q}[\log p(\alpha_{k}^{(t)}|\alpha_{k}^{(t-1)})]\notag\\
&-\sum_{d=1}^{D}\mathbb{E}_q[\log q(\theta_d|\mu_{\phi}(x_d),\Sigma_{t})]-\sum_{t=1}^{T}\mathbb{E}_{q}[\log q(\eta_{t}|w_{t})]-\sum_{t=1}^{T}\mathbb{E}_q[\log q(\Sigma_{t}|\gamma_{t})]\notag\\
&-\sum_{t=1}^{T}\sum_{k=1}^{K}\mathbb{E}_q[\log q(\alpha_{k}^{(t)}|\alpha_{k}^{(t-1)})]
\end{align}
% second term
and by rearranging the terms, the ELBO can be represented as follows, where the first term is the reconstruction loss, and the remaining is the KL-divergence between the prior and variational distribution for its variational parameters.
\begin{align}\label{eq:ch5_elbo1}
=&\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(w_{d,n}|\theta_d,\rho,\alpha^{(t_d)})]-\sum_{d=1}^{D}\text{KL}(q(\theta_d|w_d,\eta_{t_d},\Sigma_{t_d})||p(\theta_d|\eta_{t_d},\Sigma_{t_d}))\notag\\
&-\sum_{t=1}^{T}\text{KL}(q(\eta_{t}|w_{t})||p(\eta_{t}))-\text{KL}(q(\Sigma_{t}|\gamma_{t})||p(\Sigma_{t}))\notag\\
&-\sum_{k=1}^{K}\text{KL}\left(q(\alpha_{k}^{(t)}|\alpha_{k}^{(t-1)})||p(\alpha_{k}^{(t)}|\alpha_{k}^{(t-1)})\right)
\end{align}
% separate the terms
For sake of simplicity, we separate the ELBO into three parts. We first derive the ELBO for the document-topic model part, denoted as $ \mathcal{L}_{1} $, we can obtain $ p(w_d,\theta_d,\alpha|\eta_{t_d})=p(w_d|\theta_d,\alpha^{(t_d)})p(\theta_{d}|\eta_{t_d})p(\alpha)$ by factorization. 
\begin{align}\label{eq:ch5_elbo_p1}
\mathcal{L}_{1}=&\sum_{d=1}^{D}\sum_{n=1}^{V}\mathbb{E}_q[\log p(w_{d,n}|\theta_d,\rho,\alpha^{(t_d)})]-\sum_{d=1}^{D}\text{KL}(q(\theta_d|w_d,\eta_{t_d},\Sigma_{t_d})||p(\theta_d|w_d,\eta_{t_d},\Sigma_{t_d}))\notag\\
&-\sum_{t=1}^{T}\text{KL}(q(\Sigma^\phi_{t}|\gamma_{t})||p(\Sigma_{t}))
%\sum_{d=1}^{D}\int q(\theta_d)\log\frac{p(w_d|\theta_{d},\alpha)p(\theta_d|\eta_{t_d},w_d)}{q(\theta_d)q(\eta_{t_d})}d\theta_d\\
%=&\sum_{d=1}^{D}\mathbb{E}_{ q(\theta_d|\eta_{t_d},w_d)}[\log p(w_{d}|\theta_d,\alpha^{(t_d)})]-\text{KL}(q(\theta_d|\eta_{t_d},w_t)||p(\theta_d|\eta_{t_d}))\\
\end{align}
% TODO monte carlo estimation
% minibatch
To speed up computation, we apply mini-batching as previous chapter
\begin{align}
\tilde{\mathcal{L}}_{1}\approx&\frac{|\mathcal{D}|}{|\mathcal{B}|}\sum_{d\in\mathcal{D}_{\mathcal{B}}}\bigg[\sum_{n=1}^{V}\mathbb{E}_q[\log p(w_{d,n}|\theta_d,\rho,\alpha^{(t_d)})]-\text{KL}(q(\theta_d|w_d,\eta_{t_d},\Sigma_{t_d})||p(\theta_d|w_d,\eta_{t_d},\Sigma_{t_d}))\bigg]\\
&-\sum_{t=1}^{T}\text{KL}(q(\Sigma^\phi_{t}|\gamma_{t})||p(\Sigma_{t}))
\end{align}
% expectation
The first term is the expected likelihood term for reconstructing the word $ w_{dn} $ from the model, where $ p(w_{d,n}|\theta_d, \rho,\alpha^{(t_d)}) $ is the log likelihood probability parameterized by the variational parameters $ \theta_d, \alpha^{t_d} $ and transformer embedding $ \rho $, which $ \sigma(\cdot) $ is the softmax function. The topic-word proportion is a dot product of transformer embedding $ \rho $ and $ \alpha^{(t_d)} $
\begin{align}
\mathbb{E}_{q(\theta_d)q(\alpha)}[\log p(w_{d,n}|\theta_d,\rho,\alpha^{(t_d)})]=\mathbb{E}_{\theta_d\sim\mathcal{N}(\mu_{\phi}(x),\Sigma^{\phi}_{t_d})}[w_{d,n}\sigma(\theta^\top(\rho^\top\alpha^{(t_d)}))_{d,n}]
\end{align}
and then apply the reparameterization trick to maintain a low-variance gradient estimate to the likelihood term, the transformation $ \theta_d=\mu_{\phi}(x)+(\Sigma^{\phi}_{t_d})^{1/2}\epsilon $, where $ \epsilon\sim\mathcal{N}(0,I) $ is gaussian noise variance.
% expectation 2
\begin{equation}\label{eq:ch5_reconstruction}
=\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}\left[w_{d,n}\sigma((\mu_{\phi}(x_d)+(\Sigma^{\phi}_{t_d})^{1/2}\epsilon)^\top(\rho^\top\alpha^{(t_d)}))_{d,n}\right]
\end{equation}
% theta
The second term is the KL-divergence between $ p(\theta_d) $ and $ q(\theta_d) $, the distributions are logistic-normal distributions so can be represented in closed-form. So we simply derive the KL-divergence by substitution,
%\begin{align}\label{eq:ch5_kl_theta}
%\text{KL}(q(\theta_d|\mu_\phi,\sigma_\phi)||p(\theta_d|\eta_{t_d},w_d))=\frac{1}{2}\left(\log\frac{1}{\sigma_\varphi^2}+\sigma_\phi^2+(\mu_\phi-\eta_{t_d})^2-1\right)
%\end{align}
% eta
the variable x is the parameter stacked with bag-of-word $ w_d $ and residual input $ \eta_{t_d} $ from \ref{eq:ch5_variational_theta}
\begin{align}\label{eq:ch5_kl_theta}
&\text{KL}(q(\theta_d|\mu_{\phi}(x),\Sigma^{\phi}_{t_d})||p(\theta_d|\eta_{t_d},\Sigma_{t_d}))\notag\\
=&\frac{1}{2}\left(\text{tr}(\Sigma_{t_d}^{-1}\Sigma_{t_d}^{\phi})+(\eta_{t_d}-\mu_{\phi}(x_d))^\top\Sigma_{t_d}^{-1}(\eta_{t_d}-\mu_{\phi}(x_d))+\log\frac{|\Sigma_{t_d}|}{|\Sigma^{\phi}_{t_d}|}-K\right)
\end{align}
For the second part, we derive the ELBO for GPLVM, which the detailed derivation for the ELBO has been discussed in the original paper \cite{titsias_bayesian_nodate} as equation \ref{eq:ch5_elbo_p2}. $ w=\{w_t\}^{T}_{t=1}\in\mathbb{R}^{T\times V} $ is the observed data, which bag-of-words with respect to the word count by that timestamps over the documents. $ \eta=\{\eta_t\}^{T}_{t=1}\in\mathbb{R}^{T\times K} $, the latent variable distributes the topic proportion over time. Known that the dimension reduction is performed, as $ K<<V $, where the defined number of topic is supposed to be much smaller than the size of vocabularies.
% f_d, u_d, Z
$ f_d $ is the latent function which takes $ M $ inducing point $ u_d $, where M is the number of inducing points defined for the training process. And where $ u_d $ is conditioned at input locations $ Z\in\mathbb{R}^{M\times K} $. 
% phi, lambda
$ \phi $ is the local variational parameters. $ \lambda $ is the global variational parameters. $ \sigma_{w} $ is the gaussian noise.
% Variational distribution
Specifically, we only extract the terms $ \mathcal{L}_{2} $, the kl-divergence for $ eta $ and $ u $, that to be contributed in the ELBO of the whole model.
% TxV->TxK
\begin{align}\label{eq:ch5_elbo_p2}
\log p(w_t|\eta_{t_d})\geq&\sum_{d=1}^{V}\sum_{t=1}^{T}\mathbb{E}_{q_\phi(\eta_t)}\mathbb{E}_{p(f_d|u_d,\eta_t)q_{\lambda}(u_d)}\left[\log\mathcal{N}(w_{d,t};f_d(\eta_t),\sigma_w^2)\right]\notag\\
&\underbrace{-\sum_{t=1}^{T}\text{KL}(q_{\phi}(\eta_t)||p(\eta_t))-\sum_{d=1}^{V}\text{KL}(q_\lambda(u_d)||p(u_d|Z))}_{\mathcal{L}_2}
\end{align}
% Variables
The KL-divergence for $ \alpha^{(t)} $ in time t is a closed-form in normal distribution. And so the equation can be derived as \ref{eq:ch5_kl_alpha}. The variational distribution for $ q(\alpha_k^{(k)}) $ is parametrized by two inference network $ \mu_{\varphi} $ and $ \sigma_{\varphi} $, where $ \varphi $ is a local variational parameter. And the prior $ p(\alpha_k^{(t)}|\alpha^{(t-1)}_k) $ at time t takes the mean from previous step $ \alpha^{t-1}_{k} $ with variance $ \xi^2 $
In initial step, the mean for $ \alpha^{(1)} $ located at 0.
\begin{equation}\label{eq:ch5_kl_alpha}
\mathcal{L}_{3}=\text{KL}(q(\alpha_k^{(t)})||p(\alpha_k^{(t)}|\alpha^{(t-1)}_k))=\frac{1}{2}\left(\log\frac{\xi^2}{\sigma_\varphi^2}+\frac{\sigma_\varphi^2+(\mu_\varphi-\alpha_k^{(t-1)})^2}{\xi^2}-1\right)
\end{equation}
By assembling the ELBO terms from above, and substituting the KL terms $ \mathcal{L}_{1},\mathcal{L}_{2},\mathcal{L}_{3} $ from eq. \ref{eq:ch5_kl_alpha}, \ref{eq:ch5_elbo_p2},\ref{eq:ch5_kl_theta} into \ref{eq:ch5_elbo1}, the ELBO equation becomes follows
\begin{align}\label{eq:ch6_elbo2}
\log p(w|\theta,\alpha)\geq&\frac{|\mathcal{D}|}{|\mathcal{B}|}\sum_{d\in\mathcal{D}_{\mathcal{B}}}\sum_{n=1}^{V}\mathbb{E}_{\epsilon\sim\mathcal{N}(0,I)}\left[w_{d,n}\sigma((\mu_{\phi}(x_d)+(\Sigma^{\phi}_{t_d})^{1/2}\epsilon)^\top(\rho^\top\alpha^{(t_d)}))_{d,n}\right]\notag\\
&-\frac{1}{2}\frac{|\mathcal{D}|}{|\mathcal{B}|}\sum_{d\in\mathcal{D}_{\mathcal{B}}}\left(\text{tr}(\Sigma_{t_d}^{-1}\Sigma_{t_d}^{\phi})+(\eta_{t_d}-\mu_{\phi}(x_d))^\top\Sigma_{t_d}^{-1}(\eta_{t_d}-\mu_{\phi}(x_d))+\log\frac{|\Sigma_{t_d}|}{|\Sigma^{\phi}_{t_d}|}-K\right)\notag\\
&-\frac{1}{2}\sum_{t=1}^{T}\sum_{k=1}^{K}\left(\log\frac{\xi^2}{\sigma_\varphi^2}+\frac{\sigma_\varphi^2+(\mu_\varphi-\alpha_k^{(t-1)})^2}{\xi^2}-1\right)\notag\\
&-\sum_{t=1}^{T}\text{KL}(q(\Sigma^{\phi}_{t})|p(\Sigma_{t}|\gamma_{t}))\notag\\
&-\sum_{t=1}^{T}\text{KL}(q_{\phi}(\eta_t)||p(\eta_t))-\sum_{d=1}^{V}\text{KL}(q_\lambda(u_d)||p(u_d|Z))\notag\\
=&\mathcal{L}
\end{align}
By the above derivation of ELBO, we can compute the unbiased gradient with Monte Carlo sampling. \\
% TODO should we also add the term for the transformer error?
%\subsection{Stochastic variational inference for GPLVM}
%\begin{align*}
%p(\beta|t)=\int p(\beta|U_\beta,t,z_\beta)p(U_\beta|z_\beta)dU_\beta\\
%\log p(\cdot|\beta)\geq\mathbb{E_{q(\beta)}}[p(\cdot|\beta)]-KL(q(U_\beta)||p(U_\beta))
%\end{align*}
%\begin{align*}
%p(\mu|t)=\int p(\mu|U_\mu,t,z_\mu)p(U_\mu|z_\mu)dU_\mu\\
%\log p(\cdot|\mu)\geq\mathbb{E_{q(\mu)}}[p(\cdot|\mu)]-KL(q(U_\mu)||p(U_\mu))
%\end{align*}
%Variational inference for Wishart Process
%\begin{align*}
%p(f_{ij}|t)=\int p(f_{ij}|u_{ij},t,z_{ij})p(u_{ij}|z_{ij})du_{ij}
%\end{align*}
%\begin{align*}
%\log p(\cdot|\Sigma)\geq\mathbb{E}_{q(F)q(l)}[p(\cdot|\Sigma)]-\sum_{i,j}KL(q(u_{ij})||p(u_{ij}))-KL(q(l)||p(l))
%\end{align*}
%where
%\begin{align*}
%q(F)=\prod_{i,j}q(f_{ij}), q(f_{ij})=\int p(f_{ij}|u_{ij})q(u_{ij})du_{ij}
%\end{align*}
%likelihood
%\begin{align*}
%\mathbb{E}_{q(\mu)q(F)q(L)q(\beta)}[\mathcal{L}_W]=\sum_{d=1}^{D}\left(\mathbb{E}_{q(\eta_d)q(\beta_{t_d})}[\log p(W_d|\eta_d,\beta_{t_d})]-\mathbb{E}_{q(\eta_d)q(\mu_{t_d})q(F_{t_d})q(L)}[KL(q(\eta_d)||p(\eta_d|\mu_{t_d},\Sigma_{t_d}))]\right)
%\end{align*}
\begin{algorithm}[H]
Initialize weights, hyperparameters\\
\For{epoch $ 1, \dots, N$}{
\For{time t in 1\dots T}{
Sample topic embedding $ \alpha^{(t)} $ from eq. \ref{ch5:eq_var_alpha}\\
Sample time-topic proportion $ \eta_{t} $ from eq. \ref{ch5:eq_var_eta}\\
Sample $ L_t\sim \text{LKJChol}(\gamma_{t}) $\\
}
Choose a minibatch $ \mathcal{B} $ of documents\\
\For{document d in minibatch}{
% sample the topic proportion
Compute the topic proportion $ \theta_d $ from eq. \ref{eq:ch5_variational_theta}\\
\For{word n in document d}{
Sample the word $ w_{d,n} $ from eq.\ref{eq:ch5_reconstruction}
}
}
% parameters
Estimate ELBO loss $ \text{L}_{\text{ELBO}} $ from Eq. \ref{eq:ch6_elbo2}\\
Compute Transformer loss $ \text{L}_{\text{CrossEntropy}} $ \\
Compute the unbiased gradient estimate\\
Compute the stochastic gradient via backpropagation\\
Take a stochastic gradient step with Adam\\
Update the model and variational parameters
}
\caption{Training on DTECTM}
\label{algorithm:detem_algorithm}
\end{algorithm}
\paragraph{Algorithm}The procedure for the model training is described in algorithm \ref{algorithm:detem_algorithm}. To begin with, the parameters are initialized.
% Epochs
For each epochs $ 1,\dots, N $, the topic embedding $ \alpha^{(t)} $ in every single time stamp t are computed. 
To perform stochastic variational inference, we divide data set into smaller data batch $ \mathcal{B} $.
% eta
For each document d in $ \mathcal{B} $, we sample time information $ \eta_{t_d} $ from \ref{ch5:eq_var_eta} to the specific time stamp $ t_d $ the document d belongs to.
Then we compute the topic proportion $ \theta_d $. For each word position n in document, a word is then to be drawn as $ w_{d,n} $
% the optimization process
The ELBO loss is being computed by the sum of document-topic proportion part from equation \ref{eq:ch5_elbo_p1} and time-topic proportion part from equation \ref{eq:ch5_elbo_p2}.
Following that, the transformer loss is computed by cross entropy error, 
To optimizer the model, we compute unbiased gradient estimate from the model 
The procedure continue repeating until the maximum iteration is reached.